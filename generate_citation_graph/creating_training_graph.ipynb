{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718498c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6baf9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ids_file='data/student_graph/processed_ids.pickle'\n",
    "papers_data_file='data/student_graph/papers_data.pickle'\n",
    "s2_map_file='data/student_graph/s2_map.pickle'\n",
    "paper_index = pickle.load(open(papers_data_file, \"rb\")) if os.path.exists(papers_data_file) else {}\n",
    "processed = pickle.load(open(processed_ids_file, \"rb\")) if os.path.exists(processed_ids_file) else []\n",
    "s2_map = pickle.load(open(s2_map_file, \"rb\")) if os.path.exists(s2_map_file) else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bc4529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 6545/6545 [00:00<00:00, 7379.86paper/s]\n"
     ]
    }
   ],
   "source": [
    "paper_records = {}\n",
    "author_records = {}\n",
    "institution_records = set()\n",
    "field_records = set()\n",
    "paper_ids = set()\n",
    "author_ids = set()\n",
    "ref_ids = set()\n",
    "arxiv_id_to_s2_id_map = {}\n",
    "it = 0\n",
    "it2 = 0\n",
    "\n",
    "# iterate over the papers in the index\n",
    "for arxiv_paper_id, paper in tqdm(paper_index.items(), desc=\"Processing papers\", unit=\"paper\"):\n",
    "    # if it == 1:\n",
    "    #     pass\n",
    "    if \"code\" in paper.keys() or \"error\" in paper.keys():\n",
    "        continue\n",
    "    record = {\"arxiv_id\": arxiv_paper_id, \n",
    "                \"s2_paperId\": paper['s2_paperId'],\n",
    "                \"title\": paper['title'],\n",
    "                \"abstract\": paper['abstract'],\n",
    "                \"venue\": paper['venue'],\n",
    "                \"year\": paper['year'],\n",
    "                \"referenceCount\": paper['referenceCount'],\n",
    "                \"influentialCitationCount\": paper['influentialCitationCount'],\n",
    "                \"citationCount\": paper['citationCount'],\n",
    "                \"publicationType\": \" | \".join(paper['publicationTypes']) if paper['publicationTypes'] else None,\n",
    "                'reference_ids': [i['paperId'] for i in paper['references']],\n",
    "                'author_ids': [i['authorId'] for i in paper['authors']],\n",
    "                'field_ids': paper['fieldsOfStudy'],\n",
    "            }\n",
    "    paper_records[paper['s2_paperId']] = record\n",
    "    \n",
    "    paper_ids.add(paper['s2_paperId'])\n",
    "\n",
    "    field_records.update(paper['fieldsOfStudy'])\n",
    "\n",
    "    arxiv_id_to_s2_id_map[arxiv_paper_id] = paper['s2_paperId']\n",
    "\n",
    "    # iterate over the authors\n",
    "    for author in paper['authors']:\n",
    "        author_id = author['authorId']\n",
    "        if author_id not in author_ids:\n",
    "            author_record = {\n",
    "                \"authorId\": author_id,\n",
    "                \"name\": author['name'],\n",
    "                \"paperCount\": author['paperCount'],\n",
    "                \"citationCount\": author['citationCount'],\n",
    "                \"hIndex\": author['hIndex'],\n",
    "                \"affiliations\": author['affiliations'],\n",
    "            }\n",
    "            author_records[author_id] = author_record\n",
    "            author_ids.add(author_id)\n",
    "\n",
    "            institution_records.update(author['affiliations'])\n",
    "    it2 = 0\n",
    "    for ref in paper['references']:\n",
    "        # if it2 == 25:\n",
    "        #     pass\n",
    "        # if ref['paperId'] is None:\n",
    "        #     continue\n",
    "        # if ref['paperId'] not in paper_records.keys():\n",
    "        #     record = {\n",
    "        #         \"arxiv_id\": None,\n",
    "        #         \"s2_paperId\": ref['paperId'],\n",
    "        #         \"title\": ref['title'],\n",
    "        #         \"abstract\": ref['abstract'],\n",
    "        #         \"venue\": ref['venue'],\n",
    "        #         \"year\": ref['year'],\n",
    "        #         \"referenceCount\": ref['referenceCount'],\n",
    "        #         \"influentialCitationCount\": ref['influentialCitationCount'],\n",
    "        #         \"citationCount\": ref['citationCount'],\n",
    "        #         \"publicationType\": \"Not available\",\n",
    "        #     }\n",
    "            # paper_records[ref['paperId']] = record\n",
    "            # paper_ids.add(ref['paperId'])\n",
    "        paper_records[paper['s2_paperId']]['reference_ids'].append(ref['paperId'])\n",
    "        if ref['venue'] in ['Neural Information Processing Systems', 'International Conference on Learning Representations', 'International Conference on Machine Learning', 'Conference on Computer Vision and Pattern Recognition', 'European Conference on Computer Vision', 'International Conference on Computer Vision', 'Association for the Advancement of Artificial Intelligence', 'International Joint Conference on Artificial Intelligence', 'International Conference on Data Mining', 'International Conference on Knowledge Discovery and Data Mining', 'International Conference on Web Search and Data Mining', 'International Conference on Machine Learning and Applications', 'International Conference on Pattern Recognition']:\n",
    "            ref_ids.add(ref['paperId'])\n",
    "        # if ref['fieldsOfStudy'] is not None:\n",
    "        #     field_records.update(ref['fieldsOfStudy'])\n",
    "        for author in ref['authors']:\n",
    "            author_id = author['authorId']\n",
    "            if author_id not in author_ids:\n",
    "                author_ids.add(author_id)\n",
    "        \n",
    "        it2 += 1\n",
    "\n",
    "    it += 1\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46882d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6528, 7207, 268, 14, 6528, 177860)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_records), len(author_records), len(institution_records), len(field_records), len(paper_ids), len(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef7dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(paper_records, open('data/teacher_graph/records/paper_records.pkl', \"wb\"))\n",
    "pickle.dump(author_records, open('data/teacher_graph/records/author_records.pkl', \"wb\"))\n",
    "pickle.dump(institution_records, open('data/teacher_graph/records/institution_records.pkl', \"wb\"))\n",
    "pickle.dump(field_records, open('data/teacher_graph/records/field_records.pkl', \"wb\"))\n",
    "pickle.dump(paper_ids, open('data/teacher_graph/paper_ids.pkl', \"wb\"))\n",
    "pickle.dump(author_ids, open('data/teacher_graph/author_ids.pkl', \"wb\"))\n",
    "pickle.dump(arxiv_id_to_s2_id_map, open('data/student_graph/arxiv_id_to_s2_id_map.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a934c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8e75864bf912b49e78aa593b4ab3f4c50fe357c3': {'arxiv_id': '1805.12573v5',\n",
       "  's2_paperId': '8e75864bf912b49e78aa593b4ab3f4c50fe357c3',\n",
       "  'title': 'Learning a Prior over Intent via Meta-Inverse Reinforcement Learning',\n",
       "  'abstract': 'A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a \"prior\" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 65,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 66,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '758311575a6385bb15d4f9af8c0e671cb98184b4': {'arxiv_id': None,\n",
       "  's2_paperId': '758311575a6385bb15d4f9af8c0e671cb98184b4',\n",
       "  'title': 'From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 123,\n",
       "  'publicationType': 'Not available'},\n",
       " '849c91ff8bb3ff67d278adb5295fee78049c6288': {'arxiv_id': None,\n",
       "  's2_paperId': '849c91ff8bb3ff67d278adb5295fee78049c6288',\n",
       "  'title': 'Bayesian Model-Agnostic Meta-Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 54,\n",
       "  'citationCount': 492,\n",
       "  'publicationType': 'Not available'},\n",
       " '38e2f851b705faa0d0a698ed9885bd6834440073': {'arxiv_id': None,\n",
       "  's2_paperId': '38e2f851b705faa0d0a698ed9885bd6834440073',\n",
       "  'title': 'Probabilistic Model-Agnostic Meta-Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 68,\n",
       "  'citationCount': 660,\n",
       "  'publicationType': 'Not available'},\n",
       " '3aba6b43ab2cb3891557d9d61cb706ca658019e4': {'arxiv_id': None,\n",
       "  's2_paperId': '3aba6b43ab2cb3891557d9d61cb706ca658019e4',\n",
       "  'title': 'Concept Learning with Energy-Based Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 25,\n",
       "  'publicationType': 'Not available'},\n",
       " '46b072cf918ec6f50403568a73d4347ea86b7e66': {'arxiv_id': None,\n",
       "  's2_paperId': '46b072cf918ec6f50403568a73d4347ea86b7e66',\n",
       "  'title': 'Recasting Gradient-Based Meta-Learning as Hierarchical Bayes',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 61,\n",
       "  'influentialCitationCount': 42,\n",
       "  'citationCount': 504,\n",
       "  'publicationType': 'Not available'},\n",
       " '5e2c4e7b3302549b3718601c44d9af6c7554efef': {'arxiv_id': None,\n",
       "  's2_paperId': '5e2c4e7b3302549b3718601c44d9af6c7554efef',\n",
       "  'title': 'Learning Robust Rewards with Adversarial Inverse Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 158,\n",
       "  'citationCount': 733,\n",
       "  'publicationType': 'Not available'},\n",
       " '039182e2c3b5882b97cea358bbfccbb736e7eb4c': {'arxiv_id': None,\n",
       "  's2_paperId': '039182e2c3b5882b97cea358bbfccbb736e7eb4c',\n",
       "  'title': 'Burn-In Demonstrations for Multi-Modal Imitation Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Adaptive Agents and Multi-Agent Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 24,\n",
       "  'publicationType': 'Not available'},\n",
       " '43a2f11afb6904f2f9d6c5c629332d98f725caf4': {'arxiv_id': None,\n",
       "  's2_paperId': '43a2f11afb6904f2f9d6c5c629332d98f725caf4',\n",
       "  'title': 'Meta Inverse Reinforcement Learning via Maximum Reward Sharing for Human Motion Analysis',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 8,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bbe13b72314fffcc2f35b0660195f2f6607c00a0': {'arxiv_id': None,\n",
       "  's2_paperId': 'bbe13b72314fffcc2f35b0660195f2f6607c00a0',\n",
       "  'title': 'Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 88,\n",
       "  'publicationType': 'Not available'},\n",
       " '482c0cbfffa77154e3c879c497f50b605297d5bc': {'arxiv_id': None,\n",
       "  's2_paperId': '482c0cbfffa77154e3c879c497f50b605297d5bc',\n",
       "  'title': 'One-Shot Visual Imitation Learning via Meta-Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Conference on Robot Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 48,\n",
       "  'citationCount': 554,\n",
       "  'publicationType': 'Not available'},\n",
       " '00357a417ce470a78f7a84d18ae2604330455d2a': {'arxiv_id': None,\n",
       "  's2_paperId': '00357a417ce470a78f7a84d18ae2604330455d2a',\n",
       "  'title': 'Meta-Learning with Temporal Convolutions',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 199,\n",
       "  'publicationType': 'Not available'},\n",
       " '97b16661aada70a28d2a791cf597427e2aa0ad33': {'arxiv_id': '1705.10479v2',\n",
       "  's2_paperId': '97b16661aada70a28d2a791cf597427e2aa0ad33',\n",
       "  'title': 'Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets',\n",
       "  'abstract': 'Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at this http URL',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 146,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '39a6b9c8d45aa3bf29aede1ba0d4060a4afb834a': {'arxiv_id': None,\n",
       "  's2_paperId': '39a6b9c8d45aa3bf29aede1ba0d4060a4afb834a',\n",
       "  'title': 'Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 44,\n",
       "  'publicationType': 'Not available'},\n",
       " '5c57bb5630835a05eb1c3d0df3e12d6180d75de2': {'arxiv_id': None,\n",
       "  's2_paperId': '5c57bb5630835a05eb1c3d0df3e12d6180d75de2',\n",
       "  'title': 'One-Shot Imitation Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 70,\n",
       "  'influentialCitationCount': 26,\n",
       "  'citationCount': 676,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c269858a7bb34e8350f2442ccf37797856ae9bca': {'arxiv_id': '1703.05175v2',\n",
       "  's2_paperId': 'c269858a7bb34e8350f2442ccf37797856ae9bca',\n",
       "  'title': 'Prototypical Networks for Few-shot Learning',\n",
       "  'abstract': 'We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 1671,\n",
       "  'citationCount': 7860,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518': {'arxiv_id': None,\n",
       "  's2_paperId': 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518',\n",
       "  'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 2432,\n",
       "  'citationCount': 11504,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a3cd82aeecb75ef339889174706d168c40187d7d': {'arxiv_id': None,\n",
       "  's2_paperId': 'a3cd82aeecb75ef339889174706d168c40187d7d',\n",
       "  'title': 'Attentive Recurrent Comparators',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 107,\n",
       "  'publicationType': 'Not available'},\n",
       " '5b8e5804c3adeb4a4e60f8f7d8d76aab0e02cfbe': {'arxiv_id': None,\n",
       "  's2_paperId': '5b8e5804c3adeb4a4e60f8f7d8d76aab0e02cfbe',\n",
       "  'title': 'Learning to Optimize Neural Nets',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 127,\n",
       "  'publicationType': 'Not available'},\n",
       " '44d246df18a012d7dca2c8f89a33ca02d97748b9': {'arxiv_id': None,\n",
       "  's2_paperId': '44d246df18a012d7dca2c8f89a33ca02d97748b9',\n",
       "  'title': 'Incorporating Human Domain Knowledge into Large Scale Cost Function Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 13,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 15,\n",
       "  'publicationType': 'Not available'},\n",
       " '8a05db7a75c65ee61c3ca7a6e5401b946166290d': {'arxiv_id': None,\n",
       "  's2_paperId': '8a05db7a75c65ee61c3ca7a6e5401b946166290d',\n",
       "  'title': 'Semantic Scene Completion from a Single Depth Image',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 201,\n",
       "  'citationCount': 1217,\n",
       "  'publicationType': 'Not available'},\n",
       " '282a380fb5ac26d99667224cef8c630f6882704f': {'arxiv_id': None,\n",
       "  's2_paperId': '282a380fb5ac26d99667224cef8c630f6882704f',\n",
       "  'title': 'Learning to reinforcement learn',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Meeting of the Cognitive Science Society',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 100,\n",
       "  'citationCount': 965,\n",
       "  'publicationType': 'Not available'},\n",
       " '29c887794eed2ca9462638ff853e6fe1ab91d5d8': {'arxiv_id': None,\n",
       "  's2_paperId': '29c887794eed2ca9462638ff853e6fe1ab91d5d8',\n",
       "  'title': 'Optimization as a Model for Few-Shot Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 368,\n",
       "  'citationCount': 3362,\n",
       "  'publicationType': 'Not available'},\n",
       " '954b01151ff13aef416d27adc60cd9a076753b1a': {'arxiv_id': None,\n",
       "  's2_paperId': '954b01151ff13aef416d27adc60cd9a076753b1a',\n",
       "  'title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 65,\n",
       "  'influentialCitationCount': 125,\n",
       "  'citationCount': 1002,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd4c4076a4e3a1ea24f292e68dbb5eeb99e1ea35d': {'arxiv_id': None,\n",
       "  's2_paperId': 'd4c4076a4e3a1ea24f292e68dbb5eeb99e1ea35d',\n",
       "  'title': 'TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 105,\n",
       "  'publicationType': 'Not available'},\n",
       " '4deb435bd9ddd9db30909abe9a20e85c4eced5f1': {'arxiv_id': None,\n",
       "  's2_paperId': '4deb435bd9ddd9db30909abe9a20e85c4eced5f1',\n",
       "  'title': 'Learning to Learn: Model Regression Networks for Easy Small Sample Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 109,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 278,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd1e51c7e374dca4465a91300e98bfb27335be463': {'arxiv_id': None,\n",
       "  's2_paperId': 'd1e51c7e374dca4465a91300e98bfb27335be463',\n",
       "  'title': 'Watch this: Scalable cost-function learning for path planning in urban environments',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE/RJS International Conference on Intelligent RObots and Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 114,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e86f71ca2948d17b003a5f068db1ecb2b77827f7': {'arxiv_id': None,\n",
       "  's2_paperId': 'e86f71ca2948d17b003a5f068db1ecb2b77827f7',\n",
       "  'title': 'Concrete Problems in AI Safety',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 172,\n",
       "  'influentialCitationCount': 109,\n",
       "  'citationCount': 2264,\n",
       "  'publicationType': 'Not available'},\n",
       " '3904315e2eca50d0086e4b7273f7fd707c652230': {'arxiv_id': None,\n",
       "  's2_paperId': '3904315e2eca50d0086e4b7273f7fd707c652230',\n",
       "  'title': 'Meta-Learning with Memory-Augmented Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 93,\n",
       "  'citationCount': 1766,\n",
       "  'publicationType': 'Not available'},\n",
       " '71683e224ab91617950956b5005ed0439a733a71': {'arxiv_id': '1606.04474v2',\n",
       "  's2_paperId': '71683e224ab91617950956b5005ed0439a733a71',\n",
       "  'title': 'Learning to learn by gradient descent by gradient descent',\n",
       "  'abstract': 'The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 183,\n",
       "  'citationCount': 1970,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6': {'arxiv_id': '1606.04080v2',\n",
       "  's2_paperId': 'be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6',\n",
       "  'title': 'Matching Networks for One Shot Learning',\n",
       "  'abstract': 'Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 1392,\n",
       "  'citationCount': 7160,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '4ab53de69372ec2cd2d90c126b6a100165dc8ed1': {'arxiv_id': '1606.03476v1',\n",
       "  's2_paperId': '4ab53de69372ec2cd2d90c126b6a100165dc8ed1',\n",
       "  'title': 'Generative Adversarial Imitation Learning',\n",
       "  'abstract': \"Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 550,\n",
       "  'citationCount': 3027,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '0811597b0851b7ebe21aadce7cb4daac4664b44f': {'arxiv_id': None,\n",
       "  's2_paperId': '0811597b0851b7ebe21aadce7cb4daac4664b44f',\n",
       "  'title': 'One-Shot Generalization in Deep Generative Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 254,\n",
       "  'publicationType': 'Not available'},\n",
       " '846aedd869a00c09b40f1f1f35673cb22bc87490': {'arxiv_id': None,\n",
       "  's2_paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "  'title': 'Mastering the game of Go with deep neural networks and tree search',\n",
       "  'abstract': None,\n",
       "  'venue': 'Nature',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 72,\n",
       "  'influentialCitationCount': 566,\n",
       "  'citationCount': 16872,\n",
       "  'publicationType': 'Not available'},\n",
       " '9ba266a4a4644e877fc37a64be3beddce8904cf7': {'arxiv_id': None,\n",
       "  's2_paperId': '9ba266a4a4644e877fc37a64be3beddce8904cf7',\n",
       "  'title': 'Maximum Entropy Deep Inverse Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 41,\n",
       "  'citationCount': 400,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a964da3eabc16e41ab869f7e70b4c9430af74dd6': {'arxiv_id': None,\n",
       "  's2_paperId': 'a964da3eabc16e41ab869f7e70b4c9430af74dd6',\n",
       "  'title': 'Early Stopping is Nonparametric Variational Inference',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 93,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b6b8a1b80891c96c28cc6340267b58186157e536': {'arxiv_id': None,\n",
       "  's2_paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "  'title': 'End-to-End Training of Deep Visuomotor Policies',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 100,\n",
       "  'influentialCitationCount': 106,\n",
       "  'citationCount': 3382,\n",
       "  'publicationType': 'Not available'},\n",
       " '340f48901f72278f6bf78a04ee5b01df208cc508': {'arxiv_id': None,\n",
       "  's2_paperId': '340f48901f72278f6bf78a04ee5b01df208cc508',\n",
       "  'title': 'Human-level control through deep reinforcement learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Nature',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 3119,\n",
       "  'citationCount': 27119,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a6cb366736791bcccc5c8639de5a8f9636bf87e8': {'arxiv_id': None,\n",
       "  's2_paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "  'title': 'Adam: A Method for Stochastic Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 23174,\n",
       "  'citationCount': 147079,\n",
       "  'publicationType': 'Not available'},\n",
       " '6fc6803df5f9ae505cae5b2f178ade4062c768d0': {'arxiv_id': None,\n",
       "  's2_paperId': '6fc6803df5f9ae505cae5b2f178ade4062c768d0',\n",
       "  'title': 'Fully convolutional networks for semantic segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 69,\n",
       "  'influentialCitationCount': 4283,\n",
       "  'citationCount': 37079,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97': {'arxiv_id': None,\n",
       "  's2_paperId': 'f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97',\n",
       "  'title': 'Recurrent Neural Network Regularization',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 335,\n",
       "  'citationCount': 2720,\n",
       "  'publicationType': 'Not available'},\n",
       " '65438e0ba226c1f97bd8a36333ebc3297b1a32fd': {'arxiv_id': None,\n",
       "  's2_paperId': '65438e0ba226c1f97bd8a36333ebc3297b1a32fd',\n",
       "  'title': 'Reinforcement learning in robotics: A survey',\n",
       "  'abstract': 'Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.',\n",
       "  'venue': 'Int. J. Robotics Res.',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 241,\n",
       "  'influentialCitationCount': 85,\n",
       "  'citationCount': 3251,\n",
       "  'publicationType': 'Not available'},\n",
       " '91eefd86f01f9979986a529a1ee30b94a1addef5': {'arxiv_id': None,\n",
       "  's2_paperId': '91eefd86f01f9979986a529a1ee30b94a1addef5',\n",
       "  'title': 'Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 126,\n",
       "  'publicationType': 'Not available'},\n",
       " '1e045f3447f69d9a7cac18ef23062ea8dd661285': {'arxiv_id': None,\n",
       "  's2_paperId': '1e045f3447f69d9a7cac18ef23062ea8dd661285',\n",
       "  'title': 'Nonlinear Inverse Reinforcement Learning with Gaussian Processes',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 47,\n",
       "  'citationCount': 385,\n",
       "  'publicationType': 'Not available'},\n",
       " '99455cd5021f02a00927eae9d8bcafe9d6a27f8e': {'arxiv_id': None,\n",
       "  's2_paperId': '99455cd5021f02a00927eae9d8bcafe9d6a27f8e',\n",
       "  'title': 'Apprenticeship Learning About Multiple Intentions',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 22,\n",
       "  'citationCount': 186,\n",
       "  'publicationType': 'Not available'},\n",
       " '2832e4cdb1f927dca50422ff010bbd2a9934292e': {'arxiv_id': None,\n",
       "  's2_paperId': '2832e4cdb1f927dca50422ff010bbd2a9934292e',\n",
       "  'title': 'Bayesian Multitask Inverse Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Workshop on Reinforcement Learning',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 106,\n",
       "  'publicationType': 'Not available'},\n",
       " '79ab3c49903ec8cb339437ccf5cf998607fc313e': {'arxiv_id': None,\n",
       "  's2_paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "  'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 448,\n",
       "  'citationCount': 3124,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649': {'arxiv_id': None,\n",
       "  's2_paperId': 'ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649',\n",
       "  'title': 'Understanding the difficulty of training deep feedforward neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 838,\n",
       "  'citationCount': 17325,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c8221c054459e37edbf313668523d667fe5c1536': {'arxiv_id': None,\n",
       "  's2_paperId': 'c8221c054459e37edbf313668523d667fe5c1536',\n",
       "  'title': 'Maximum Entropy Inverse Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 11,\n",
       "  'influentialCitationCount': 461,\n",
       "  'citationCount': 2916,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fd62bc380b66500c31a8f1a8b566bcaea25d1652': {'arxiv_id': None,\n",
       "  's2_paperId': 'fd62bc380b66500c31a8f1a8b566bcaea25d1652',\n",
       "  'title': 'Bayesian Inverse Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Joint Conference on Artificial Intelligence',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 140,\n",
       "  'citationCount': 809,\n",
       "  'publicationType': 'Not available'},\n",
       " '117a50fbdfd473e43e550c6103733e6cb4aecb4c': {'arxiv_id': None,\n",
       "  's2_paperId': '117a50fbdfd473e43e550c6103733e6cb4aecb4c',\n",
       "  'title': 'Maximum margin planning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2006,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 87,\n",
       "  'citationCount': 771,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f65020fc3b1692d7989e099d6b6e698be5a50a93': {'arxiv_id': None,\n",
       "  's2_paperId': 'f65020fc3b1692d7989e099d6b6e698be5a50a93',\n",
       "  'title': 'Apprenticeship learning via inverse reinforcement learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 409,\n",
       "  'citationCount': 3391,\n",
       "  'publicationType': 'Not available'},\n",
       " 'dd6960d576da0d769ca30de6eb6607a66806211f': {'arxiv_id': None,\n",
       "  's2_paperId': 'dd6960d576da0d769ca30de6eb6607a66806211f',\n",
       "  'title': 'Algorithms for Inverse Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2000,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 201,\n",
       "  'citationCount': 1931,\n",
       "  'publicationType': 'Not available'},\n",
       " '94066dc12fe31e96af7557838159bde598cb4f10': {'arxiv_id': None,\n",
       "  's2_paperId': '94066dc12fe31e96af7557838159bde598cb4f10',\n",
       "  'title': 'Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 1999,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 248,\n",
       "  'citationCount': 2386,\n",
       "  'publicationType': 'Not available'},\n",
       " '2e9d221c206e9503ceb452302d68d10e293f2a10': {'arxiv_id': None,\n",
       "  's2_paperId': '2e9d221c206e9503ceb452302d68d10e293f2a10',\n",
       "  'title': 'Long Short-Term Memory',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Computation',\n",
       "  'year': 1997,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 9398,\n",
       "  'citationCount': 88568,\n",
       "  'publicationType': 'Not available'},\n",
       " '801f703e710946d647432ebf523f870aeab5aec5': {'arxiv_id': None,\n",
       "  's2_paperId': '801f703e710946d647432ebf523f870aeab5aec5',\n",
       "  'title': 'Equivalence of regularization and truncated iteration for general ill-posed problems☆',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1996,\n",
       "  'referenceCount': 11,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 39,\n",
       "  'publicationType': 'Not available'},\n",
       " '0d77652784236fd5e5b36839de59a23e730ecf52': {'arxiv_id': None,\n",
       "  's2_paperId': '0d77652784236fd5e5b36839de59a23e730ecf52',\n",
       "  'title': 'Overtraining, regularization and searching for a minimum, with application to neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1995,\n",
       "  'referenceCount': 12,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 170,\n",
       "  'publicationType': 'Not available'},\n",
       " '17594df98c222217a11510dd454ba52a5a737378': {'arxiv_id': None,\n",
       "  's2_paperId': '17594df98c222217a11510dd454ba52a5a737378',\n",
       "  'title': 'On the computational power of neural nets',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 1992,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 50,\n",
       "  'citationCount': 1049,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b64e846fe88acaf302248249696c3b7badde41b5': {'arxiv_id': None,\n",
       "  's2_paperId': 'b64e846fe88acaf302248249696c3b7badde41b5',\n",
       "  'title': 'Meta-neural networks that learn by learning',\n",
       "  'abstract': None,\n",
       "  'venue': '[Proceedings 1992] IJCNN International Joint Conference on Neural Networks',\n",
       "  'year': 1992,\n",
       "  'referenceCount': 3,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 205,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd130325c41947a41a55a4431e9e8e15be89da8ea': {'arxiv_id': None,\n",
       "  's2_paperId': 'd130325c41947a41a55a4431e9e8e15be89da8ea',\n",
       "  'title': 'Learning a synaptic learning rule',\n",
       "  'abstract': None,\n",
       "  'venue': 'IJCNN-91-Seattle International Joint Conference on Neural Networks',\n",
       "  'year': 1991,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 374,\n",
       "  'publicationType': 'Not available'},\n",
       " '44b6da0cd36c0fa2f7d3485c6dc0b6d2fbe379bb': {'arxiv_id': None,\n",
       "  's2_paperId': '44b6da0cd36c0fa2f7d3485c6dc0b6d2fbe379bb',\n",
       "  'title': 'Learning how to learn',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 1,\n",
       "  'influentialCitationCount': 129,\n",
       "  'citationCount': 2125,\n",
       "  'publicationType': 'Not available'},\n",
       " '941ba185f01b1a0a27453fd178aa5f010510ee8b': {'arxiv_id': None,\n",
       "  's2_paperId': '941ba185f01b1a0a27453fd178aa5f010510ee8b',\n",
       "  'title': 'Learning Robust Rewards with Adverserial Inverse Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 53,\n",
       "  'citationCount': 202,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f216444d4f2959b4520c61d20003fa30a199670a': {'arxiv_id': None,\n",
       "  's2_paperId': 'f216444d4f2959b4520c61d20003fa30a199670a',\n",
       "  'title': 'Siamese Neural Networks for One-Shot Image Recognition',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 31,\n",
       "  'influentialCitationCount': 321,\n",
       "  'citationCount': 4109,\n",
       "  'publicationType': 'Not available'},\n",
       " '72a36acfdd23ec56821a7756929639d0dc161f23': {'arxiv_id': None,\n",
       "  's2_paperId': '72a36acfdd23ec56821a7756929639d0dc161f23',\n",
       "  'title': 'Goal Inference as Inverse Planning',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 11,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 166,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bdaec1b3eb9a8f7a2b296be009a148c35236f3ce': {'arxiv_id': None,\n",
       "  's2_paperId': 'bdaec1b3eb9a8f7a2b296be009a148c35236f3ce',\n",
       "  'title': 'Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-. hook',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1987,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 746,\n",
       "  'publicationType': 'Not available'},\n",
       " '607542fc08d6f328f84b6b99cb6f8dff338ece6a': {'arxiv_id': '2105.14099v2',\n",
       "  's2_paperId': '607542fc08d6f328f84b6b99cb6f8dff338ece6a',\n",
       "  'title': 'Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot Meta-Learning',\n",
       "  'abstract': 'Despite recent advances in its theoretical understanding, there still remains a significant gap in the ability of existing PAC-Bayesian theories on meta-learning to explain performance improvements in the few-shot learning setting, where the number of training examples in the target tasks is severely limited. This gap originates from an assumption in the existing theories which supposes that the number of training examples in the observed tasks and the number of training examples in the target tasks follow the same distribution, an assumption that rarely holds in practice. By relaxing this assumption, we develop two PAC-Bayesian bounds tailored for the few-shot learning setting and show that two existing meta-learning algorithms (MAML and Reptile) can be derived from our bounds, thereby bridging the gap between practice and PAC-Bayesian theories. Furthermore, we derive a new computationally-efficient PACMAML algorithm, and show it outperforms existing meta-learning algorithms on several few-shot benchmark datasets.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 31,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '59467dd8020d493a52116c5be18c958fb1925717': {'arxiv_id': None,\n",
       "  's2_paperId': '59467dd8020d493a52116c5be18c958fb1925717',\n",
       "  'title': 'Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks',\n",
       "  'abstract': 'Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient -- when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.',\n",
       "  'venue': 'Conference on Empirical Methods in Natural Language Processing',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 64,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 85,\n",
       "  'publicationType': 'Not available'},\n",
       " '6e2d24dbf959aeb855926430bf1cb476346719b3': {'arxiv_id': None,\n",
       "  's2_paperId': '6e2d24dbf959aeb855926430bf1cb476346719b3',\n",
       "  'title': 'On the Theory of Transfer Learning: The Importance of Task Diversity',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 214,\n",
       "  'publicationType': 'Not available'},\n",
       " '51914901d39eb19d48c7adf53e3596350724ddd8': {'arxiv_id': '2002.05551v5',\n",
       "  's2_paperId': '51914901d39eb19d48c7adf53e3596350724ddd8',\n",
       "  'title': 'PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees',\n",
       "  'abstract': 'Meta-learning can successfully acquire useful inductive biases from data, especially when a large number of meta-tasks are available. Yet, its generalization properties to unseen tasks are poorly understood. Particularly if the number of meta-tasks is small, this raises concerns about overfitting. We provide a theoretical analysis using the PAC-Bayesian framework and derive novel generalization bounds for meta-learning with unbounded loss functions and Bayesian base learners. Using these bounds, we develop a class of PAC-optimal meta-learning algorithms with performance guarantees and a principled meta-regularization. When instantiating our PAC-optimal hyper-posterior (PACOH) with Gaussian processes as base learners, the resulting method consistently outperforms several popular meta-learning methods, both in terms of predictive accuracy and the quality of uncertainty estimates.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 73,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 123,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '3c8a456509e6c0805354bd40a35e3f2dbf8069b1': {'arxiv_id': '1912.01703v1',\n",
       "  's2_paperId': '3c8a456509e6c0805354bd40a35e3f2dbf8069b1',\n",
       "  'title': 'PyTorch: An Imperative Style, High-Performance Deep Learning Library',\n",
       "  'abstract': 'Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 4203,\n",
       "  'citationCount': 40859,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '6b2f425da3c8756ee777987b487526077b50a90e': {'arxiv_id': None,\n",
       "  's2_paperId': '6b2f425da3c8756ee777987b487526077b50a90e',\n",
       "  'title': 'Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks',\n",
       "  'abstract': 'Pre-trained transformer models have shown enormous success in improving performance on several downstream tasks. However, fine-tuning on a new task still requires large amounts of task-specific labeled data to achieve good performance. We consider this problem of learning to generalize to new tasks, with a few examples, as a meta-learning problem. While meta-learning has shown tremendous progress in recent years, its application is still limited to simulated problems or problems with limited diversity across tasks. We develop a novel method, LEOPARD, which enables optimization-based meta-learning across tasks with a different number of classes, and evaluate different methods on generalization to diverse NLP classification tasks. LEOPARD is trained with the state-of-the-art transformer architecture and shows better generalization to tasks not seen at all during training, with as few as 4 examples per label. Across 17 NLP tasks, including diverse domains of entity typing, natural language inference, sentiment analysis, and several other text classification tasks, we show that LEOPARD learns better initial parameters for few-shot learning than self-supervised pre-training or multi-task training, outperforming many strong baselines, for example, yielding 14.6% average relative gain in accuracy on unseen tasks with only 4 examples per label.',\n",
       "  'venue': 'International Conference on Computational Linguistics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 82,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 118,\n",
       "  'publicationType': 'Not available'},\n",
       " 'abf5478c24664a1380b7e213a3ab1c4af54775d0': {'arxiv_id': None,\n",
       "  's2_paperId': 'abf5478c24664a1380b7e213a3ab1c4af54775d0',\n",
       "  'title': 'Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 112,\n",
       "  'citationCount': 629,\n",
       "  'publicationType': 'Not available'},\n",
       " '451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c': {'arxiv_id': None,\n",
       "  's2_paperId': '451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c',\n",
       "  'title': 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding',\n",
       "  'abstract': 'Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.',\n",
       "  'venue': 'BlackboxNLP@EMNLP',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 77,\n",
       "  'influentialCitationCount': 1254,\n",
       "  'citationCount': 6863,\n",
       "  'publicationType': 'Not available'},\n",
       " '90dc22818bd2d97d8deaff168b0137b75a962767': {'arxiv_id': None,\n",
       "  's2_paperId': '90dc22818bd2d97d8deaff168b0137b75a962767',\n",
       "  'title': 'On First-Order Meta-Learning Algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 268,\n",
       "  'citationCount': 2185,\n",
       "  'publicationType': 'Not available'},\n",
       " '6748cab9c2a36d5edb0d787baf2d99ab6e067692': {'arxiv_id': None,\n",
       "  's2_paperId': '6748cab9c2a36d5edb0d787baf2d99ab6e067692',\n",
       "  'title': 'Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 173,\n",
       "  'publicationType': 'Not available'},\n",
       " '9bcef44c5a383499f80df861744465e62f6ba33f': {'arxiv_id': '1709.09346v2',\n",
       "  's2_paperId': '9bcef44c5a383499f80df861744465e62f6ba33f',\n",
       "  'title': 'Cold-Start Reinforcement Learning with Softmax Policy Gradient',\n",
       "  'abstract': 'Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 46,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '768f7353718c6d95f2d63f954f2236369a409135': {'arxiv_id': '1608.04471v3',\n",
       "  's2_paperId': '768f7353718c6d95f2d63f954f2236369a409135',\n",
       "  'title': 'Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm',\n",
       "  'abstract': \"We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 274,\n",
       "  'citationCount': 1059,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'af65eadf393d3470281a3838ec81f29a35777773': {'arxiv_id': '1605.08636v4',\n",
       "  's2_paperId': 'af65eadf393d3470281a3838ec81f29a35777773',\n",
       "  'title': 'PAC-Bayesian Theory Meets Bayesian Inference',\n",
       "  'abstract': \"We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 181,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'a54e971b32cad6dab590f730396f4452a78d052b': {'arxiv_id': None,\n",
       "  's2_paperId': 'a54e971b32cad6dab590f730396f4452a78d052b',\n",
       "  'title': 'Lifelong Learning with Non-i.i.d. Tasks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 97,\n",
       "  'publicationType': 'Not available'},\n",
       " '4aad752c985f305c94e4cd9d6072ec691cf44a56': {'arxiv_id': None,\n",
       "  's2_paperId': '4aad752c985f305c94e4cd9d6072ec691cf44a56',\n",
       "  'title': 'A New PAC-Bayesian Perspective on Domain Adaptation',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 64,\n",
       "  'publicationType': 'Not available'},\n",
       " '8456ca873e80301e70ce209498f9e0a49539985a': {'arxiv_id': None,\n",
       "  's2_paperId': '8456ca873e80301e70ce209498f9e0a49539985a',\n",
       "  'title': 'On the properties of variational approximations of Gibbs posteriors',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 44,\n",
       "  'citationCount': 246,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd': {'arxiv_id': None,\n",
       "  's2_paperId': 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd',\n",
       "  'title': 'ImageNet Large Scale Visual Recognition Challenge',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Journal of Computer Vision',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 124,\n",
       "  'influentialCitationCount': 4686,\n",
       "  'citationCount': 38964,\n",
       "  'publicationType': 'Not available'},\n",
       " '899746ba46279cf2b842615ed38d998f9c4654df': {'arxiv_id': '1311.2838v2',\n",
       "  's2_paperId': '899746ba46279cf2b842615ed38d998f9c4654df',\n",
       "  'title': 'A PAC-Bayesian bound for Lifelong Learning',\n",
       "  'abstract': 'Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far. \\n \\nIn this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 205,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'aeed631d6a84100b5e9a021ec1914095c66de415': {'arxiv_id': None,\n",
       "  's2_paperId': 'aeed631d6a84100b5e9a021ec1914095c66de415',\n",
       "  'title': 'Bayesian Learning via Stochastic Gradient Langevin Dynamics',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 399,\n",
       "  'citationCount': 2593,\n",
       "  'publicationType': 'Not available'},\n",
       " '66314ee0ab45982a3d811b134cb57a20f2b39444': {'arxiv_id': None,\n",
       "  's2_paperId': '66314ee0ab45982a3d811b134cb57a20f2b39444',\n",
       "  'title': 'PAC-Bayesian learning of linear classifiers',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 8,\n",
       "  'influentialCitationCount': 45,\n",
       "  'citationCount': 244,\n",
       "  'publicationType': 'Not available'},\n",
       " '0e2f6482e7230e1d12af88d6b8afcef3d5d733e3': {'arxiv_id': None,\n",
       "  's2_paperId': '0e2f6482e7230e1d12af88d6b8afcef3d5d733e3',\n",
       "  'title': 'Some PAC-Bayesian Theorems',\n",
       "  'abstract': None,\n",
       "  'venue': \"COLT' 98\",\n",
       "  'year': 1998,\n",
       "  'referenceCount': 7,\n",
       "  'influentialCitationCount': 86,\n",
       "  'citationCount': 740,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e526a65b9ef5afb6639fd3a062f4045d24448232': {'arxiv_id': None,\n",
       "  's2_paperId': 'e526a65b9ef5afb6639fd3a062f4045d24448232',\n",
       "  'title': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Machine-mediated learning',\n",
       "  'year': 1992,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 105,\n",
       "  'citationCount': 985,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f': {'arxiv_id': None,\n",
       "  's2_paperId': 'ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f',\n",
       "  'title': 'AUTO-ENCODING VARIATIONAL BAYES',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 2040,\n",
       "  'citationCount': 15463,\n",
       "  'publicationType': 'Not available'},\n",
       " 'df2b0e26d0599ce3e70df8a9da02e51594e0e992': {'arxiv_id': None,\n",
       "  's2_paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'abstract': None,\n",
       "  'venue': 'North American Chapter of the Association for Computational Linguistics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 63,\n",
       "  'influentialCitationCount': 19956,\n",
       "  'citationCount': 90781,\n",
       "  'publicationType': 'Not available'},\n",
       " '4c915c1eecb217c123a36dc6d3ce52d12c742614': {'arxiv_id': None,\n",
       "  's2_paperId': '4c915c1eecb217c123a36dc6d3ce52d12c742614',\n",
       "  'title': 'Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Machine-mediated learning',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 1068,\n",
       "  'citationCount': 8648,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e879d379a92dade3be3e3a3589a08ae491060553': {'arxiv_id': '2006.05066v4',\n",
       "  's2_paperId': 'e879d379a92dade3be3e3a3589a08ae491060553',\n",
       "  'title': 'Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks',\n",
       "  'abstract': 'Modern convolutional neural networks (CNNs) have massive identical convolution blocks, and, hence, recursive sharing of parameters across these blocks has been proposed to reduce the amount of parameters. However, naive sharing of parameters poses many challenges such as limited representational power and the vanishing/exploding gradients problem of recursively shared parameters. In this paper, we present a recursive convolution block design and training method, in which a recursively shareable part, or a filter basis, is separated and learned while effectively avoiding the vanishing/exploding gradients problem during training. We show that the unwieldy vanishing/exploding gradients problem can be controlled by enforcing the elements of the filter basis orthonormal, and empirically demonstrate that the proposed orthogonality regularization improves the flow of gradients during training. Experimental results on image classification and object detection show that our approach, unlike previous parameter-sharing approaches, does not trade performance to save parameters and consistently outperforms overparameterized counterpart networks. This superior performance demonstrates that the proposed recursive convolution block design and the orthogonality regularization not only prevent performance degradation, but also consistently improve the representation capability while a significant amount of parameters are recursively shared.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 2,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '135d49caffe937c2e6f3014de7e5af42afd27599': {'arxiv_id': None,\n",
       "  's2_paperId': '135d49caffe937c2e6f3014de7e5af42afd27599',\n",
       "  'title': 'Structured Convolutions for Efficient Neural Network Design',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 8,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fb7972f30812c7dd056d7943c3e3f00af022d607': {'arxiv_id': None,\n",
       "  's2_paperId': 'fb7972f30812c7dd056d7943c3e3f00af022d607',\n",
       "  'title': 'Dynamic Convolution: Attention Over Convolution Kernels',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 73,\n",
       "  'citationCount': 832,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c678b75beee20328bbd1f9a62316205e8ad2fdd2': {'arxiv_id': None,\n",
       "  's2_paperId': 'c678b75beee20328bbd1f9a62316205e8ad2fdd2',\n",
       "  'title': 'Learning Filter Basis for Convolutional Neural Network Compression',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 97,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c2c083df88e88223e1a411e61040b94c233b1b63': {'arxiv_id': None,\n",
       "  's2_paperId': 'c2c083df88e88223e1a411e61040b94c233b1b63',\n",
       "  'title': 'MMDetection: Open MMLab Detection Toolbox and Benchmark',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 227,\n",
       "  'citationCount': 2764,\n",
       "  'publicationType': 'Not available'},\n",
       " '2cb5a88d314b0867c0481a93df9f0d345fe5a540': {'arxiv_id': None,\n",
       "  's2_paperId': '2cb5a88d314b0867c0481a93df9f0d345fe5a540',\n",
       "  'title': 'Dynamic Recursive Neural Network',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 54,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a2e38ee74b74abc69f56e93c3eb71696957425c9': {'arxiv_id': None,\n",
       "  's2_paperId': 'a2e38ee74b74abc69f56e93c3eb71696957425c9',\n",
       "  'title': 'LegoNet: Efficient Convolutional Neural Networks with Lego Filters',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 33,\n",
       "  'publicationType': 'Not available'},\n",
       " '3e70bbe6c4cd98d66599db709e32b748f184a2d4': {'arxiv_id': None,\n",
       "  's2_paperId': '3e70bbe6c4cd98d66599db709e32b748f184a2d4',\n",
       "  'title': 'CondConv: Conditionally Parameterized Convolutions for Efficient Inference',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 61,\n",
       "  'citationCount': 602,\n",
       "  'publicationType': 'Not available'},\n",
       " '575a8d6ed2e177c7328f8a3524a0631e522cf2dc': {'arxiv_id': None,\n",
       "  's2_paperId': '575a8d6ed2e177c7328f8a3524a0631e522cf2dc',\n",
       "  'title': 'FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 17,\n",
       "  'publicationType': 'Not available'},\n",
       " '359cdea86e4203f73de4c12356fd9139dba9a745': {'arxiv_id': None,\n",
       "  's2_paperId': '359cdea86e4203f73de4c12356fd9139dba9a745',\n",
       "  'title': 'Learning Implicitly Recurrent CNNs Through Parameter Sharing',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 67,\n",
       "  'publicationType': 'Not available'},\n",
       " '45707ac4b40e4e52a22dc7255d68beb6ccaabeb7': {'arxiv_id': None,\n",
       "  's2_paperId': '45707ac4b40e4e52a22dc7255d68beb6ccaabeb7',\n",
       "  'title': 'Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 129,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f53d572f3a0fcbd5e6f9f4381301ed86615c98ec': {'arxiv_id': None,\n",
       "  's2_paperId': 'f53d572f3a0fcbd5e6f9f4381301ed86615c98ec',\n",
       "  'title': 'DCFNet: Deep Neural Network with Decomposed Convolutional Filters',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 69,\n",
       "  'publicationType': 'Not available'},\n",
       " '098b96b713aa8a324090662c7f35213512f4525e': {'arxiv_id': None,\n",
       "  's2_paperId': '098b96b713aa8a324090662c7f35213512f4525e',\n",
       "  'title': 'Residual Connections Encourage Iterative Inference',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 152,\n",
       "  'publicationType': 'Not available'},\n",
       " '79cfb51a51fc093f66aac8e858afe2e14d4a1f20': {'arxiv_id': None,\n",
       "  's2_paperId': '79cfb51a51fc093f66aac8e858afe2e14d4a1f20',\n",
       "  'title': 'Focal Loss for Dense Object Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 2982,\n",
       "  'citationCount': 23636,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ee53c9480132fc0d09b1192226cb2c460462fd6d': {'arxiv_id': None,\n",
       "  's2_paperId': 'ee53c9480132fc0d09b1192226cb2c460462fd6d',\n",
       "  'title': 'Channel Pruning for Accelerating Very Deep Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 243,\n",
       "  'citationCount': 2478,\n",
       "  'publicationType': 'Not available'},\n",
       " '3647d6d0f151dc05626449ee09cc7bce55be497e': {'arxiv_id': None,\n",
       "  's2_paperId': '3647d6d0f151dc05626449ee09cc7bce55be497e',\n",
       "  'title': 'MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 2693,\n",
       "  'citationCount': 20137,\n",
       "  'publicationType': 'Not available'},\n",
       " '1a0912bb76777469295bb2c059faee907e7f3258': {'arxiv_id': None,\n",
       "  's2_paperId': '1a0912bb76777469295bb2c059faee907e7f3258',\n",
       "  'title': 'Mask R-CNN',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 3869,\n",
       "  'citationCount': 26308,\n",
       "  'publicationType': 'Not available'},\n",
       " '9d1fb89b99aafc01ee56fc92df1ad150fba67c22': {'arxiv_id': '1702.00071v4',\n",
       "  's2_paperId': '9d1fb89b99aafc01ee56fc92df1ad150fba67c22',\n",
       "  'title': 'On orthogonality and learning recurrent networks with long term dependencies',\n",
       "  'abstract': 'It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 21,\n",
       "  'citationCount': 236,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '21330285593da35ae24196f7fa4b145e06f6facc': {'arxiv_id': None,\n",
       "  's2_paperId': '21330285593da35ae24196f7fa4b145e06f6facc',\n",
       "  'title': 'Feedback Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 69,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 209,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f6e0856b4a9199fa968ac00da612a9407b5cb85c': {'arxiv_id': None,\n",
       "  's2_paperId': 'f6e0856b4a9199fa968ac00da612a9407b5cb85c',\n",
       "  'title': 'Aggregated Residual Transformations for Deep Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 1276,\n",
       "  'citationCount': 10080,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c2a1cb1612ba21e067a5c3ba478a8d73b796b77a': {'arxiv_id': None,\n",
       "  's2_paperId': 'c2a1cb1612ba21e067a5c3ba478a8d73b796b77a',\n",
       "  'title': 'Pruning Filters for Efficient ConvNets',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 662,\n",
       "  'citationCount': 3610,\n",
       "  'publicationType': 'Not available'},\n",
       " '5694e46284460a648fe29117cbc55f6c9be3fa3c': {'arxiv_id': None,\n",
       "  's2_paperId': '5694e46284460a648fe29117cbc55f6c9be3fa3c',\n",
       "  'title': 'Densely Connected Convolutional Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 4465,\n",
       "  'citationCount': 35739,\n",
       "  'publicationType': 'Not available'},\n",
       " '2c03df8b48bf3fa39054345bafabfeff15bfd11d': {'arxiv_id': None,\n",
       "  's2_paperId': '2c03df8b48bf3fa39054345bafabfeff15bfd11d',\n",
       "  'title': 'Deep Residual Learning for Image Recognition',\n",
       "  'abstract': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 29207,\n",
       "  'citationCount': 187845,\n",
       "  'publicationType': 'Not available'},\n",
       " '06c06885fd53b2cbd407704cf14f658842ed48e5': {'arxiv_id': None,\n",
       "  's2_paperId': '06c06885fd53b2cbd407704cf14f658842ed48e5',\n",
       "  'title': 'Deeply-Recursive Convolutional Network for Image Super-Resolution',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 309,\n",
       "  'citationCount': 2472,\n",
       "  'publicationType': 'Not available'},\n",
       " '7d40e7e5c01bd551edf65902386401e1b8b8014b': {'arxiv_id': None,\n",
       "  's2_paperId': '7d40e7e5c01bd551edf65902386401e1b8b8014b',\n",
       "  'title': 'Channel-Level Acceleration of Deep Face Representations',\n",
       "  'abstract': 'A major challenge in biometrics is performing the test at the client side, where hardware resources are often limited. Deep learning approaches pose a unique challenge: while such architectures dominate the field of face recognition with regard to accuracy, they require elaborate, multi-stage computations. Recently, there has been some work on compressing networks for the purpose of reducing run time and network size. However, it is not clear that these compression methods would work in deep face nets, which are, generally speaking, less redundant than the object recognition networks, i.e., they are already relatively lean. We propose two novel methods for compression: one based on eliminating lowly active channels and the other on coupling pruning with repeated use of already computed elements. Pruning of entire channels is an appealing idea, since it leads to direct saving in run time in almost every reasonable architecture.',\n",
       "  'venue': 'IEEE Access',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 170,\n",
       "  'publicationType': 'Not available'},\n",
       " '642d0f49b7826adcf986616f4af77e736229990f': {'arxiv_id': None,\n",
       "  's2_paperId': '642d0f49b7826adcf986616f4af77e736229990f',\n",
       "  'title': 'Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 839,\n",
       "  'citationCount': 8629,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f9c990b1b5724e50e5632b94fdb7484ece8a6ce7': {'arxiv_id': None,\n",
       "  's2_paperId': 'f9c990b1b5724e50e5632b94fdb7484ece8a6ce7',\n",
       "  'title': 'Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 922,\n",
       "  'citationCount': 7788,\n",
       "  'publicationType': 'Not available'},\n",
       " '4b39a79a1dc8e578e38ee4b7f00b4ec9ded50dd0': {'arxiv_id': None,\n",
       "  's2_paperId': '4b39a79a1dc8e578e38ee4b7f00b4ec9ded50dd0',\n",
       "  'title': 'Recurrent convolutional neural network for object recognition',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 62,\n",
       "  'influentialCitationCount': 107,\n",
       "  'citationCount': 953,\n",
       "  'publicationType': 'Not available'},\n",
       " '424561d8585ff8ebce7d5d07de8dbf7aae5e7270': {'arxiv_id': None,\n",
       "  's2_paperId': '424561d8585ff8ebce7d5d07de8dbf7aae5e7270',\n",
       "  'title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 8960,\n",
       "  'citationCount': 60478,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b89d7f7439cab841934a1ede06bf6b1f593c754f': {'arxiv_id': None,\n",
       "  's2_paperId': 'b89d7f7439cab841934a1ede06bf6b1f593c754f',\n",
       "  'title': 'Accelerating Very Deep Convolutional Networks for Classification and Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 63,\n",
       "  'citationCount': 786,\n",
       "  'publicationType': 'Not available'},\n",
       " '995c5f5e62614fcb4d2796ad2faab969da51713e': {'arxiv_id': '1502.03167v3',\n",
       "  's2_paperId': '995c5f5e62614fcb4d2796ad2faab969da51713e',\n",
       "  'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "  'abstract': \"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.\",\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 2075,\n",
       "  'citationCount': 42630,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '021fc345d40d3e6332cd2ef276e2eaa5e71102e4': {'arxiv_id': None,\n",
       "  's2_paperId': '021fc345d40d3e6332cd2ef276e2eaa5e71102e4',\n",
       "  'title': 'Speeding up Convolutional Neural Networks with Low Rank Expansions',\n",
       "  'abstract': None,\n",
       "  'venue': 'British Machine Vision Conference',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 87,\n",
       "  'citationCount': 1454,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e5ae8ab688051931b4814f6d32b18391f8d1fa8d': {'arxiv_id': None,\n",
       "  's2_paperId': 'e5ae8ab688051931b4814f6d32b18391f8d1fa8d',\n",
       "  'title': 'Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 117,\n",
       "  'citationCount': 1672,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e2d894584986b44710f634b696db371f8aff92e0': {'arxiv_id': None,\n",
       "  's2_paperId': 'e2d894584986b44710f634b696db371f8aff92e0',\n",
       "  'title': 'Understanding Deep Architectures using a Recursive Convolutional Network',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 146,\n",
       "  'publicationType': 'Not available'},\n",
       " '4177ec52d1b80ed57f2e72b0f9a42365f1a8598d': {'arxiv_id': None,\n",
       "  's2_paperId': '4177ec52d1b80ed57f2e72b0f9a42365f1a8598d',\n",
       "  'title': 'Speech recognition with deep recurrent neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Acoustics, Speech, and Signal Processing',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 411,\n",
       "  'citationCount': 8437,\n",
       "  'publicationType': 'Not available'},\n",
       " '84069287da0a6b488b8c933f3cb5be759cb6237e': {'arxiv_id': None,\n",
       "  's2_paperId': '84069287da0a6b488b8c933f3cb5be759cb6237e',\n",
       "  'title': 'On the difficulty of training recurrent neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 338,\n",
       "  'citationCount': 5241,\n",
       "  'publicationType': 'Not available'},\n",
       " '9c0ddf74f87d154db88d79c640578c1610451eec': {'arxiv_id': None,\n",
       "  's2_paperId': '9c0ddf74f87d154db88d79c640578c1610451eec',\n",
       "  'title': 'Parsing Natural Scenes and Natural Language with Recursive Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 104,\n",
       "  'citationCount': 1458,\n",
       "  'publicationType': 'Not available'},\n",
       " '16462190e4122bfd9d5b42bfd234ce6d3eaa6fed': {'arxiv_id': None,\n",
       "  's2_paperId': '16462190e4122bfd9d5b42bfd234ce6d3eaa6fed',\n",
       "  'title': 'Learning Versatile Filters for Efficient Convolutional Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 46,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e7297db245c3feb1897720b173a59fe7e36babb7': {'arxiv_id': None,\n",
       "  's2_paperId': 'e7297db245c3feb1897720b173a59fe7e36babb7',\n",
       "  'title': 'Optimal Brain Damage',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 1989,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 257,\n",
       "  'citationCount': 4687,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a9f72dcd2afcad9b24418a4d795a227ebe0c3c4d': {'arxiv_id': '2002.08841v2',\n",
       "  's2_paperId': 'a9f72dcd2afcad9b24418a4d795a227ebe0c3c4d',\n",
       "  'title': 'Contextual Reserve Price Optimization in Auctions',\n",
       "  'abstract': 'We study the problem of learning a linear model to set the reserve price in order to maximize expected revenue in an auction, given contextual information. First, we show that it is not possible to solve this problem in polynomial time unless the \\\\emph{Exponential Time Hypothesis} fails. Second, we present a strong mixed-integer programming (MIP) formulation for this problem, which is capable of exactly modeling the nonconvex and discontinuous expected reward function. Moreover, we show that this MIP formulation is ideal (the strongest possible formulation) for the revenue function. Since it can be computationally expensive to exactly solve the MIP formulation, we also study the performance of its linear programming (LP) relaxation. We show that, unfortunately, in the worst case the objective gap of the linear programming relaxation can be $O(n)$ times larger than the optimal objective of the actual problem, where $n$ is the number of samples. Finally, we present computational results, showcasing that the mixed-integer programming formulation, along with its linear programming relaxation, are able to superior both the in-sample performance and the out-of-sample performance of the state-of-the-art algorithms on both real and synthetic datasets.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 7,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '385057ad5c678b5cd413835b64a813e930c77a4c': {'arxiv_id': None,\n",
       "  's2_paperId': '385057ad5c678b5cd413835b64a813e930c77a4c',\n",
       "  'title': 'CAQL: Continuous Action Q-Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 41,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ba73fe5e00212289dea15c642a2bd0e1db3b9b6c': {'arxiv_id': None,\n",
       "  's2_paperId': 'ba73fe5e00212289dea15c642a2bd0e1db3b9b6c',\n",
       "  'title': 'Strong mixed-integer programming formulations for trained neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Mathematical programming',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 98,\n",
       "  'influentialCitationCount': 29,\n",
       "  'citationCount': 249,\n",
       "  'publicationType': 'Not available'},\n",
       " '1af891e02e349684b6dc30ee5b25a40c5eb33463': {'arxiv_id': None,\n",
       "  's2_paperId': '1af891e02e349684b6dc30ee5b25a40c5eb33463',\n",
       "  'title': 'An Algorithmic Approach to Linear Regression',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 23,\n",
       "  'publicationType': 'Not available'},\n",
       " '0e49c64cce8f87858cc7a528f18cb9a71143d395': {'arxiv_id': None,\n",
       "  's2_paperId': '0e49c64cce8f87858cc7a528f18cb9a71143d395',\n",
       "  'title': 'Integer Programming',\n",
       "  'abstract': None,\n",
       "  'venue': 'Optimizations and Programming',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 13,\n",
       "  'influentialCitationCount': 140,\n",
       "  'citationCount': 2605,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bf6d434eed90f4ee1622585cb11ea06ccd6076ea': {'arxiv_id': None,\n",
       "  's2_paperId': 'bf6d434eed90f4ee1622585cb11ea06ccd6076ea',\n",
       "  'title': '(Gap/S)ETH hardness of SVP',\n",
       "  'abstract': None,\n",
       "  'venue': 'Symposium on the Theory of Computing',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 64,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 39,\n",
       "  'publicationType': 'Not available'},\n",
       " 'edce5208900a2702eee5b2452781909a4c8aa77a': {'arxiv_id': None,\n",
       "  's2_paperId': 'edce5208900a2702eee5b2452781909a4c8aa77a',\n",
       "  'title': 'Verifying Neural Networks with Mixed Integer Programming',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 117,\n",
       "  'publicationType': 'Not available'},\n",
       " '3e3cf7002740cff2dc3143a2d340bd2af9e96f2a': {'arxiv_id': None,\n",
       "  's2_paperId': '3e3cf7002740cff2dc3143a2d340bd2af9e96f2a',\n",
       "  'title': 'Optimal classification trees',\n",
       "  'abstract': None,\n",
       "  'venue': 'Machine-mediated learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 79,\n",
       "  'citationCount': 615,\n",
       "  'publicationType': 'Not available'},\n",
       " '447fd7c5c9caf86a45494273c033ce95c4d63086': {'arxiv_id': None,\n",
       "  's2_paperId': '447fd7c5c9caf86a45494273c033ce95c4d63086',\n",
       "  'title': 'Almost-polynomial ratio ETH-hardness of approximating densest k-subgraph',\n",
       "  'abstract': None,\n",
       "  'venue': 'Symposium on the Theory of Computing',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 151,\n",
       "  'publicationType': 'Not available'},\n",
       " '1a62fa3b81471924d55a281049e401cd7eb3d4c7': {'arxiv_id': None,\n",
       "  's2_paperId': '1a62fa3b81471924d55a281049e401cd7eb3d4c7',\n",
       "  'title': 'JuMP: A Modeling Language for Mathematical Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Review',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 100,\n",
       "  'influentialCitationCount': 246,\n",
       "  'citationCount': 1422,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e4a69f63f19fa208df3c852c2a19eed619515210': {'arxiv_id': None,\n",
       "  's2_paperId': 'e4a69f63f19fa208df3c852c2a19eed619515210',\n",
       "  'title': 'Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot',\n",
       "  'abstract': None,\n",
       "  'venue': 'Autonomous Robots',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 71,\n",
       "  'influentialCitationCount': 29,\n",
       "  'citationCount': 802,\n",
       "  'publicationType': 'Not available'},\n",
       " '7b0e9f4bf1ce717bb017f9cb90fcbe244eff351a': {'arxiv_id': None,\n",
       "  's2_paperId': '7b0e9f4bf1ce717bb017f9cb90fcbe244eff351a',\n",
       "  'title': 'Efficient mixed-integer planning for UAVs in cluttered environments',\n",
       "  'abstract': 'We present a new approach to the design of smooth trajectories for quadrotor unmanned aerial vehicles (UAVs), which are free of collisions with obstacles along their entire length. To avoid the non-convex constraints normally required for obstacle-avoidance, we perform a mixed-integer optimization in which polynomial trajectories are assigned to convex regions which are known to be obstacle-free. Prior approaches have used the faces of the obstacles themselves to define these convex regions. We instead use IRIS, a recently developed technique for greedy convex segmentation [1], to pre-compute convex regions of safe space. This results in a substantially reduced number of integer variables, which improves the speed with which the optimization can be solved to its global optimum, even for tens or hundreds of obstacle faces. In addition, prior approaches have typically enforced obstacle avoidance at a finite set of sample or knot points. We introduce a technique based on sums-of-squares (SOS) programming that allows us to ensure that the entire piecewise polynomial trajectory is free of collisions using convex constraints. We demonstrate this technique in 2D and in 3D using a dynamical model in the Drake toolbox for Matlab [2].',\n",
       "  'venue': 'IEEE International Conference on Robotics and Automation',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 210,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fae855b6200ab1d50270ed538ae61458c3f1e717': {'arxiv_id': None,\n",
       "  's2_paperId': 'fae855b6200ab1d50270ed538ae61458c3f1e717',\n",
       "  'title': 'ETH Hardness for Densest-k-Subgraph with Perfect Completeness',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM-SIAM Symposium on Discrete Algorithms',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 53,\n",
       "  'publicationType': 'Not available'},\n",
       " '1a03194d28c1cf070b1b8233729f8756babccdd9': {'arxiv_id': None,\n",
       "  's2_paperId': '1a03194d28c1cf070b1b8233729f8756babccdd9',\n",
       "  'title': 'Mixed Integer Linear Programming Formulation Techniques',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Review',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 177,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 313,\n",
       "  'publicationType': 'Not available'},\n",
       " '7ae0f89dcca19f7c75286682a3dd3b4827b30108': {'arxiv_id': None,\n",
       "  's2_paperId': '7ae0f89dcca19f7c75286682a3dd3b4827b30108',\n",
       "  'title': 'Global optimization method for network design problem with stochastic user equilibrium',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 111,\n",
       "  'publicationType': 'Not available'},\n",
       " 'aec7fe9c97b36d904af8da180475267d55ef338f': {'arxiv_id': None,\n",
       "  's2_paperId': 'aec7fe9c97b36d904af8da180475267d55ef338f',\n",
       "  'title': 'Approximating the best Nash Equilibrium in no(log n)-time breaks the Exponential Time Hypothesis',\n",
       "  'abstract': None,\n",
       "  'venue': 'Electron. Colloquium Comput. Complex.',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 65,\n",
       "  'publicationType': 'Not available'},\n",
       " '0eac20ee2ebaede1381f20a276a0d8b6cdf73165': {'arxiv_id': None,\n",
       "  's2_paperId': '0eac20ee2ebaede1381f20a276a0d8b6cdf73165',\n",
       "  'title': 'Julia: A Fresh Approach to Numerical Computing',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Review',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 676,\n",
       "  'citationCount': 5408,\n",
       "  'publicationType': 'Not available'},\n",
       " '0e851ffd05fb16f152f91b2c39f2460b1777a467': {'arxiv_id': None,\n",
       "  's2_paperId': '0e851ffd05fb16f152f91b2c39f2460b1777a467',\n",
       "  'title': 'Footstep planning on uneven terrain with mixed-integer convex optimization',\n",
       "  'abstract': \"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].\",\n",
       "  'venue': 'IEEE-RAS International Conference on Humanoid Robots',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 22,\n",
       "  'citationCount': 334,\n",
       "  'publicationType': 'Not available'},\n",
       " '2ed1e7bab25e414bff8e6b29412d63d27b6712bf': {'arxiv_id': None,\n",
       "  's2_paperId': '2ed1e7bab25e414bff8e6b29412d63d27b6712bf',\n",
       "  'title': 'Incentive-Compatible Learning of Reserve Prices for Repeated Auctions',\n",
       "  'abstract': None,\n",
       "  'venue': 'Workshop on Internet and Network Economics',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 84,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 48,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e17a758133e96fcc2eae864c41b32f82eb4c8c64': {'arxiv_id': None,\n",
       "  's2_paperId': 'e17a758133e96fcc2eae864c41b32f82eb4c8c64',\n",
       "  'title': 'Mixed-integer linear methods for layout-optimization of screening systems in recovered paper production',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 97,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 21,\n",
       "  'publicationType': 'Not available'},\n",
       " '7787051231a92d98001ed73eab17e18b7cbfe8bc': {'arxiv_id': None,\n",
       "  's2_paperId': '7787051231a92d98001ed73eab17e18b7cbfe8bc',\n",
       "  'title': 'Computing in Operations Research Using Julia',\n",
       "  'abstract': None,\n",
       "  'venue': 'INFORMS journal on computing',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 62,\n",
       "  'citationCount': 343,\n",
       "  'publicationType': 'Not available'},\n",
       " '110519a1d6c874e618d41047e1390a99ceb4745c': {'arxiv_id': None,\n",
       "  's2_paperId': '110519a1d6c874e618d41047e1390a99ceb4745c',\n",
       "  'title': 'Learning Prices for Repeated Auctions with Strategic Buyers',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 149,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b92ba05d4d4285b28dcdeac4ea2438545077e957': {'arxiv_id': None,\n",
       "  's2_paperId': 'b92ba05d4d4285b28dcdeac4ea2438545077e957',\n",
       "  'title': 'Regret Minimization for Reserve Prices in Second-Price Auctions',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 168,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ca8efcf6ef46df6c09090b46382603aed3793391': {'arxiv_id': None,\n",
       "  's2_paperId': 'ca8efcf6ef46df6c09090b46382603aed3793391',\n",
       "  'title': 'Complexity of SAT Problems, Clone Theory and the Exponential Time Hypothesis',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM-SIAM Symposium on Discrete Algorithms',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 26,\n",
       "  'publicationType': 'Not available'},\n",
       " '47b422a5f4027ddc56289489a1b9b51b7fb82e34': {'arxiv_id': None,\n",
       "  's2_paperId': '47b422a5f4027ddc56289489a1b9b51b7fb82e34',\n",
       "  'title': 'The Exponential Time Hypothesis and the Parameterized Clique Problem',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Symposium on Parameterized and Exact Computation',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 10,\n",
       "  'publicationType': 'Not available'},\n",
       " '373471553fadd638de24fe73bb286cfe06a940e2': {'arxiv_id': None,\n",
       "  's2_paperId': '373471553fadd638de24fe73bb286cfe06a940e2',\n",
       "  'title': 'Mixed-integer quadratic program trajectory generation for heterogeneous quadrotor teams',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Robotics and Automation',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 285,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ca675e3bc0c9941157c22532802bf694991ffaee': {'arxiv_id': None,\n",
       "  's2_paperId': 'ca675e3bc0c9941157c22532802bf694991ffaee',\n",
       "  'title': 'Reserve Prices in Internet Advertising Auctions: A Field Experiment',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Political Economy',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 150,\n",
       "  'publicationType': 'Not available'},\n",
       " '342fe6a6338e73fd4d34c4f37f41e3bbad274dd2': {'arxiv_id': None,\n",
       "  's2_paperId': '342fe6a6338e73fd4d34c4f37f41e3bbad274dd2',\n",
       "  'title': 'Networks',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 248,\n",
       "  'citationCount': 6332,\n",
       "  'publicationType': 'Not available'},\n",
       " '05288a8f7e4ddc69fb23455d38140fc6e6f78c78': {'arxiv_id': None,\n",
       "  's2_paperId': '05288a8f7e4ddc69fb23455d38140fc6e6f78c78',\n",
       "  'title': 'A Comparison of Mixed - Integer Programming Models for Nonconvex Piecewise Linear Cost Minimization Problems',\n",
       "  'abstract': None,\n",
       "  'venue': 'Management Sciences',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 210,\n",
       "  'publicationType': 'Not available'},\n",
       " '10e3a9a8bca1a384cc3e7744e54dbdd4512c8d4e': {'arxiv_id': None,\n",
       "  's2_paperId': '10e3a9a8bca1a384cc3e7744e54dbdd4512c8d4e',\n",
       "  'title': 'Continuous-Action Q-Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Machine-mediated learning',\n",
       "  'year': 2002,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 142,\n",
       "  'publicationType': 'Not available'},\n",
       " '0dad8a5040861e2ee8e106be33d639bfa58d1584': {'arxiv_id': None,\n",
       "  's2_paperId': '0dad8a5040861e2ee8e106be33d639bfa58d1584',\n",
       "  'title': 'DC Programming: Overview',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1999,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 49,\n",
       "  'citationCount': 692,\n",
       "  'publicationType': 'Not available'},\n",
       " '94ebc9130e780c017e6e851c8c3d24b7c61c727d': {'arxiv_id': None,\n",
       "  's2_paperId': '94ebc9130e780c017e6e851c8c3d24b7c61c727d',\n",
       "  'title': 'Complexity of k-SAT',\n",
       "  'abstract': None,\n",
       "  'venue': 'Proceedings. Fourteenth Annual IEEE Conference on Computational Complexity (Formerly: Structure in Complexity Theory Conference) (Cat.No.99CB36317)',\n",
       "  'year': 1999,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 218,\n",
       "  'citationCount': 1394,\n",
       "  'publicationType': 'Not available'},\n",
       " '8b514b051db93dedce05d36846e1abb7fea4ba1a': {'arxiv_id': None,\n",
       "  's2_paperId': '8b514b051db93dedce05d36846e1abb7fea4ba1a',\n",
       "  'title': 'Disjunctive Programming: Properties of the Convex Hull of Feasible Points',\n",
       "  'abstract': None,\n",
       "  'venue': 'Discrete Applied Mathematics',\n",
       "  'year': 1998,\n",
       "  'referenceCount': 9,\n",
       "  'influentialCitationCount': 90,\n",
       "  'citationCount': 452,\n",
       "  'publicationType': 'Not available'},\n",
       " '2908628d4e61b98d05f1ba8f8b0d6265d3e08662': {'arxiv_id': None,\n",
       "  's2_paperId': '2908628d4e61b98d05f1ba8f8b0d6265d3e08662',\n",
       "  'title': 'Simulation of Hybrid Circuits in Constraint Logic Programming',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Joint Conference on Artificial Intelligence',\n",
       "  'year': 1989,\n",
       "  'referenceCount': 13,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 45,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e267e3b2dd4b625b78028a919cf8696773bfc00d': {'arxiv_id': None,\n",
       "  's2_paperId': 'e267e3b2dd4b625b78028a919cf8696773bfc00d',\n",
       "  'title': 'Disjunctive programming and a hierarchy of relaxations for discrete optimization problems',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1985,\n",
       "  'referenceCount': 9,\n",
       "  'influentialCitationCount': 53,\n",
       "  'citationCount': 448,\n",
       "  'publicationType': 'Not available'},\n",
       " '6288b34c730198afd122b174ef86e2d2842cb4e3': {'arxiv_id': None,\n",
       "  's2_paperId': '6288b34c730198afd122b174ef86e2d2842cb4e3',\n",
       "  'title': 'Modelling with Integer Variables.',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1984,\n",
       "  'referenceCount': 11,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 150,\n",
       "  'publicationType': 'Not available'},\n",
       " '500a334ce24838bad36b83dec0f8678416c91463': {'arxiv_id': None,\n",
       "  's2_paperId': '500a334ce24838bad36b83dec0f8678416c91463',\n",
       "  'title': 'University of Birmingham A tight algorithm for Strongly Connected Steiner Subgraph on two terminals with demands',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 8,\n",
       "  'publicationType': 'Not available'},\n",
       " '65f274ec920ce0024fbe09abc1692d55c9b5781a': {'arxiv_id': None,\n",
       "  's2_paperId': '65f274ec920ce0024fbe09abc1692d55c9b5781a',\n",
       "  'title': 'Learning Algorithms for Second-Price Auctions with Reserve',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 41,\n",
       "  'publicationType': 'Not available'},\n",
       " 'be321868f0af92ed2db2ef026a48557692e251e4': {'arxiv_id': None,\n",
       "  's2_paperId': 'be321868f0af92ed2db2ef026a48557692e251e4',\n",
       "  'title': 'Hardness of Easy Problems: Basing Hardness on Popular Conjectures such as the Strong Exponential Time Hypothesis (Invited Talk)',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Symposium on Parameterized and Exact Computation',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 145,\n",
       "  'publicationType': 'Not available'},\n",
       " '0c69892e2eb9638929b89b15268730df9b65ce52': {'arxiv_id': None,\n",
       "  's2_paperId': '0c69892e2eb9638929b89b15268730df9b65ce52',\n",
       "  'title': 'Dynamic Reserve Prices for Repeated Auctions: Learning from Bids - Working Paper.',\n",
       "  'abstract': None,\n",
       "  'venue': 'Workshop on Internet and Network Economics',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 48,\n",
       "  'publicationType': 'Not available'},\n",
       " '77042a24ac267ba3d486c9dc53693e03626c9b80': {'arxiv_id': None,\n",
       "  's2_paperId': '77042a24ac267ba3d486c9dc53693e03626c9b80',\n",
       "  'title': 'Lower bounds based on the Exponential Time Hypothesis',\n",
       "  'abstract': None,\n",
       "  'venue': 'Bull. EATCS',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 365,\n",
       "  'publicationType': 'Not available'},\n",
       " 'cfaa821e157ed83a4d2305d0c2d00a31664bba37': {'arxiv_id': None,\n",
       "  's2_paperId': 'cfaa821e157ed83a4d2305d0c2d00a31664bba37',\n",
       "  'title': 'Variable Disaggregation in Network Flow Problems with Piecewise Linear Costs',\n",
       "  'abstract': None,\n",
       "  'venue': 'Operational Research',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 89,\n",
       "  'publicationType': 'Not available'},\n",
       " '5ca8286ddee96980bc301c5c4035a4c2998dc8ea': {'arxiv_id': None,\n",
       "  's2_paperId': '5ca8286ddee96980bc301c5c4035a4c2998dc8ea',\n",
       "  'title': 'On the Computational Complexity of Combinatorial Problems',\n",
       "  'abstract': None,\n",
       "  'venue': 'Networks',\n",
       "  'year': 1975,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 636,\n",
       "  'publicationType': 'Not available'},\n",
       " '8000a4a63ac97ffb84917f910e2ce747e48d409f': {'arxiv_id': '1906.02367v2',\n",
       "  's2_paperId': '8000a4a63ac97ffb84917f910e2ce747e48d409f',\n",
       "  'title': 'Qsparse-Local-SGD: Distributed SGD With Quantization, Sparsification, and Local Computations',\n",
       "  'abstract': 'Communication bottleneck has been identified as a significant issue in distributed optimization of large-scale learning models. Recently, several approaches to mitigate this problem have been proposed, including different forms of gradient compression or computing local models and mixing them iteratively. In this paper, we propose Qsparse-local-SGD algorithm, which combines aggressive sparsification with quantization and local computation along with error compensation, by keeping track of the difference between the true and compressed gradients. We propose both synchronous and asynchronous implementations of Qsparse-local-SGD. We analyze convergence for Qsparse-local-SGD in the distributed setting for smooth non-convex and convex objective functions. We demonstrate that Qsparse-local-SGD converges at the same rate as vanilla distributed SGD for many important classes of sparsifiers and quantizers. We use Qsparse-local-SGD to train ResNet-50 on ImageNet and show that it results in significant savings over the state-of-the-art, in the number of bits transmitted to reach target accuracy.',\n",
       "  'venue': 'IEEE Journal on Selected Areas in Information Theory',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 43,\n",
       "  'citationCount': 398,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '1e13454c3388ccc2ae44ff307979256b09bed1bf': {'arxiv_id': '1905.03817v1',\n",
       "  's2_paperId': '1e13454c3388ccc2ae44ff307979256b09bed1bf',\n",
       "  'title': 'On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization',\n",
       "  'abstract': 'Recent developments on large-scale distributed machine learning applications, e.g., deep neural networks, benefit enormously from the advances in distributed non-convex optimization techniques, e.g., distributed Stochastic Gradient Descent (SGD). A series of recent works study the linear speedup property of distributed SGD variants with reduced communication. The linear speedup property enable us to scale out the computing capability by adding more computing nodes into our system. The reduced communication complexity is desirable since communication overhead is often the performance bottleneck in distributed systems. Recently, momentum methods are more and more widely adopted in training machine learning models and can often converge faster and generalize better. For example, many practitioners use distributed SGD with momentum to train deep neural networks with big data. However, it remains unclear whether any distributed momentum SGD possesses the same linear speedup property as distributed SGD and has reduced communication complexity. This paper fills the gap by considering a distributed communication efficient momentum SGD method and proving its linear speedup property.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 57,\n",
       "  'citationCount': 374,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '44b3b3bb40a9055eccdf86ea1702f6ae8b38934c': {'arxiv_id': None,\n",
       "  's2_paperId': '44b3b3bb40a9055eccdf86ea1702f6ae8b38934c',\n",
       "  'title': 'Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 78,\n",
       "  'citationCount': 496,\n",
       "  'publicationType': 'Not available'},\n",
       " '7c22a6a07e89461178b794681c675b209332ee15': {'arxiv_id': '1901.09847v2',\n",
       "  's2_paperId': '7c22a6a07e89461178b794681c675b209332ee15',\n",
       "  'title': 'Error Feedback Fixes SignSGD and other Gradient Compression Schemes',\n",
       "  'abstract': 'Sign-based algorithms (e.g. signSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers. We show simple convex counter-examples where signSGD does not converge to the optimum. Further, even when it does converge, signSGD may generalize poorly when compared with SGD. These issues arise because of the biased nature of the sign compression operator. We then show that using error-feedback, i.e. incorporating the error made by the compression operator into the next step, overcomes these issues. We prove that our algorithm EF-SGD with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions. Thus EF-SGD achieves gradient compression for free. Our experiments thoroughly substantiate the theory and show that error-feedback improves both convergence and generalization. Code can be found at \\\\url{this https URL}.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 81,\n",
       "  'citationCount': 487,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '2d8c43aa050203e2b49cd8021d0f65c7d2cca00e': {'arxiv_id': None,\n",
       "  's2_paperId': '2d8c43aa050203e2b49cd8021d0f65c7d2cca00e',\n",
       "  'title': 'The Convergence of Sparsified Gradient Methods',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 63,\n",
       "  'citationCount': 482,\n",
       "  'publicationType': 'Not available'},\n",
       " '38f1ef8ab96e5e0195abcd197bf6df47eb308e8a': {'arxiv_id': '1809.07599v2',\n",
       "  's2_paperId': '38f1ef8ab96e5e0195abcd197bf6df47eb308e8a',\n",
       "  'title': 'Sparsified SGD with Memory',\n",
       "  'abstract': 'Huge scale machine learning problems are nowadays tackled by distributed optimization algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far. In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory). That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the good scalability for distributed applications.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 138,\n",
       "  'citationCount': 727,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '7a96002fe623c7b138584b1161a9bf2eb0a41876': {'arxiv_id': None,\n",
       "  's2_paperId': '7a96002fe623c7b138584b1161a9bf2eb0a41876',\n",
       "  'title': 'Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 42,\n",
       "  'citationCount': 347,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e0e152748ea0badfbd798dfed3ac743abb58af26': {'arxiv_id': None,\n",
       "  's2_paperId': 'e0e152748ea0badfbd798dfed3ac743abb58af26',\n",
       "  'title': 'Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 69,\n",
       "  'citationCount': 586,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd29a248bfe7ab08eb99d841f5fd6287f64b8d394': {'arxiv_id': None,\n",
       "  's2_paperId': 'd29a248bfe7ab08eb99d841f5fd6287f64b8d394',\n",
       "  'title': 'Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 231,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b611636f3cfe7b9aa41a606bec1d9fa72e1359ae': {'arxiv_id': '1806.04090v3',\n",
       "  's2_paperId': 'b611636f3cfe7b9aa41a606bec1d9fa72e1359ae',\n",
       "  'title': 'ATOMO: Communication-efficient Learning via Atomic Sparsification',\n",
       "  'abstract': 'Distributed model training suffers from communication overheads due to frequent gradient updates transmitted between compute nodes. To mitigate these overheads, several studies propose the use of sparsified stochastic gradients. We argue that these are facets of a general sparsification method that can operate on any possible atomic decomposition. Notable examples include element-wise, singular value, and Fourier decompositions. We present ATOMO, a general framework for atomic sparsification of stochastic gradients. Given a gradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random unbiased sparsification of the atoms minimizing variance. We show that methods such as QSGD and TernGrad are special cases of ATOMO and show that sparsifiying gradients in their singular value decomposition (SVD), rather than the coordinate-wise one, can lead to significantly faster distributed training.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 31,\n",
       "  'citationCount': 347,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '7cfa76a82be96c74b2eff514265b7fd271a179cd': {'arxiv_id': None,\n",
       "  's2_paperId': '7cfa76a82be96c74b2eff514265b7fd271a179cd',\n",
       "  'title': 'Local SGD Converges Fast and Communicates Little',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 149,\n",
       "  'citationCount': 1027,\n",
       "  'publicationType': 'Not available'},\n",
       " '0192e32c1c158917eb92cb21431a63c93b2893ee': {'arxiv_id': None,\n",
       "  's2_paperId': '0192e32c1c158917eb92cb21431a63c93b2893ee',\n",
       "  'title': 'Communication Compression for Decentralized Training',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 41,\n",
       "  'citationCount': 269,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e2c8726d092aea573e69f5b0a2654225883cfacf': {'arxiv_id': None,\n",
       "  's2_paperId': 'e2c8726d092aea573e69f5b0a2654225883cfacf',\n",
       "  'title': 'Horovod: fast and easy distributed deep learning in TensorFlow',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 190,\n",
       "  'citationCount': 1188,\n",
       "  'publicationType': 'Not available'},\n",
       " '97884ff15e0a4e83f534b7b13979e519d1c50a54': {'arxiv_id': None,\n",
       "  's2_paperId': '97884ff15e0a4e83f534b7b13979e519d1c50a54',\n",
       "  'title': 'signSGD: compressed optimisation for non-convex problems',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 158,\n",
       "  'citationCount': 1009,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd17a3e9f34125ba6454e365ffe403ce8a91f2632': {'arxiv_id': None,\n",
       "  's2_paperId': 'd17a3e9f34125ba6454e365ffe403ce8a91f2632',\n",
       "  'title': 'SGD and Hogwild! Convergence Without the Bounded Gradients Assumption',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 222,\n",
       "  'publicationType': 'Not available'},\n",
       " '92495abbac86394cb759bec15a763dbf49a8e590': {'arxiv_id': None,\n",
       "  's2_paperId': '92495abbac86394cb759bec15a763dbf49a8e590',\n",
       "  'title': 'Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 162,\n",
       "  'citationCount': 1374,\n",
       "  'publicationType': 'Not available'},\n",
       " '1caff87770b1cbddcf94edc0a9b1bce029324765': {'arxiv_id': None,\n",
       "  's2_paperId': '1caff87770b1cbddcf94edc0a9b1bce029324765',\n",
       "  'title': 'Gradient Sparsification for Communication-Efficient Distributed Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 36,\n",
       "  'citationCount': 517,\n",
       "  'publicationType': 'Not available'},\n",
       " '7445813df25919d995468e9e0e083d057782f6ad': {'arxiv_id': None,\n",
       "  's2_paperId': '7445813df25919d995468e9e0e083d057782f6ad',\n",
       "  'title': 'Stochastic, Distributed and Federated Optimization for Machine Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 206,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 37,\n",
       "  'publicationType': 'Not available'},\n",
       " '5690cae43495be52bab835023e34997ea2784abf': {'arxiv_id': None,\n",
       "  's2_paperId': '5690cae43495be52bab835023e34997ea2784abf',\n",
       "  'title': 'Training Quantized Nets: A Deeper Understanding',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 208,\n",
       "  'publicationType': 'Not available'},\n",
       " '4bdb91a6e47385292ab7a18e8901a6a25f50cc6b': {'arxiv_id': '1705.07878v6',\n",
       "  's2_paperId': '4bdb91a6e47385292ab7a18e8901a6a25f50cc6b',\n",
       "  'title': 'TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning',\n",
       "  'abstract': 'High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1,0,1}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet does not incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 111,\n",
       "  'citationCount': 975,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'e8437e3b32f091f62f3796556435e139db130f90': {'arxiv_id': None,\n",
       "  's2_paperId': 'e8437e3b32f091f62f3796556435e139db130f90',\n",
       "  'title': 'Sparse Communication for Distributed Gradient Descent',\n",
       "  'abstract': 'We make distributed stochastic gradient descent faster by exchanging sparse updates instead of dense updates. Gradient updates are positively skewed as most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices. This method can be combined with quantization to further improve the compression. We explore different configurations and apply them to neural machine translation and MNIST image classification tasks. Most configurations work on MNIST, whereas different configurations reduce convergence rate on the more complex translation task. Our experiments show that we can achieve up to 49% speed up on MNIST and 22% on NMT without damaging the final accuracy or BLEU.',\n",
       "  'venue': 'Conference on Empirical Methods in Natural Language Processing',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 80,\n",
       "  'citationCount': 717,\n",
       "  'publicationType': 'Not available'},\n",
       " 'dbcee766687b947548803d41c66e7962f2aaebf7': {'arxiv_id': '1611.00429v3',\n",
       "  's2_paperId': 'dbcee766687b947548803d41c66e7962f2aaebf7',\n",
       "  'title': 'Distributed Mean Estimation with Limited Communication',\n",
       "  'abstract': \"Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation. Unlike previous works, we make no probabilistic assumptions on the data. We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error (MSE) of Θ (d/n) and uses a constant number of bits per dimension per client. We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to O((log d)/n) and a better coding strategy further reduces the error to O(1/n). We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost. We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd's algorithm for k-means and power iteration for PCA.\",\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 35,\n",
       "  'citationCount': 358,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'c06f66ae93add37765997dab3467930600f4a9a2': {'arxiv_id': None,\n",
       "  's2_paperId': 'c06f66ae93add37765997dab3467930600f4a9a2',\n",
       "  'title': 'Decentralized consensus optimization with asynchrony and delays',\n",
       "  'abstract': None,\n",
       "  'venue': 'Asilomar Conference on Signals, Systems and Computers',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 124,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a3b4537344ddcd32be3a9b8c0882a2eb769983b4': {'arxiv_id': '1610.02132v4',\n",
       "  's2_paperId': 'a3b4537344ddcd32be3a9b8c0882a2eb769983b4',\n",
       "  'title': 'QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks',\n",
       "  'abstract': 'Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compression heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal. \\nIn this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks. \\nIn particular, experiments show that gradient quantization applied to training of deep neural networks for image classification and automated speech recognition can lead to significant reductions in communication cost, and end-to-end training time. For instance, on 16 GPUs, we are able to train a ResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show that there exist generic parameter settings under which all known network architectures preserve or slightly improve their full accuracy when using quantization.',\n",
       "  'venue': '',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 84,\n",
       "  'citationCount': 428,\n",
       "  'publicationType': None},\n",
       " '907c94f0cd6ddd1da2ee027edba9dbe730285100': {'arxiv_id': None,\n",
       "  's2_paperId': '907c94f0cd6ddd1da2ee027edba9dbe730285100',\n",
       "  'title': 'Parallel SGD: When does averaging help?',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 108,\n",
       "  'publicationType': 'Not available'},\n",
       " '4954fa180728932959997a4768411ff9136aac81': {'arxiv_id': None,\n",
       "  's2_paperId': '4954fa180728932959997a4768411ff9136aac81',\n",
       "  'title': 'TensorFlow: A system for large-scale machine learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'USENIX Symposium on Operating Systems Design and Implementation',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 91,\n",
       "  'influentialCitationCount': 2068,\n",
       "  'citationCount': 18101,\n",
       "  'publicationType': 'Not available'},\n",
       " '5536b61896dd4be21f4ea1ed486b7c9950b23527': {'arxiv_id': None,\n",
       "  's2_paperId': '5536b61896dd4be21f4ea1ed486b7c9950b23527',\n",
       "  'title': 'Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Acoustics, Speech, and Signal Processing',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 22,\n",
       "  'citationCount': 142,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd1dbf643447405984eeef098b1b320dee0b3b8a7': {'arxiv_id': None,\n",
       "  's2_paperId': 'd1dbf643447405984eeef098b1b320dee0b3b8a7',\n",
       "  'title': 'Communication-Efficient Learning of Deep Networks from Decentralized Data',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 3838,\n",
       "  'citationCount': 16503,\n",
       "  'publicationType': 'Not available'},\n",
       " '21ffd44e545b82c843c89c1793e90c8a8817fc1d': {'arxiv_id': None,\n",
       "  's2_paperId': '21ffd44e545b82c843c89c1793e90c8a8817fc1d',\n",
       "  'title': 'Top-k Multiclass SVM',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 90,\n",
       "  'publicationType': 'Not available'},\n",
       " '07ecaee0a11dc8cb95e7c97f497a0b231bbf9460': {'arxiv_id': None,\n",
       "  's2_paperId': '07ecaee0a11dc8cb95e7c97f497a0b231bbf9460',\n",
       "  'title': 'Perturbed Iterate Analysis for Asynchronous Stochastic Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Journal on Optimization',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 45,\n",
       "  'influentialCitationCount': 38,\n",
       "  'citationCount': 230,\n",
       "  'publicationType': 'Not available'},\n",
       " '138fda7e6a92ce0be455901bdfc41c9b76d6e060': {'arxiv_id': None,\n",
       "  's2_paperId': '138fda7e6a92ce0be455901bdfc41c9b76d6e060',\n",
       "  'title': 'Iterative parameter mixing for distributed large-margin training of structured predictors for natural language processing',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 202,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 19,\n",
       "  'publicationType': 'Not available'},\n",
       " '3439a127e45fb763881f03ef3ec735a1db0e0ccc': {'arxiv_id': None,\n",
       "  's2_paperId': '3439a127e45fb763881f03ef3ec735a1db0e0ccc',\n",
       "  'title': '1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs',\n",
       "  'abstract': None,\n",
       "  'venue': 'Interspeech',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 31,\n",
       "  'influentialCitationCount': 109,\n",
       "  'citationCount': 977,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a310cf55afffe76aa627b79ae8744c2a90f27818': {'arxiv_id': None,\n",
       "  's2_paperId': 'a310cf55afffe76aa627b79ae8744c2a90f27818',\n",
       "  'title': 'Information-theoretic lower bounds for distributed statistical estimation with communication constraints',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 269,\n",
       "  'publicationType': 'Not available'},\n",
       " '2820973cf69daba62547f76de082bed5e1c646e6': {'arxiv_id': None,\n",
       "  's2_paperId': '2820973cf69daba62547f76de082bed5e1c646e6',\n",
       "  'title': 'Communication-efficient algorithms for statistical optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Conference on Decision and Control',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 63,\n",
       "  'citationCount': 538,\n",
       "  'publicationType': 'Not available'},\n",
       " '7658cecad68afc970f18cadbf6390439b17def87': {'arxiv_id': None,\n",
       "  's2_paperId': '7658cecad68afc970f18cadbf6390439b17def87',\n",
       "  'title': 'Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 108,\n",
       "  'citationCount': 762,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e08f14111728901945f8c3b384b2f75746935e0d': {'arxiv_id': None,\n",
       "  's2_paperId': 'e08f14111728901945f8c3b384b2f75746935e0d',\n",
       "  'title': 'Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 81,\n",
       "  'citationCount': 757,\n",
       "  'publicationType': 'Not available'},\n",
       " '36f49b05d764bf5c10428b082c2d96c13c4203b9': {'arxiv_id': None,\n",
       "  's2_paperId': '36f49b05d764bf5c10428b082c2d96c13c4203b9',\n",
       "  'title': 'Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 256,\n",
       "  'citationCount': 2265,\n",
       "  'publicationType': 'Not available'},\n",
       " 'da0877799c8daab985853c0aeed7c04f987bad4a': {'arxiv_id': None,\n",
       "  's2_paperId': 'da0877799c8daab985853c0aeed7c04f987bad4a',\n",
       "  'title': 'Robust Stochastic Approximation Approach to Stochastic Programming',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Journal on Optimization',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 77,\n",
       "  'citationCount': 569,\n",
       "  'publicationType': 'Not available'},\n",
       " '9691f67f5075bde2fd70da0135a4a70f25ef042b': {'arxiv_id': None,\n",
       "  's2_paperId': '9691f67f5075bde2fd70da0135a4a70f25ef042b',\n",
       "  'title': 'Pegasos: primal estimated sub-gradient solver for SVM',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 273,\n",
       "  'citationCount': 2290,\n",
       "  'publicationType': 'Not available'},\n",
       " '916ceefae4b11dadc3ee754ce590381c568c90de': {'arxiv_id': None,\n",
       "  's2_paperId': '916ceefae4b11dadc3ee754ce590381c568c90de',\n",
       "  'title': 'A direct adaptive method for faster backpropagation learning: the RPROP algorithm',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Neural Networks',\n",
       "  'year': 1993,\n",
       "  'referenceCount': 6,\n",
       "  'influentialCitationCount': 334,\n",
       "  'citationCount': 4666,\n",
       "  'publicationType': 'Not available'},\n",
       " '0636343ff39c9e2c47d2a8f048814103dc66a6a8': {'arxiv_id': None,\n",
       "  's2_paperId': '0636343ff39c9e2c47d2a8f048814103dc66a6a8',\n",
       "  'title': 'On the design of gradient algorithms for digitally implemented adaptive filters',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1973,\n",
       "  'referenceCount': 5,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 128,\n",
       "  'publicationType': 'Not available'},\n",
       " '34ddd8865569c2c32dec9bf7ffc817ff42faaa01': {'arxiv_id': None,\n",
       "  's2_paperId': '34ddd8865569c2c32dec9bf7ffc817ff42faaa01',\n",
       "  'title': 'A Stochastic Approximation Method',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1951,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 1016,\n",
       "  'citationCount': 9148,\n",
       "  'publicationType': 'Not available'},\n",
       " '9ee76c41dd161df75cb50ac06d2868afec63b0db': {'arxiv_id': None,\n",
       "  's2_paperId': '9ee76c41dd161df75cb50ac06d2868afec63b0db',\n",
       "  'title': 'Scalable distributed DNN training using commodity GPU cloud computing',\n",
       "  'abstract': None,\n",
       "  'venue': 'Interspeech',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 45,\n",
       "  'citationCount': 549,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c185bdce2cafdf25917ba1be5b2db16ab6669373': {'arxiv_id': None,\n",
       "  's2_paperId': 'c185bdce2cafdf25917ba1be5b2db16ab6669373',\n",
       "  'title': 'Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 22,\n",
       "  'citationCount': 207,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fbc6562814e08e416e28a268ce7beeaa3d0708c8': {'arxiv_id': None,\n",
       "  's2_paperId': 'fbc6562814e08e416e28a268ce7beeaa3d0708c8',\n",
       "  'title': 'Large-Scale Machine Learning with Stochastic Gradient Descent',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Computational Statistics',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 726,\n",
       "  'citationCount': 5766,\n",
       "  'publicationType': 'Not available'},\n",
       " '162d958ff885f1462aeda91cd72582323fd6a1f4': {'arxiv_id': None,\n",
       "  's2_paperId': '162d958ff885f1462aeda91cd72582323fd6a1f4',\n",
       "  'title': 'Gradient-based learning applied to document recognition',\n",
       "  'abstract': None,\n",
       "  'venue': 'Proceedings of the IEEE',\n",
       "  'year': 1998,\n",
       "  'referenceCount': 150,\n",
       "  'influentialCitationCount': 6008,\n",
       "  'citationCount': 53222,\n",
       "  'publicationType': 'Not available'},\n",
       " 'aa93f08487ec29f0129ba09a1b70c96aa7a115c1': {'arxiv_id': '2302.04262v3',\n",
       "  's2_paperId': 'aa93f08487ec29f0129ba09a1b70c96aa7a115c1',\n",
       "  'title': 'Algorithmic Collective Action in Machine Learning',\n",
       "  'abstract': \"We initiate a principled study of algorithmic collective action on digital platforms that deploy machine learning algorithms. We propose a simple theoretical model of a collective interacting with a firm's learning algorithm. The collective pools the data of participating individuals and executes an algorithmic strategy by instructing participants how to modify their own data to achieve a collective goal. We investigate the consequences of this model in three fundamental learning-theoretic settings: the case of a nonparametric optimal learning algorithm, a parametric risk minimizer, and gradient-based optimization. In each setting, we come up with coordinated algorithmic strategies and characterize natural success criteria as a function of the collective's size. Complementing our theory, we conduct systematic experiments on a skill classification task involving tens of thousands of resumes from a gig platform for freelancers. Through more than two thousand model training runs of a BERT-like language model, we see a striking correspondence emerge between our empirical observations and the predictions made by our theory. Taken together, our theory and experiments broadly support the conclusion that algorithmic collectives of exceedingly small fractional size can exert significant control over a platform's learning algorithm.\",\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 20,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '9e6988c7a64fc983819e4bc04e5acf298aafb92c': {'arxiv_id': None,\n",
       "  's2_paperId': '9e6988c7a64fc983819e4bc04e5acf298aafb92c',\n",
       "  'title': 'Online Algorithmic Recourse by Collective Action',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 3,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b3ca486c7655ec09612855d716ed8e8d1b536579': {'arxiv_id': None,\n",
       "  's2_paperId': 'b3ca486c7655ec09612855d716ed8e8d1b536579',\n",
       "  'title': 'Exponential Families in Theory and Practice',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 23,\n",
       "  'publicationType': 'Not available'},\n",
       " '2e295f9cc799997b7ecec75e4795c1701b9cd98c': {'arxiv_id': None,\n",
       "  's2_paperId': '2e295f9cc799997b7ecec75e4795c1701b9cd98c',\n",
       "  'title': 'The organizational psychology of gig work: An integrative conceptual review.',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Applied Psychology',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 97,\n",
       "  'publicationType': 'Not available'},\n",
       " '15f87ce1634093c8760e534bf2dd5daf66c7b54b': {'arxiv_id': None,\n",
       "  's2_paperId': '15f87ce1634093c8760e534bf2dd5daf66c7b54b',\n",
       "  'title': 'Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 89,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 96,\n",
       "  'publicationType': 'Not available'},\n",
       " '0301917931b8e8858e7c1864e3bb73b4dc0590d2': {'arxiv_id': None,\n",
       "  's2_paperId': '0301917931b8e8858e7c1864e3bb73b4dc0590d2',\n",
       "  'title': 'A Comprehensive Survey on Poisoning Attacks and Countermeasures in Machine Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM Computing Surveys',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 168,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 180,\n",
       "  'publicationType': 'Not available'},\n",
       " '97fcf1696b55067b1167400739cd5bf2c59152c0': {'arxiv_id': None,\n",
       "  's2_paperId': '97fcf1696b55067b1167400739cd5bf2c59152c0',\n",
       "  'title': 'Adversarial Scrutiny of Evidentiary Statistical Software',\n",
       "  'abstract': None,\n",
       "  'venue': 'Conference on Fairness, Accountability and Transparency',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 115,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 5,\n",
       "  'publicationType': 'Not available'},\n",
       " '17faec330fd133ab91ce32b71903d5b21c82ce4b': {'arxiv_id': '2202.08578v2',\n",
       "  's2_paperId': '17faec330fd133ab91ce32b71903d5b21c82ce4b',\n",
       "  'title': 'An Equivalence Between Data Poisoning and Byzantine Gradient Attacks',\n",
       "  'abstract': 'To study the resilience of distributed learning, the “Byzantine” literature considers a strong threat model where workers can report arbitrary gradients to the parameter server. Whereas this model helped obtain several fundamental results, it has sometimes been considered unrealistic, when the workers are mostly trustworthy machines. In this paper, we show a surprising equivalence between this model and data poisoning, a threat considered much more realistic. More speciﬁ-cally, we prove that every gradient attack can be reduced to data poisoning, in any personalized federated learning system with PAC guarantees (which we show are both desirable and realis-tic). This equivalence makes it possible to obtain new impossibility results on the resilience of any “robust” learning algorithm to data poisoning in highly heterogeneous applications, as corollaries of existing impossibility theorems on Byzantine machine learning. Moreover, using our equivalence, we derive a practical attack that we show (theoretically and empirically) can be very effective against classical personalized federated learning models.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 89,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 24,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'acaa400363effcbaaf91eb50065bc902507ad4ec': {'arxiv_id': None,\n",
       "  's2_paperId': 'acaa400363effcbaaf91eb50065bc902507ad4ec',\n",
       "  'title': 'The emergence of algorithmic solidarity: unveiling mutual aid practices and resistance among Chinese delivery workers',\n",
       "  'abstract': 'This study explores how Chinese riders game the algorithm-mediated governing system of food delivery service platforms and how they mobilize WeChat to build solidarity networks to assist each other and better cope with the platform economy. We rely on 12 interviews with Chinese riders from 4 platforms (Meituan, Eleme, SF Express and Flash EX) in 5 cities, and draw on a 4-month online observation of 7 private WeChat groups. The article provides a detailed account of the gamification ranking and competition techniques employed by delivery platforms to drive the riders to achieve efficiency and productivity gains. Then, it critically explores how Chinese riders adapt and react to the algorithmic systems that govern their work by setting up private WeChat groups and developing everyday practices of resilience and resistance. This study demonstrates that Chinese riders working for food delivery platforms incessantly create a complex repertoire of tactics and develop hidden transcripts to resist the algorithmic control of digital platforms.',\n",
       "  'venue': 'Media International Australia',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 30,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd2b43c4ef321fb88c8b46f9edded6cc836b2b7a0': {'arxiv_id': None,\n",
       "  's2_paperId': 'd2b43c4ef321fb88c8b46f9edded6cc836b2b7a0',\n",
       "  'title': 'Expanding the Locus of Resistance: Understanding the Co-constitution of Control and Resistance in the Gig Economy',\n",
       "  'abstract': None,\n",
       "  'venue': 'Organization science (Providence, R.I.)',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 136,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 127,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a04633398182ecec1a475c75352a18ec4d80d5f9': {'arxiv_id': None,\n",
       "  's2_paperId': 'a04633398182ecec1a475c75352a18ec4d80d5f9',\n",
       "  'title': 'An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences',\n",
       "  'abstract': 'Together with impressive advances touching every aspect of our society, AI technology based on Deep Neural Networks (DNN) is bringing increasing security concerns. While attacks operating at test time have monopolised the initial attention of researchers, backdoor attacks, exploiting the possibility of corrupting DNN models by interfering with the training process, represent a further serious threat undermining the dependability of AI techniques. In backdoor attacks, the attacker corrupts the training data to induce an erroneous behaviour at test time. Test-time errors, however, are activated only in the presence of a triggering event. In this way, the corrupted network continues to work as expected for regular inputs, and the malicious behaviour occurs only when the attacker decides to activate the backdoor hidden within the network. Recently, backdoor attacks have been an intense research domain focusing on both the development of new classes of attacks, and the proposal of possible countermeasures. The goal of this overview is to review the works published until now, classifying the different types of attacks and defences proposed so far. The classification guiding the analysis is based on the amount of control that the attacker has on the training process, and the capability of the defender to verify the integrity of the data used for training, and to monitor the operations of the DNN at training and test time. Hence, the proposed analysis is suited to highlight the strengths and weaknesses of both attacks and defences with reference to the application scenarios they are operating in.',\n",
       "  'venue': 'IEEE Open Journal of Signal Processing',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 125,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 62,\n",
       "  'publicationType': 'Not available'},\n",
       " '289671e30d48e374d3c907fe16e44d3bed2ec195': {'arxiv_id': '2109.00685v3',\n",
       "  's2_paperId': '289671e30d48e374d3c907fe16e44d3bed2ec195',\n",
       "  'title': 'Excess Capacity and Backdoor Poisoning',\n",
       "  'abstract': 'A backdoor data poisoning attack is an adversarial attack wherein the attacker injects several watermarked, mislabeled training examples into a training set. The watermark does not impact the test-time performance of the model on typical data; however, the model reliably errs on watermarked examples. To gain a better foundational understanding of backdoor data poisoning attacks, we present a formal theoretical framework within which one can discuss backdoor data poisoning attacks for classification problems. We then use this to analyze important statistical and computational issues surrounding these attacks. On the statistical front, we identify a parameter we call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows us to argue about the robustness of several natural learning problems to backdoor attacks. Our results favoring the attacker involve presenting explicit constructions of backdoor attacks, and our robustness results show that some natural problem settings cannot yield successful backdoor attacks. From a computational standpoint, we show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. We then show that under similar assumptions, two closely related problems we call backdoor filtering and robust generalization are nearly equivalent. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 23,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '52b9897815f6426929bdedf32509f79c8dff4613': {'arxiv_id': None,\n",
       "  's2_paperId': '52b9897815f6426929bdedf32509f79c8dff4613',\n",
       "  'title': 'Who Leads and Who Follows in Strategic Classification?',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 49,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a98af4d48c9d5e981aa6982065e49e46905baed2': {'arxiv_id': None,\n",
       "  's2_paperId': 'a98af4d48c9d5e981aa6982065e49e46905baed2',\n",
       "  'title': 'After the Gig: How the Sharing Economy Got Hijacked and How to Win It Back',\n",
       "  'abstract': None,\n",
       "  'venue': 'Contributions to Political Economy',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 81,\n",
       "  'publicationType': 'Not available'},\n",
       " '8e93dc84fb73088913327c54c934fd64d1206ce8': {'arxiv_id': None,\n",
       "  's2_paperId': '8e93dc84fb73088913327c54c934fd64d1206ce8',\n",
       "  'title': \"Adversarial for Good? How the Adversarial ML Community's Values Impede Socially Beneficial Uses of Attacks\",\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 112,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 4,\n",
       "  'publicationType': 'Not available'},\n",
       " '6f2153affc62483f49831a860b28faf5f871a4c9': {'arxiv_id': None,\n",
       "  's2_paperId': '6f2153affc62483f49831a860b28faf5f871a4c9',\n",
       "  'title': 'Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions',\n",
       "  'abstract': 'Backdoor attacks inject poisoning samples during training, with the goal of forcing a machine learning model to output an attacker-chosen class when presented with a specific trigger at test time. Although backdoor attacks have been demonstrated in a variety of settings and against different models, the factors affecting their effectiveness are still not well understood. In this work, we provide a unifying framework to study the process of backdoor learning under the lens of incremental learning and influence functions. We show that the effectiveness of backdoor attacks depends on (i) the complexity of the learning algorithm, controlled by its hyperparameters; (ii) the fraction of backdoor samples injected into the training set; and (iii) the size and visibility of the backdoor trigger. These factors affect how fast a model learns to correlate the presence of the backdoor trigger with the target class. Our analysis unveils the intriguing existence of a region in the hyperparameter space in which the accuracy of clean test samples is still high while backdoor attacks are ineffective, thereby suggesting novel criteria to improve existing defenses.',\n",
       "  'venue': 'International Journal of Machine Learning and Cybernetics',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 9,\n",
       "  'publicationType': 'Not available'},\n",
       " '6f59cb30a6070df32c2c30888e56359ab0d94402': {'arxiv_id': None,\n",
       "  's2_paperId': '6f59cb30a6070df32c2c30888e56359ab0d94402',\n",
       "  'title': 'Can \"Conscious Data Contribution\" Help Users to Exert \"Data Leverage\" Against Technology Companies?',\n",
       "  'abstract': None,\n",
       "  'venue': 'Proc. ACM Hum. Comput. Interact.',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 75,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 9,\n",
       "  'publicationType': 'Not available'},\n",
       " '47382f2966e9cbcc28f379df00bda11f8881383f': {'arxiv_id': None,\n",
       "  's2_paperId': '47382f2966e9cbcc28f379df00bda11f8881383f',\n",
       "  'title': 'The Invisible Cage: Workers’ Reactivity to Opaque Algorithmic Evaluations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Administrative Science Quarterly',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 99,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 163,\n",
       "  'publicationType': 'Not available'},\n",
       " '73263d9c1ce3e0a105aa9d79d6a87d111d0465e3': {'arxiv_id': None,\n",
       "  's2_paperId': '73263d9c1ce3e0a105aa9d79d6a87d111d0465e3',\n",
       "  'title': 'LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 22,\n",
       "  'citationCount': 134,\n",
       "  'publicationType': 'Not available'},\n",
       " 'cb79fbaf2e446e1dc25d8579d748ac7042ff7266': {'arxiv_id': None,\n",
       "  's2_paperId': 'cb79fbaf2e446e1dc25d8579d748ac7042ff7266',\n",
       "  'title': 'Data Leverage: A Framework for Empowering the Public in its Relationship with Technology Companies',\n",
       "  'abstract': None,\n",
       "  'venue': 'Conference on Fairness, Accountability and Transparency',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 142,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 53,\n",
       "  'publicationType': 'Not available'},\n",
       " '8dc712493df0a46fef830a6c3be64899880100c4': {'arxiv_id': None,\n",
       "  's2_paperId': '8dc712493df0a46fef830a6c3be64899880100c4',\n",
       "  'title': \"Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching\",\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 70,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 212,\n",
       "  'publicationType': 'Not available'},\n",
       " '474f8d11bd3ba72d6baaccfb145c05ea94ddf522': {'arxiv_id': None,\n",
       "  's2_paperId': '474f8d11bd3ba72d6baaccfb145c05ea94ddf522',\n",
       "  'title': 'Skills prediction based on multi-label resume classification using CNN with model predictions explanation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural computing & applications (Print)',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 41,\n",
       "  'publicationType': 'Not available'},\n",
       " '7b63572539245ba251d42a11b27e4cd7a385f75c': {'arxiv_id': None,\n",
       "  's2_paperId': '7b63572539245ba251d42a11b27e4cd7a385f75c',\n",
       "  'title': 'Dependence and precarity in the platform economy',\n",
       "  'abstract': None,\n",
       "  'venue': 'Theory and society',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 126,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 282,\n",
       "  'publicationType': 'Not available'},\n",
       " '271690a646b29b26e82362522b572b4a2481b0f2': {'arxiv_id': None,\n",
       "  's2_paperId': '271690a646b29b26e82362522b572b4a2481b0f2',\n",
       "  'title': 'What Do Platforms Do? Understanding the Gig Economy',\n",
       "  'abstract': None,\n",
       "  'venue': 'The Annual review of sociology',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 152,\n",
       "  'influentialCitationCount': 35,\n",
       "  'citationCount': 661,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b8332c54fba445b81a757b5dc96d4eeeecac2d9d': {'arxiv_id': None,\n",
       "  's2_paperId': 'b8332c54fba445b81a757b5dc96d4eeeecac2d9d',\n",
       "  'title': 'Backdoor smoothing: Demystifying backdoor attacks on deep neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computers & security',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 8,\n",
       "  'publicationType': 'Not available'},\n",
       " '755a392fe813c9ba3282f60c0e1f1ec81e68f263': {'arxiv_id': None,\n",
       "  's2_paperId': '755a392fe813c9ba3282f60c0e1f1ec81e68f263',\n",
       "  'title': 'Fawkes: Protecting Privacy against Unauthorized Deep Learning Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'USENIX Security Symposium',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 72,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 221,\n",
       "  'publicationType': 'Not available'},\n",
       " 'dfc86a91fc79c57de28b865ee5cf81a2475f5878': {'arxiv_id': None,\n",
       "  's2_paperId': 'dfc86a91fc79c57de28b865ee5cf81a2475f5878',\n",
       "  'title': 'Politics of Adversarial Machine Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Social Science Research Network',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 18,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f7c196d7c088290a15ed881c50ce70da1feca0b0': {'arxiv_id': None,\n",
       "  's2_paperId': 'f7c196d7c088290a15ed881c50ce70da1feca0b0',\n",
       "  'title': 'The Gig Economy: A Critical Introduction',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 221,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a54b56af24bb4873ed0163b77df63b92bd018ddc': {'arxiv_id': None,\n",
       "  's2_paperId': 'a54b56af24bb4873ed0163b77df63b92bd018ddc',\n",
       "  'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 986,\n",
       "  'citationCount': 7063,\n",
       "  'publicationType': 'Not available'},\n",
       " '15cd260d57c5f8f47d638c43705498e7cd9cec3c': {'arxiv_id': None,\n",
       "  's2_paperId': '15cd260d57c5f8f47d638c43705498e7cd9cec3c',\n",
       "  'title': 'Weapons of the Chic: Instagram Influencer Engagement Pods as Practices of Resistance to Instagram Platform Labor',\n",
       "  'abstract': 'This article examines the phenomenon of Instagram influencer “engagement pods” as an emergent form of resistance that responds to the reconfigured working conditions of platformized cultural production. Engagement pods are grassroots communities that agree to mutually like, comment on, share, or otherwise engage with each other’s posts, no matter the content, to game Instagram’s algorithm into prioritizing the participants’ content and show it to a broader audience. I argue that engagement pods represent a response to the material conditions of platformized cultural production on Instagram, where proprietary curation algorithms wrest knowledge and control of the labor process from producers. Cooperative algorithm hacking of this sort, although quite distinct from traditional organizing strategies, responds to the coercive force of the “threat of invisibility” that necessitates constant data production. They represent a collective attempt to exert some control over their “conditions of presence-to-others” and, in so doing, combat precarity and protect wages in the field. In a post-industrial economy where traditional models of labor organizing have struggled to address the conditions of platformized cultural work, I argue that the unusual phenomenon of Instagram engagement pods represents an organic form of worker resistance that responds to the unique conditions of these workers.',\n",
       "  'venue': 'Social Media + Society',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 70,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 113,\n",
       "  'publicationType': 'Not available'},\n",
       " '41cbffad975874060d643c36c8bdb5c72637564e': {'arxiv_id': None,\n",
       "  's2_paperId': '41cbffad975874060d643c36c8bdb5c72637564e',\n",
       "  'title': '“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies',\n",
       "  'abstract': None,\n",
       "  'venue': 'The Web Conference',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 61,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 23,\n",
       "  'publicationType': 'Not available'},\n",
       " '47f936331872d75df74473883bb65068c14fa7da': {'arxiv_id': None,\n",
       "  's2_paperId': '47f936331872d75df74473883bb65068c14fa7da',\n",
       "  'title': 'Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 63,\n",
       "  'citationCount': 664,\n",
       "  'publicationType': 'Not available'},\n",
       " '801dd02c3ff584ba0f5a4befe8578c770e774fa3': {'arxiv_id': None,\n",
       "  's2_paperId': '801dd02c3ff584ba0f5a4befe8578c770e774fa3',\n",
       "  'title': 'Algorithmic Management and Algorithmic Competencies: Understanding and Appropriating Algorithms in Gig Work',\n",
       "  'abstract': None,\n",
       "  'venue': 'iConference',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 104,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bd73789ef3a47917ac5b4f228a20652f4d2f56eb': {'arxiv_id': None,\n",
       "  's2_paperId': 'bd73789ef3a47917ac5b4f228a20652f4d2f56eb',\n",
       "  'title': 'Your order, their labor: An exploration of algorithms and laboring on food delivery platforms in China',\n",
       "  'abstract': None,\n",
       "  'venue': 'Chinese Journal of Communication',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 165,\n",
       "  'publicationType': 'Not available'},\n",
       " '549f269a4b48fbcc93f3bf280b4df7a6fdd394f7': {'arxiv_id': None,\n",
       "  's2_paperId': '549f269a4b48fbcc93f3bf280b4df7a6fdd394f7',\n",
       "  'title': 'The sharing economy and digital platforms: A review and research agenda',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Journal of Information Management',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 150,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 518,\n",
       "  'publicationType': 'Not available'},\n",
       " 'de21b652cd622c93576ff95bd93be8009b575ebf': {'arxiv_id': None,\n",
       "  's2_paperId': 'de21b652cd622c93576ff95bd93be8009b575ebf',\n",
       "  'title': 'Good Gig, Bad Gig: Autonomy and Algorithmic Control in the Global Gig Economy',\n",
       "  'abstract': 'This article evaluates the job quality of work in the remote gig economy. Such work consists of the remote provision of a wide variety of digital services mediated by online labour platforms. Focusing on workers in Southeast Asia and Sub-Saharan Africa, the article draws on semi-structured interviews in six countries (N = 107) and a cross-regional survey (N = 679) to detail the manner in which remote gig work is shaped by platform-based algorithmic control. Despite varying country contexts and types of work, we show that algorithmic control is central to the operation of online labour platforms. Algorithmic management techniques tend to offer workers high levels of flexibility, autonomy, task variety and complexity. However, these mechanisms of control can also result in low pay, social isolation, working unsocial and irregular hours, overwork, sleep deprivation and exhaustion.',\n",
       "  'venue': 'Work, Employment and Society',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 69,\n",
       "  'citationCount': 1084,\n",
       "  'publicationType': 'Not available'},\n",
       " '2e5c3ec191eb377dd022f22c2555924f3a1ac352': {'arxiv_id': None,\n",
       "  's2_paperId': '2e5c3ec191eb377dd022f22c2555924f3a1ac352',\n",
       "  'title': 'Thrown under the bus and outrunning it! The logic of Didi and taxi drivers’ labour and activism in the on-demand economy',\n",
       "  'abstract': None,\n",
       "  'venue': 'New Media & Society',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 58,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 165,\n",
       "  'publicationType': 'Not available'},\n",
       " '91a8da3bbc643dff3fdc443fd2384581aa05e786': {'arxiv_id': None,\n",
       "  's2_paperId': '91a8da3bbc643dff3fdc443fd2384581aa05e786',\n",
       "  'title': 'POTs: protective optimization technologies',\n",
       "  'abstract': None,\n",
       "  'venue': 'FAT*',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 146,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 95,\n",
       "  'publicationType': 'Not available'},\n",
       " '9583ac53a19cdf0db81fef6eb0b63e66adbe2324': {'arxiv_id': None,\n",
       "  's2_paperId': '9583ac53a19cdf0db81fef6eb0b63e66adbe2324',\n",
       "  'title': 'Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 477,\n",
       "  'citationCount': 1684,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e6356bbc6b5d223ddee1fa7688f48a4319f85454': {'arxiv_id': None,\n",
       "  's2_paperId': 'e6356bbc6b5d223ddee1fa7688f48a4319f85454',\n",
       "  'title': 'When Algorithms Shape Collective Action: Social Media and the Dynamics of Cloud Protesting',\n",
       "  'abstract': 'How does the algorithmically mediated environment of social media restructure social action? This article combines social movement studies and science and technology studies to explore the role of social media in the organization, unfolding, and diffusion of contemporary protests. In particular, it examines how activists leverage the technical properties of social media to develop a joint narrative and a collective identity. To this end, it offers the notion of cloud protesting as a theoretical approach and framework for empirical analysis. Cloud protesting indicates a specific type of mobilization that is grounded on, modeled around, and enabled by social media platforms and mobile devices and the virtual universes they identify. The notion emphasizes both the productive mediation of social and mobile media and the importance of activists’ sense-making activities. It also acknowledges that social media set in motion a process that is sociotechnical in nature rather than merely sociological or communicative, and thus can be understood only by intersecting the material and the symbolic dimensions of contemporary digitally mediated collective action. The article shows how the specific materiality of social media intervenes in the actors’ meaning work by fostering four mechanisms—namely performance, interpellation, temporality, and reproducibility—which concur to create a “politics of visibility” that alters traditional identity dynamics. In addition, it exposes the connection between organizational patterns and the role of individuals, explaining how the politics of visibility is the result of a process that originates and ends within the individual—which ultimately creates individuals-in-the-group rather than groups.',\n",
       "  'venue': '',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 87,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 138,\n",
       "  'publicationType': 'Not available'},\n",
       " '81fd20c2b903d979075e0c6a59258b0a84213095': {'arxiv_id': None,\n",
       "  's2_paperId': '81fd20c2b903d979075e0c6a59258b0a84213095',\n",
       "  'title': 'Strategic Classification',\n",
       "  'abstract': None,\n",
       "  'venue': 'Information Technology Convergence and Services',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 47,\n",
       "  'citationCount': 357,\n",
       "  'publicationType': 'Not available'},\n",
       " '03d4ce5a1066d72cd20fe3af744fe985d024b095': {'arxiv_id': None,\n",
       "  's2_paperId': '03d4ce5a1066d72cd20fe3af744fe985d024b095',\n",
       "  'title': 'We Are Dynamo: Overcoming Stalling and Friction in Collective Action for Crowd Workers',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Human Factors in Computing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 301,\n",
       "  'publicationType': 'Not available'},\n",
       " '54af2b16cc0bbc0ab28d1ce2edec4eec1133f3df': {'arxiv_id': None,\n",
       "  's2_paperId': '54af2b16cc0bbc0ab28d1ce2edec4eec1133f3df',\n",
       "  'title': 'Exploiting Social Navigation',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 50,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e55732de72c5b590b17a6032403514fbc3533633': {'arxiv_id': None,\n",
       "  's2_paperId': 'e55732de72c5b590b17a6032403514fbc3533633',\n",
       "  'title': 'Turkopticon: interrupting worker invisibility in amazon mechanical turk',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Human Factors in Computing Systems',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 73,\n",
       "  'citationCount': 809,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bce6ca4c1471e1035f8319b6d57dd042f61bc728': {'arxiv_id': None,\n",
       "  's2_paperId': 'bce6ca4c1471e1035f8319b6d57dd042f61bc728',\n",
       "  'title': 'The Logic of Collective Action: Public Goods and the Theory of Groups',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1969,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 351,\n",
       "  'citationCount': 5175,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ad7fbead28e014d9d3f7077cd7ae033f32048e16': {'arxiv_id': None,\n",
       "  's2_paperId': 'ad7fbead28e014d9d3f7077cd7ae033f32048e16',\n",
       "  'title': 'The logic of collective action : public goods and the theory of groups',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1967,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 359,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd86be39adff7a961ebd6402fa464834c1c0c30b4': {'arxiv_id': None,\n",
       "  's2_paperId': 'd86be39adff7a961ebd6402fa464834c1c0c30b4',\n",
       "  'title': 'Model-Targeted Poisoning Attacks with Provable Convergence',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 41,\n",
       "  'publicationType': 'Not available'},\n",
       " '86d099704bfa8481b4faf2bd852808ef6d76c6e9': {'arxiv_id': None,\n",
       "  's2_paperId': '86d099704bfa8481b4faf2bd852808ef6d76c6e9',\n",
       "  'title': 'The Rise of Algorithmic Work: Implications for Organizational Control and Worker Autonomy',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 13,\n",
       "  'publicationType': 'Not available'},\n",
       " '798852b8cc24f81401bb7e1c538373fab380618e': {'arxiv_id': None,\n",
       "  's2_paperId': '798852b8cc24f81401bb7e1c538373fab380618e',\n",
       "  'title': \"UvA-DARE (Digital Academic Repository) Platform Labor: On the Gendered and Racialized Exploitation of Low-Income Service Work in the 'On-Demand' Economy\",\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 61,\n",
       "  'influentialCitationCount': 41,\n",
       "  'citationCount': 425,\n",
       "  'publicationType': 'Not available'},\n",
       " '03ad6e1a217711567b0968042a47610b6b56a30f': {'arxiv_id': None,\n",
       "  's2_paperId': '03ad6e1a217711567b0968042a47610b6b56a30f',\n",
       "  'title': 'Making a digital working class : Uber drivers in Boston, 2016-2017',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 17,\n",
       "  'publicationType': 'Not available'},\n",
       " '5d2d46ff6f16c0d74308e200524413c33759c061': {'arxiv_id': None,\n",
       "  's2_paperId': '5d2d46ff6f16c0d74308e200524413c33759c061',\n",
       "  'title': 'Challenging Codes: Collective Action in the Information Age',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1996,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 155,\n",
       "  'citationCount': 1995,\n",
       "  'publicationType': 'Not available'},\n",
       " '5d03416c5dbfb1b63e6f297fbd350b7798a047f2': {'arxiv_id': '1105.0697v1',\n",
       "  's2_paperId': '5d03416c5dbfb1b63e6f297fbd350b7798a047f2',\n",
       "  'title': 'Uncovering the Temporal Dynamics of Diffusion Networks',\n",
       "  'abstract': 'Time plays an essential role in the diffusion of information, influence and disease over networks. In many cases we only observe when a node copies information, makes a decision or becomes infected – but the connectivity, transmission rates between nodes and transmission sources are unknown. Inferring the underlying dynamics is of outstanding interest since it enables forecasting, influencing and retarding infections, broadly construed. To this end, we model diffusion processes as discrete networks of continuous temporal processes occurring at different rates. Given cascade data – observed infection times of nodes – we infer the edges of the global diffusion network and estimate the transmission rates of each edge that best explain the observed data. The optimization problem is convex. The model naturally (without heuristics) imposes sparse solutions and requires no parameter tuning. The problem decouples into a collection of independent smaller problems, thus scaling easily to networks on the order of hundreds of thousands of nodes. Experiments on real and synthetic data show that our algorithm both recovers the edges of diffusion networks and accurately estimates their transmission rates from cascade data.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 92,\n",
       "  'citationCount': 594,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'f5be7e832a8ff1393e48d5d4949ba5a69e525480': {'arxiv_id': None,\n",
       "  's2_paperId': 'f5be7e832a8ff1393e48d5d4949ba5a69e525480',\n",
       "  'title': 'On the Convexity of Latent Social Network Inference',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 31,\n",
       "  'influentialCitationCount': 27,\n",
       "  'citationCount': 302,\n",
       "  'publicationType': 'Not available'},\n",
       " '72716e49b923a84db430e100feb7f6cb975f3302': {'arxiv_id': None,\n",
       "  's2_paperId': '72716e49b923a84db430e100feb7f6cb975f3302',\n",
       "  'title': 'Finding effectors in social networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Knowledge Discovery and Data Mining',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 24,\n",
       "  'citationCount': 221,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f07b535e3f69161cea69fea25221615f33bae4c4': {'arxiv_id': None,\n",
       "  's2_paperId': 'f07b535e3f69161cea69fea25221615f33bae4c4',\n",
       "  'title': 'Inferring networks of diffusion and influence',\n",
       "  'abstract': None,\n",
       "  'venue': 'Knowledge Discovery and Data Mining',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 78,\n",
       "  'influentialCitationCount': 87,\n",
       "  'citationCount': 1146,\n",
       "  'publicationType': 'Not available'},\n",
       " '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752': {'arxiv_id': None,\n",
       "  's2_paperId': '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752',\n",
       "  'title': 'Convex optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 396,\n",
       "  'influentialCitationCount': 4663,\n",
       "  'citationCount': 41296,\n",
       "  'publicationType': 'Not available'},\n",
       " '7c1a89a59c0c61f3459a53c8d8f72caa9466fcc3': {'arxiv_id': None,\n",
       "  's2_paperId': '7c1a89a59c0c61f3459a53c8d8f72caa9466fcc3',\n",
       "  'title': 'Kronecker Graphs: An Approach to Modeling Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 95,\n",
       "  'influentialCitationCount': 139,\n",
       "  'citationCount': 1068,\n",
       "  'publicationType': 'Not available'},\n",
       " '00b7ffd43e9b6b70c80449872a8c9ec49c7d045a': {'arxiv_id': None,\n",
       "  's2_paperId': '00b7ffd43e9b6b70c80449872a8c9ec49c7d045a',\n",
       "  'title': 'Hierarchical structure and the prediction of missing links in networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Nature',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 67,\n",
       "  'influentialCitationCount': 100,\n",
       "  'citationCount': 2064,\n",
       "  'publicationType': 'Not available'},\n",
       " '233084c0d1c818c842be6a9bb50f5dd2d1d1682f': {'arxiv_id': None,\n",
       "  's2_paperId': '233084c0d1c818c842be6a9bb50f5dd2d1d1682f',\n",
       "  'title': 'Statistical properties of community structure in large social and information networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'The Web Conference',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 163,\n",
       "  'influentialCitationCount': 64,\n",
       "  'citationCount': 819,\n",
       "  'publicationType': 'Not available'},\n",
       " '5b21d5866ee7a4d6fb4e9af965f465f4db3b65ab': {'arxiv_id': None,\n",
       "  's2_paperId': '5b21d5866ee7a4d6fb4e9af965f465f4db3b65ab',\n",
       "  'title': 'Influentials, Networks, and Public Opinion Formation',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 121,\n",
       "  'influentialCitationCount': 95,\n",
       "  'citationCount': 1899,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e63a855799bdfcc236d28d6db606e0904c6652f5': {'arxiv_id': None,\n",
       "  's2_paperId': 'e63a855799bdfcc236d28d6db606e0904c6652f5',\n",
       "  'title': 'Tracking information epidemics in blogspace',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Wirtschaftsinformatik',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 451,\n",
       "  'publicationType': 'Not available'},\n",
       " '1a2a0f3a88f8314052ed66816366ca2f8beb9512': {'arxiv_id': None,\n",
       "  's2_paperId': '1a2a0f3a88f8314052ed66816366ca2f8beb9512',\n",
       "  'title': 'Different Epidemic Curves for Severe Acute Respiratory Syndrome Reveal Similar Impacts of Control Measures',\n",
       "  'abstract': None,\n",
       "  'venue': 'American Journal of Epidemiology',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 109,\n",
       "  'citationCount': 1190,\n",
       "  'publicationType': 'Not available'},\n",
       " '13df42f24714d1d0c365a2ca6f62dd5cac9534de': {'arxiv_id': None,\n",
       "  's2_paperId': '13df42f24714d1d0c365a2ca6f62dd5cac9534de',\n",
       "  'title': 'Maximizing the spread of influence through a social network',\n",
       "  'abstract': 'Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of \"word of mouth\" in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target?We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks.We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks.',\n",
       "  'venue': 'Knowledge Discovery and Data Mining',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 1520,\n",
       "  'citationCount': 8546,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f564e707a11446f70fc00cbcc97fb9ce843a5b5d': {'arxiv_id': None,\n",
       "  's2_paperId': 'f564e707a11446f70fc00cbcc97fb9ce843a5b5d',\n",
       "  'title': 'Statistical Models and Methods for Lifetime Data',\n",
       "  'abstract': None,\n",
       "  'venue': 'Technometrics',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 13,\n",
       "  'influentialCitationCount': 52,\n",
       "  'citationCount': 3421,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e4a86a2caf9d997e4043d5ae4372b1a0ae310db2': {'arxiv_id': None,\n",
       "  's2_paperId': 'e4a86a2caf9d997e4043d5ae4372b1a0ae310db2',\n",
       "  'title': 'Statistical Models and Methods for Lifetime Data',\n",
       "  'abstract': None,\n",
       "  'venue': 'Technometrics',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 407,\n",
       "  'citationCount': 2968,\n",
       "  'publicationType': 'Not available'},\n",
       " '525199d30a5a975fb32e7944924c82b584fea1d0': {'arxiv_id': None,\n",
       "  's2_paperId': '525199d30a5a975fb32e7944924c82b584fea1d0',\n",
       "  'title': 'Emergence of scaling in random networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Science',\n",
       "  'year': 1999,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 352,\n",
       "  'citationCount': 6443,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a828fd17399d0ec9f59801e21230e7f6391757f4': {'arxiv_id': None,\n",
       "  's2_paperId': 'a828fd17399d0ec9f59801e21230e7f6391757f4',\n",
       "  'title': 'Emergence of Scaling in Random Networks',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1999,\n",
       "  'referenceCount': 11,\n",
       "  'influentialCitationCount': 2045,\n",
       "  'citationCount': 31281,\n",
       "  'publicationType': 'Not available'},\n",
       " '3ff91f28967e0702667a644f8f9c53d964d63e4c': {'arxiv_id': None,\n",
       "  's2_paperId': '3ff91f28967e0702667a644f8f9c53d964d63e4c',\n",
       "  'title': 'Large sample estimation and hypothesis testing',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1986,\n",
       "  'referenceCount': 78,\n",
       "  'influentialCitationCount': 723,\n",
       "  'citationCount': 3449,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a5aad5abb32f6b15f31b92312bb3b0f7b6470977': {'arxiv_id': None,\n",
       "  's2_paperId': 'a5aad5abb32f6b15f31b92312bb3b0f7b6470977',\n",
       "  'title': 'On the evolution of random graphs',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1984,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 724,\n",
       "  'citationCount': 8160,\n",
       "  'publicationType': 'Not available'},\n",
       " '864012bd1e0bbc590151c270564feb42889481e5': {'arxiv_id': None,\n",
       "  's2_paperId': '864012bd1e0bbc590151c270564feb42889481e5',\n",
       "  'title': 'The evolution of random graphs',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1984,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 5107,\n",
       "  'publicationType': 'Not available'},\n",
       " '983b110cfe69a6090f2b70ea2d04edd2648063aa': {'arxiv_id': '1905.11759v2',\n",
       "  's2_paperId': '983b110cfe69a6090f2b70ea2d04edd2648063aa',\n",
       "  'title': 'Manipulating a Learning Defender and Ways to Counteract',\n",
       "  'abstract': \"In Stackelberg security games when information about the attacker's payoffs is uncertain, algorithms have been proposed to learn the optimal defender commitment by interacting with the attacker and observing their best responses. In this paper, we show that, however, these algorithms can be easily manipulated if the attacker responds untruthfully. As a key finding, attacker manipulation normally leads to the defender learning a maximin strategy, which effectively renders the learning attempt meaningless as to compute a maximin strategy requires no additional information about the other player at all. We then apply a game-theoretic framework at a higher level to counteract such manipulation, in which the defender commits to a policy that specifies her strategy commitment according to the learned information. We provide a polynomial-time algorithm to compute the optimal such policy, and in addition, a heuristic approach that applies even when the attacker's payoff space is infinite or completely unknown. Empirical evaluation shows that our approaches can improve the defender's utility significantly as compared to the situation when attacker manipulation is ignored.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 20,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " 'e4ea1b32757599c5be9d65bee2809832f63f5d64': {'arxiv_id': None,\n",
       "  's2_paperId': 'e4ea1b32757599c5be9d65bee2809832f63f5d64',\n",
       "  'title': 'Learning Optimal Strategies to Commit To',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 63,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e1a808c3c0b14ff27b0c62a2537e457864083826': {'arxiv_id': None,\n",
       "  's2_paperId': 'e1a808c3c0b14ff27b0c62a2537e457864083826',\n",
       "  'title': 'Imitative Follower Deception in Stackelberg Games',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM Conference on Economics and Computation',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 33,\n",
       "  'publicationType': 'Not available'},\n",
       " '869fdb53a40290a3941fd6ab808835e9b5184d62': {'arxiv_id': None,\n",
       "  's2_paperId': '869fdb53a40290a3941fd6ab808835e9b5184d62',\n",
       "  'title': 'Adversarial Attacks and Defences: A Survey',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 75,\n",
       "  'influentialCitationCount': 29,\n",
       "  'citationCount': 660,\n",
       "  'publicationType': 'Not available'},\n",
       " '4a8c332b09bb99333a8bce6a4640a20c1352aa63': {'arxiv_id': None,\n",
       "  's2_paperId': '4a8c332b09bb99333a8bce6a4640a20c1352aa63',\n",
       "  'title': 'A Survey on Security Threats and Defensive Techniques of Machine Learning: A Data Driven View',\n",
       "  'abstract': 'Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future.',\n",
       "  'venue': 'IEEE Access',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 106,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 344,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f90517a6489107700f6d0d827222d0ae0976050d': {'arxiv_id': None,\n",
       "  's2_paperId': 'f90517a6489107700f6d0d827222d0ae0976050d',\n",
       "  'title': 'Three Strategies to Success: Learning Adversary Models in Security Games',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Joint Conference on Artificial Intelligence',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 41,\n",
       "  'publicationType': 'Not available'},\n",
       " '5f97238612bea789463858f2ba316d352fbe92c6': {'arxiv_id': None,\n",
       "  's2_paperId': '5f97238612bea789463858f2ba316d352fbe92c6',\n",
       "  'title': 'Playing Repeated Security Games with No Prior Knowledge',\n",
       "  'abstract': None,\n",
       "  'venue': 'Adaptive Agents and Multi-Agent Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 32,\n",
       "  'publicationType': 'Not available'},\n",
       " '6a56932b966e1252e5793fce524a14454a8f0ade': {'arxiv_id': None,\n",
       "  's2_paperId': '6a56932b966e1252e5793fce524a14454a8f0ade',\n",
       "  'title': 'Commitment Without Regrets: Online Learning in Stackelberg Security Games',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM Conference on Economics and Computation',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 133,\n",
       "  'publicationType': 'Not available'},\n",
       " 'be768de7da6dca72400b4e85905f75e18c88b08d': {'arxiv_id': None,\n",
       "  's2_paperId': 'be768de7da6dca72400b4e85905f75e18c88b08d',\n",
       "  'title': 'Watch and learn: optimizing from revealed preferences feedback',\n",
       "  'abstract': None,\n",
       "  'venue': 'SeCO Workshops',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 70,\n",
       "  'publicationType': 'Not available'},\n",
       " '7db52b4ec9f3388e3fae4dcba22e3e58f1d5a6b5': {'arxiv_id': None,\n",
       "  's2_paperId': '7db52b4ec9f3388e3fae4dcba22e3e58f1d5a6b5',\n",
       "  'title': 'Learning Optimal Commitment to Overcome Insecurity',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 116,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b4c570d689ecb387ea47ec2ea89f9e25d4a71665': {'arxiv_id': None,\n",
       "  's2_paperId': 'b4c570d689ecb387ea47ec2ea89f9e25d4a71665',\n",
       "  'title': 'Regret-Based Optimization and Preference Elicitation for Stackelberg Security Games with Uncertainty',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 50,\n",
       "  'publicationType': 'Not available'},\n",
       " '489e5b6cfabb908b23fd0dd291f9bc3f75148aa2': {'arxiv_id': None,\n",
       "  's2_paperId': '489e5b6cfabb908b23fd0dd291f9bc3f75148aa2',\n",
       "  'title': 'Security games with interval uncertainty',\n",
       "  'abstract': None,\n",
       "  'venue': 'Adaptive Agents and Multi-Agent Systems',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 72,\n",
       "  'publicationType': 'Not available'},\n",
       " '990a02f20529f5ce3b382f1d54648afaab391179': {'arxiv_id': '1206.6389v3',\n",
       "  's2_paperId': '990a02f20529f5ce3b382f1d54648afaab391179',\n",
       "  'title': 'Poisoning Attacks against Support Vector Machines',\n",
       "  'abstract': \"We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. \\n \\nThe proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.\",\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 104,\n",
       "  'citationCount': 1547,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'e37226a2f099c9a1ad13edce395ccca29225193c': {'arxiv_id': None,\n",
       "  's2_paperId': 'e37226a2f099c9a1ad13edce395ccca29225193c',\n",
       "  'title': 'Support Vector Machines Under Adversarial Label Noise',\n",
       "  'abstract': None,\n",
       "  'venue': 'Asian Conference on Machine Learning',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 394,\n",
       "  'publicationType': 'Not available'},\n",
       " 'afd0859a858481d2f36109f68090aebd77456b7f': {'arxiv_id': None,\n",
       "  's2_paperId': 'afd0859a858481d2f36109f68090aebd77456b7f',\n",
       "  'title': 'The security of machine learning',\n",
       "  'abstract': 'Machine learning’s ability to rapidly evolve to changing and complex situations has helped it become a fundamental tool for computer security. That adaptability is also a vulnerability: attackers can exploit machine learning systems. We present a taxonomy identifying and analyzing attacks against machine learning systems. We show how these classes influence the costs for the attacker and defender, and we give a formal structure defining their interaction. We use our framework to survey and analyze the literature of attacks against machine learning systems. We also illustrate our taxonomy by showing how it can guide attacks against SpamBayes, a popular statistical spam filter. Finally, we discuss how our taxonomy suggests new lines of defenses.',\n",
       "  'venue': 'Machine-mediated learning',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 45,\n",
       "  'influentialCitationCount': 56,\n",
       "  'citationCount': 862,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bdbb5b52c1bb8f83929d0bddc19f9b7eb6704d56': {'arxiv_id': None,\n",
       "  's2_paperId': 'bdbb5b52c1bb8f83929d0bddc19f9b7eb6704d56',\n",
       "  'title': 'Learning and Approximating the Optimal Strategy to Commit To',\n",
       "  'abstract': None,\n",
       "  'venue': 'Algorithmic Game Theory',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 144,\n",
       "  'publicationType': 'Not available'},\n",
       " '60cd91e79919a10fa92acf5d46f4962be26879ea': {'arxiv_id': None,\n",
       "  's2_paperId': '60cd91e79919a10fa92acf5d46f4962be26879ea',\n",
       "  'title': 'Effective solutions for real-world Stackelberg games: when agents must deal with human uncertainties',\n",
       "  'abstract': None,\n",
       "  'venue': 'Adaptive Agents and Multi-Agent Systems',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 86,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e8b78cc4d799f669ca61bf6455b211b8d33870df': {'arxiv_id': None,\n",
       "  's2_paperId': 'e8b78cc4d799f669ca61bf6455b211b8d33870df',\n",
       "  'title': 'Bayesian stackelberg games and their application for security at Los Angeles international airport',\n",
       "  'abstract': None,\n",
       "  'venue': 'SeCO Workshops',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 12,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 49,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a97f12e2d7e5881b6db790b7883618340bed87cf': {'arxiv_id': None,\n",
       "  's2_paperId': 'a97f12e2d7e5881b6db790b7883618340bed87cf',\n",
       "  'title': 'Playing games for security: an efficient exact algorithm for solving Bayesian Stackelberg games',\n",
       "  'abstract': None,\n",
       "  'venue': 'Adaptive Agents and Multi-Agent Systems',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 50,\n",
       "  'citationCount': 497,\n",
       "  'publicationType': 'Not available'},\n",
       " '7b3a3c7ac077474064e40b7baa5479ce2f36d4ea': {'arxiv_id': None,\n",
       "  's2_paperId': '7b3a3c7ac077474064e40b7baa5479ce2f36d4ea',\n",
       "  'title': 'Computing the optimal strategy to commit to',\n",
       "  'abstract': 'In multiagent systems, strategic settings are often analyzed under the assumption that the players choose their strategies simultaneously. However, this model is not always realistic. In many settings, one player is able to commit to a strategy before the other player makes a decision. Such models are synonymously referred to as leadership, commitment, or Stackelberg models, and optimal play in such models is often significantly different from optimal play in the model where strategies are selected simultaneously.The recent surge in interest in computing game-theoretic solutions has so far ignored leadership models (with the exception of the interest in mechanism design, where the designer is implicitly in a leadership position). In this paper, we study how to compute optimal strategies to commit to under both commitment to pure strategies and commitment to mixed strategies, in both normal-form and Bayesian games. We give both positive results (efficient algorithms) and negative results (NP-hardness results).',\n",
       "  'venue': 'ACM Conference on Economics and Computation',\n",
       "  'year': 2006,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 89,\n",
       "  'citationCount': 568,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd94f56e7ef07b6e785b830e41da30c257b9a6ece': {'arxiv_id': None,\n",
       "  's2_paperId': 'd94f56e7ef07b6e785b830e41da30c257b9a6ece',\n",
       "  'title': 'Adversarial classification',\n",
       "  'abstract': None,\n",
       "  'venue': 'Knowledge Discovery and Data Mining',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 60,\n",
       "  'citationCount': 1019,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e31d26df4c3e8652d633e2d26c2444c8a45657c9': {'arxiv_id': None,\n",
       "  's2_paperId': 'e31d26df4c3e8652d633e2d26c2444c8a45657c9',\n",
       "  'title': 'Run the GAMUT: a comprehensive approach to evaluating game-theoretic algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': 'Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 219,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ceed9beecb1a8dd73d624bd19e337c0148ff3036': {'arxiv_id': None,\n",
       "  's2_paperId': 'ceed9beecb1a8dd73d624bd19e337c0148ff3036',\n",
       "  'title': 'Leadership with commitment to mixed strategies',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 170,\n",
       "  'publicationType': 'Not available'},\n",
       " '5b706b9bf333922df42cd4790549f081bbc8cab4': {'arxiv_id': None,\n",
       "  's2_paperId': '5b706b9bf333922df42cd4790549f081bbc8cab4',\n",
       "  'title': 'Quantal Response Equilibria for Normal Form Games',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1995,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 288,\n",
       "  'citationCount': 2687,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f5271aa616b8a9c220dfb75be6ffbfe0746bdccb': {'arxiv_id': None,\n",
       "  's2_paperId': 'f5271aa616b8a9c220dfb75be6ffbfe0746bdccb',\n",
       "  'title': 'Security And Game Theory Algorithms Deployed Systems Lessons Learned',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 76,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c0ae5848ba0141dd3f827321f46110f52946764b': {'arxiv_id': '2205.10536v3',\n",
       "  's2_paperId': 'c0ae5848ba0141dd3f827321f46110f52946764b',\n",
       "  'title': 'Knowledge Distillation from A Stronger Teacher',\n",
       "  'abstract': 'Unlike existing knowledge distillation methods focus on the baseline settings, where the teacher models and training strategies are not that strong and competing as state-of-the-art approaches, this paper presents a method dubbed DIST to distill better from a stronger teacher. We empirically find that the discrepancy of predictions between the student and a stronger teacher may tend to be fairly severer. As a result, the exact match of predictions in KL divergence would disturb the training and make existing methods perform poorly. In this paper, we show that simply preserving the relations between the predictions of teacher and student would suffice, and propose a correlation-based loss to capture the intrinsic inter-class relations from the teacher explicitly. Besides, considering that different instances have different semantic similarities to each class, we also extend this relational match to the intra-class level. Our method is simple yet practical, and extensive experiments demonstrate that it adapts well to various architectures, model sizes and training strategies, and can achieve state-of-the-art performance consistently on image classification, object detection, and semantic segmentation tasks. Code is available at: https://github.com/hunto/DIST_KD .',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 58,\n",
       "  'influentialCitationCount': 30,\n",
       "  'citationCount': 224,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " 'ea7029d694d12292ef8374ac0f5ca821d48edb87': {'arxiv_id': None,\n",
       "  's2_paperId': 'ea7029d694d12292ef8374ac0f5ca821d48edb87',\n",
       "  'title': 'Cross-Image Relational Knowledge Distillation for Semantic Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 21,\n",
       "  'citationCount': 165,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c5be829c512300d79762bb9424927d8dd51ba76a': {'arxiv_id': None,\n",
       "  's2_paperId': 'c5be829c512300d79762bb9424927d8dd51ba76a',\n",
       "  'title': 'DyRep: Bootstrapping Training with Dynamic Re-parameterization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 26,\n",
       "  'publicationType': 'Not available'},\n",
       " '96f79556c4a6f11f88ede749f2e379cc97d237c2': {'arxiv_id': None,\n",
       "  's2_paperId': '96f79556c4a6f11f88ede749f2e379cc97d237c2',\n",
       "  'title': 'Relational Surrogate Loss Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 5,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f454f6b5f2ca9749ddf442eb5134612ef7f758c1': {'arxiv_id': None,\n",
       "  's2_paperId': 'f454f6b5f2ca9749ddf442eb5134612ef7f758c1',\n",
       "  'title': 'ResNet strikes back: An improved training procedure in timm',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 62,\n",
       "  'influentialCitationCount': 66,\n",
       "  'citationCount': 474,\n",
       "  'publicationType': 'Not available'},\n",
       " '463c3e13c2d6fb3148bb2a1f0328b484d8a353f9': {'arxiv_id': None,\n",
       "  's2_paperId': '463c3e13c2d6fb3148bb2a1f0328b484d8a353f9',\n",
       "  'title': 'Distilling Knowledge via Knowledge Review',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 73,\n",
       "  'citationCount': 406,\n",
       "  'publicationType': 'Not available'},\n",
       " '1c38fb9ee7a86755c184a37bcb654786150038c5': {'arxiv_id': None,\n",
       "  's2_paperId': '1c38fb9ee7a86755c184a37bcb654786150038c5',\n",
       "  'title': 'Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 73,\n",
       "  'publicationType': 'Not available'},\n",
       " '237c2d518b79678f7332a4243f57a285711a4c37': {'arxiv_id': None,\n",
       "  's2_paperId': '237c2d518b79678f7332a4243f57a285711a4c37',\n",
       "  'title': 'Channel-wise Knowledge Distillation for Dense Prediction*',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 48,\n",
       "  'citationCount': 248,\n",
       "  'publicationType': 'Not available'},\n",
       " '5e1b76361163a2a61b8652d8769109b1fe1cd4b2': {'arxiv_id': None,\n",
       "  's2_paperId': '5e1b76361163a2a61b8652d8769109b1fe1cd4b2',\n",
       "  'title': 'Densely Guided Knowledge Distillation using Multiple Teacher Assistants',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 107,\n",
       "  'publicationType': 'Not available'},\n",
       " '1728cb805a9573b59330890ba9723e73d6c3c974': {'arxiv_id': None,\n",
       "  's2_paperId': '1728cb805a9573b59330890ba9723e73d6c3c974',\n",
       "  'title': 'Knowledge Distillation: A Survey',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Journal of Computer Vision',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 374,\n",
       "  'influentialCitationCount': 90,\n",
       "  'citationCount': 2767,\n",
       "  'publicationType': 'Not available'},\n",
       " 'cc384eecd111f08e22a1e8e111129ea42aa11646': {'arxiv_id': None,\n",
       "  's2_paperId': 'cc384eecd111f08e22a1e8e111129ea42aa11646',\n",
       "  'title': 'Probabilistic Knowledge Transfer for Lightweight Deep Representation Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Neural Networks and Learning Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 93,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fb0dec9641c3676b4d90f35e357825175be84875': {'arxiv_id': None,\n",
       "  's2_paperId': 'fb0dec9641c3676b4d90f35e357825175be84875',\n",
       "  'title': 'Learning Student Networks with Few Data',\n",
       "  'abstract': 'Recently, the teacher-student learning paradigm has drawn much attention in compressing neural networks on low-end edge devices, such as mobile phones and wearable watches. Current algorithms mainly assume the complete dataset for the teacher network is also available for the training of the student network. However, for real-world scenarios, users may only have access to part of training examples due to commercial profits or data privacy, and severe over-fitting issues would happen as a result. In this paper, we tackle the challenge of learning student networks with few data by investigating the ground-truth data-generating distribution underlying these few data. Taking Wasserstein distance as the measurement, we assume this ideal data distribution lies in a neighborhood of the discrete empirical distribution induced by the training examples. Thus we propose to safely optimize the worst-case cost within this neighborhood to boost the generalization. Furthermore, with theoretical analysis, we derive a novel and easy-to-implement loss for training the student network in an end-to-end fashion. Experimental results on benchmark datasets validate the effectiveness of our proposed method.',\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 31,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 11,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f601da1b9dace6fcb5b252f6da0bc9074dd49311': {'arxiv_id': None,\n",
       "  's2_paperId': 'f601da1b9dace6fcb5b252f6da0bc9074dd49311',\n",
       "  'title': 'GreedyNAS: Towards Fast One-Shot NAS With Greedy Supernet',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 143,\n",
       "  'publicationType': 'Not available'},\n",
       " '197498cb2ad787e4f71c05098cee6b10d9d067bd': {'arxiv_id': None,\n",
       "  's2_paperId': '197498cb2ad787e4f71c05098cee6b10d9d067bd',\n",
       "  'title': 'Contrastive Representation Distillation',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 228,\n",
       "  'citationCount': 1018,\n",
       "  'publicationType': 'Not available'},\n",
       " '8de7f044a673d1f5e3b454d0663811f91aa9811a': {'arxiv_id': None,\n",
       "  's2_paperId': '8de7f044a673d1f5e3b454d0663811f91aa9811a',\n",
       "  'title': 'On the Efficacy of Knowledge Distillation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 53,\n",
       "  'citationCount': 584,\n",
       "  'publicationType': 'Not available'},\n",
       " '87f6a7c014ce206ac5b57299c07e10667d194b39': {'arxiv_id': None,\n",
       "  's2_paperId': '87f6a7c014ce206ac5b57299c07e10667d194b39',\n",
       "  'title': 'Randaugment: Practical automated data augmentation with a reduced search space',\n",
       "  'abstract': None,\n",
       "  'venue': '2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 61,\n",
       "  'influentialCitationCount': 485,\n",
       "  'citationCount': 3349,\n",
       "  'publicationType': 'Not available'},\n",
       " '9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08': {'arxiv_id': None,\n",
       "  's2_paperId': '9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08',\n",
       "  'title': 'Similarity-Preserving Knowledge Distillation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 106,\n",
       "  'citationCount': 942,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bc626a52664e948a0ffb2b95d0e1e6377a01171a': {'arxiv_id': None,\n",
       "  's2_paperId': 'bc626a52664e948a0ffb2b95d0e1e6377a01171a',\n",
       "  'title': 'Cascade R-CNN: High Quality Object Detection and Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 67,\n",
       "  'influentialCitationCount': 212,\n",
       "  'citationCount': 1273,\n",
       "  'publicationType': 'Not available'},\n",
       " '4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9': {'arxiv_id': None,\n",
       "  's2_paperId': '4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9',\n",
       "  'title': 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 2049,\n",
       "  'citationCount': 17253,\n",
       "  'publicationType': 'Not available'},\n",
       " '92ebadf9913e6800331e5f9b2699812fe77313ff': {'arxiv_id': None,\n",
       "  's2_paperId': '92ebadf9913e6800331e5f9b2699812fe77313ff',\n",
       "  'title': 'Variational Information Distillation for Knowledge Transfer',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 68,\n",
       "  'citationCount': 597,\n",
       "  'publicationType': 'Not available'},\n",
       " '0f736d2067ee9c950b876f14521268c6009e67d6': {'arxiv_id': None,\n",
       "  's2_paperId': '0f736d2067ee9c950b876f14521268c6009e67d6',\n",
       "  'title': 'Relational Knowledge Distillation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 179,\n",
       "  'citationCount': 1357,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b096edfdecaf44c3783ee71cda9bf829809d0f17': {'arxiv_id': None,\n",
       "  's2_paperId': 'b096edfdecaf44c3783ee71cda9bf829809d0f17',\n",
       "  'title': 'A Comprehensive Overhaul of Feature Distillation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 55,\n",
       "  'citationCount': 558,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c4696d47e0d43a6761742e15c262a224466c4b85': {'arxiv_id': None,\n",
       "  's2_paperId': 'c4696d47e0d43a6761742e15c262a224466c4b85',\n",
       "  'title': 'Correlation Congruence for Knowledge Distillation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 61,\n",
       "  'citationCount': 502,\n",
       "  'publicationType': 'Not available'},\n",
       " '252d5cf2a9ff20eb526321e8fbe5a2b790a42a12': {'arxiv_id': None,\n",
       "  's2_paperId': '252d5cf2a9ff20eb526321e8fbe5a2b790a42a12',\n",
       "  'title': 'Structured Knowledge Distillation for Dense Prediction',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 93,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 183,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bc6dfc6bda2d929fec91042dce1831fd07999b39': {'arxiv_id': None,\n",
       "  's2_paperId': 'bc6dfc6bda2d929fec91042dce1831fd07999b39',\n",
       "  'title': 'Improved Knowledge Distillation via Teacher Assistant',\n",
       "  'abstract': 'Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.',\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 96,\n",
       "  'citationCount': 1043,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bc38dbf06fe17b3c81001dcb939ae0f0a432f0b6': {'arxiv_id': None,\n",
       "  's2_paperId': 'bc38dbf06fe17b3c81001dcb939ae0f0a432f0b6',\n",
       "  'title': 'Learning Student Networks via Feature Embedding',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Neural Networks and Learning Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 93,\n",
       "  'publicationType': 'Not available'},\n",
       " '8cb3000e8959d1065532d54a07cf8fe97ef6b9c6': {'arxiv_id': None,\n",
       "  's2_paperId': '8cb3000e8959d1065532d54a07cf8fe97ef6b9c6',\n",
       "  'title': 'Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 34,\n",
       "  'citationCount': 506,\n",
       "  'publicationType': 'Not available'},\n",
       " '9217e28b2273eb3b26e4e9b7b498b4661e6e09f5': {'arxiv_id': None,\n",
       "  's2_paperId': '9217e28b2273eb3b26e4e9b7b498b4661e6e09f5',\n",
       "  'title': 'Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 88,\n",
       "  'influentialCitationCount': 1663,\n",
       "  'citationCount': 12570,\n",
       "  'publicationType': 'Not available'},\n",
       " '9f58a7e844cd15d8aef8b2de7f246979ababe429': {'arxiv_id': '1802.04977v3',\n",
       "  's2_paperId': '9f58a7e844cd15d8aef8b2de7f246979ababe429',\n",
       "  'title': 'Paraphrasing Complex Network: Network Compression via Factor Transfer',\n",
       "  'abstract': \"Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 36,\n",
       "  'citationCount': 536,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '4feef0fd284feb1233399b400eb897f59ec92755': {'arxiv_id': None,\n",
       "  's2_paperId': '4feef0fd284feb1233399b400eb897f59ec92755',\n",
       "  'title': 'mixup: Beyond Empirical Risk Minimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 1521,\n",
       "  'citationCount': 9414,\n",
       "  'publicationType': 'Not available'},\n",
       " '3c4b6f59a4dd9b6fb589abb826d063f7872a5808': {'arxiv_id': None,\n",
       "  's2_paperId': '3c4b6f59a4dd9b6fb589abb826d063f7872a5808',\n",
       "  'title': 'Learning from Multiple Teacher Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Knowledge Discovery and Data Mining',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 367,\n",
       "  'publicationType': 'Not available'},\n",
       " '9da734397acd7ff7c557960c62fb1b400b27bd89': {'arxiv_id': None,\n",
       "  's2_paperId': '9da734397acd7ff7c557960c62fb1b400b27bd89',\n",
       "  'title': 'ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices',\n",
       "  'abstract': None,\n",
       "  'venue': '2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 622,\n",
       "  'citationCount': 6613,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ee4a012a4b12d11d7ab8c0e79c61e807927a163c': {'arxiv_id': None,\n",
       "  's2_paperId': 'ee4a012a4b12d11d7ab8c0e79c61e807927a163c',\n",
       "  'title': 'Rethinking Atrous Convolution for Semantic Image Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 96,\n",
       "  'influentialCitationCount': 1056,\n",
       "  'citationCount': 8201,\n",
       "  'publicationType': 'Not available'},\n",
       " '2a94c84383ee3de5e6211d43d16e7de387f68878': {'arxiv_id': None,\n",
       "  's2_paperId': '2a94c84383ee3de5e6211d43d16e7de387f68878',\n",
       "  'title': 'Feature Pyramid Networks for Object Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 2921,\n",
       "  'citationCount': 21291,\n",
       "  'publicationType': 'Not available'},\n",
       " '1031a69923b80ad01cf3fbb703d10757a80e699b': {'arxiv_id': None,\n",
       "  's2_paperId': '1031a69923b80ad01cf3fbb703d10757a80e699b',\n",
       "  'title': 'Pyramid Scene Parsing Network',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 1555,\n",
       "  'citationCount': 11619,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f7b032a4df721d4ed2bab97f6acd33d62477b7a5': {'arxiv_id': None,\n",
       "  's2_paperId': 'f7b032a4df721d4ed2bab97f6acd33d62477b7a5',\n",
       "  'title': 'Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 304,\n",
       "  'citationCount': 2510,\n",
       "  'publicationType': 'Not available'},\n",
       " '0c908739fbff75f03469d13d4a1a07de3414ee19': {'arxiv_id': None,\n",
       "  's2_paperId': '0c908739fbff75f03469d13d4a1a07de3414ee19',\n",
       "  'title': 'Distilling the Knowledge in a Neural Network',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 9,\n",
       "  'influentialCitationCount': 2321,\n",
       "  'citationCount': 18919,\n",
       "  'publicationType': 'Not available'},\n",
       " '8604f376633af8b347e31d84c6150a93b11e34c2': {'arxiv_id': None,\n",
       "  's2_paperId': '8604f376633af8b347e31d84c6150a93b11e34c2',\n",
       "  'title': 'FitNets: Hints for Thin Deep Nets',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 440,\n",
       "  'citationCount': 3785,\n",
       "  'publicationType': 'Not available'},\n",
       " 'eb42cf88027de515750f230b23b1a057dc782108': {'arxiv_id': None,\n",
       "  's2_paperId': 'eb42cf88027de515750f230b23b1a057dc782108',\n",
       "  'title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 13797,\n",
       "  'citationCount': 98293,\n",
       "  'publicationType': 'Not available'},\n",
       " '71b7178df5d2b112d07e45038cb5637208659ff7': {'arxiv_id': None,\n",
       "  's2_paperId': '71b7178df5d2b112d07e45038cb5637208659ff7',\n",
       "  'title': 'Microsoft COCO: Common Objects in Context',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 6436,\n",
       "  'citationCount': 42157,\n",
       "  'publicationType': 'Not available'},\n",
       " '41bebd1951e57588e7829e44fab1bac0cc9251d2': {'arxiv_id': None,\n",
       "  's2_paperId': '41bebd1951e57588e7829e44fab1bac0cc9251d2',\n",
       "  'title': 'Torchvision the machine-vision package of torch',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM Multimedia',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 3,\n",
       "  'influentialCitationCount': 45,\n",
       "  'citationCount': 504,\n",
       "  'publicationType': 'Not available'},\n",
       " '94883f8cdfe2cd891c0b65f8724b4e4259d1630e': {'arxiv_id': None,\n",
       "  's2_paperId': '94883f8cdfe2cd891c0b65f8724b4e4259d1630e',\n",
       "  'title': 'The Concise Encyclopedia of Statistics',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 38,\n",
       "  'citationCount': 366,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd7b7f16ec98591b535f7b03c5e7d3327a4b2cac8': {'arxiv_id': None,\n",
       "  's2_paperId': 'd7b7f16ec98591b535f7b03c5e7d3327a4b2cac8',\n",
       "  'title': 'A NEW MEASURE OF RANK CORRELATION',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1938,\n",
       "  'referenceCount': 1,\n",
       "  'influentialCitationCount': 525,\n",
       "  'citationCount': 6654,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c8b25fab5608c3e033d34b4483ec47e68ba109b7': {'arxiv_id': None,\n",
       "  's2_paperId': 'c8b25fab5608c3e033d34b4483ec47e68ba109b7',\n",
       "  'title': 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 86,\n",
       "  'influentialCitationCount': 2930,\n",
       "  'citationCount': 19832,\n",
       "  'publicationType': 'Not available'},\n",
       " '3555f47e781e25cf6e1b80d0daf0c39ad1a2d705': {'arxiv_id': None,\n",
       "  's2_paperId': '3555f47e781e25cf6e1b80d0daf0c39ad1a2d705',\n",
       "  'title': 'Knowledge distillation via softmax regression representation learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 140,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ba28c939b3e1f66d60e212457b2d76973ec7846f': {'arxiv_id': None,\n",
       "  's2_paperId': 'ba28c939b3e1f66d60e212457b2d76973ec7846f',\n",
       "  'title': 'Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 30,\n",
       "  'citationCount': 201,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c7befbedcc98325762d4a80918218a5bebfe8b84': {'arxiv_id': None,\n",
       "  's2_paperId': 'c7befbedcc98325762d4a80918218a5bebfe8b84',\n",
       "  'title': 'Intra-class Feature Variation Distillation for Semantic Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 34,\n",
       "  'citationCount': 132,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c6f93cd84b942323c9fc87a6b26624f8344b3d42': {'arxiv_id': None,\n",
       "  's2_paperId': 'c6f93cd84b942323c9fc87a6b26624f8344b3d42',\n",
       "  'title': 'Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 104,\n",
       "  'publicationType': 'Not available'},\n",
       " '89e8c86c6144c752885baa07bf3dbe11142e5fab': {'arxiv_id': None,\n",
       "  's2_paperId': '89e8c86c6144c752885baa07bf3dbe11142e5fab',\n",
       "  'title': 'Mathematical Contributions to the Theory of Evolution. III. Regression, Heredity, and Panmixia',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': None,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 40,\n",
       "  'citationCount': 1658,\n",
       "  'publicationType': 'Not available'},\n",
       " '36fff28902cfb6c99c7c98f284639ad8e0133c44': {'arxiv_id': '2306.01567v2',\n",
       "  's2_paperId': '36fff28902cfb6c99c7c98f284639ad8e0133c44',\n",
       "  'title': 'Segment Anything in High Quality',\n",
       "  'abstract': \"The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures. We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM's mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 10 diverse segmentation datasets across different downstream tasks, where 8 out of them are evaluated in a zero-shot transfer protocol. Our code and pretrained models are at https://github.com/SysCV/SAM-HQ.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 48,\n",
       "  'citationCount': 297,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " 'fdaa21fce5ee5aadf1bb8c55d5f7c3155b905846': {'arxiv_id': None,\n",
       "  's2_paperId': 'fdaa21fce5ee5aadf1bb8c55d5f7c3155b905846',\n",
       "  'title': 'Faster Segment Anything: Towards Lightweight SAM for Mobile Applications',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 34,\n",
       "  'citationCount': 312,\n",
       "  'publicationType': 'Not available'},\n",
       " '0819c1e60c13b9797f937282d06b54d252d9d6ec': {'arxiv_id': None,\n",
       "  's2_paperId': '0819c1e60c13b9797f937282d06b54d252d9d6ec',\n",
       "  'title': 'Segment Everything Everywhere All at Once',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 90,\n",
       "  'influentialCitationCount': 28,\n",
       "  'citationCount': 441,\n",
       "  'publicationType': 'Not available'},\n",
       " '90af7c6cdf4b3359f6d275afb436f54f60082364': {'arxiv_id': None,\n",
       "  's2_paperId': '90af7c6cdf4b3359f6d275afb436f54f60082364',\n",
       "  'title': 'SegGPT: Segmenting Everything In Context',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 28,\n",
       "  'citationCount': 196,\n",
       "  'publicationType': 'Not available'},\n",
       " '7470a1702c8c86e6f28d32cfa315381150102f5b': {'arxiv_id': None,\n",
       "  's2_paperId': '7470a1702c8c86e6f28d32cfa315381150102f5b',\n",
       "  'title': 'Segment Anything',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 148,\n",
       "  'influentialCitationCount': 1085,\n",
       "  'citationCount': 6431,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0': {'arxiv_id': None,\n",
       "  's2_paperId': 'c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0',\n",
       "  'title': 'Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 68,\n",
       "  'influentialCitationCount': 266,\n",
       "  'citationCount': 1731,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fac388b8c24044dea06cc8c7b03dd1d99c8439a0': {'arxiv_id': None,\n",
       "  's2_paperId': 'fac388b8c24044dea06cc8c7b03dd1d99c8439a0',\n",
       "  'title': 'AIM: Adapting Image Models for Efficient Video Action Recognition',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 79,\n",
       "  'influentialCitationCount': 21,\n",
       "  'citationCount': 141,\n",
       "  'publicationType': 'Not available'},\n",
       " '48238f26f3fc02377711d5d6f4e8894faa1c883b': {'arxiv_id': None,\n",
       "  's2_paperId': '48238f26f3fc02377711d5d6f4e8894faa1c883b',\n",
       "  'title': 'PatchDCT: Patch Refinement for High Quality Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 14,\n",
       "  'publicationType': 'Not available'},\n",
       " '765377c3ed4d567a0c80def473a88f9ea3fcd51f': {'arxiv_id': None,\n",
       "  's2_paperId': '765377c3ed4d567a0c80def473a88f9ea3fcd51f',\n",
       "  'title': 'Intra-Batch Supervision for Panoptic Segmentation on High-Resolution Images',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Workshop/Winter Conference on Applications of Computer Vision',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 6,\n",
       "  'publicationType': 'Not available'},\n",
       " '967907503b24423b9b74621051811fcf684e3957': {'arxiv_id': None,\n",
       "  's2_paperId': '967907503b24423b9b74621051811fcf684e3957',\n",
       "  'title': 'Generalized Decoding for Pixel, Image, and Language',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 106,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 236,\n",
       "  'publicationType': 'Not available'},\n",
       " '41d3b9617772fda44cd81a3a11eead7236a0c01b': {'arxiv_id': None,\n",
       "  's2_paperId': '41d3b9617772fda44cd81a3a11eead7236a0c01b',\n",
       "  'title': 'What do Vision Transformers Learn? A Visual Exploration',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 60,\n",
       "  'publicationType': 'Not available'},\n",
       " '67db43cb6cc618c873c63fe2c83025c335b7a230': {'arxiv_id': None,\n",
       "  's2_paperId': '67db43cb6cc618c873c63fe2c83025c335b7a230',\n",
       "  'title': 'ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 62,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 165,\n",
       "  'publicationType': 'Not available'},\n",
       " '7d2d7c52311968855130c123acf2fbf70f948129': {'arxiv_id': None,\n",
       "  's2_paperId': '7d2d7c52311968855130c123acf2fbf70f948129',\n",
       "  'title': 'Video Mask Transfiner for High-Quality Video Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 29,\n",
       "  'publicationType': 'Not available'},\n",
       " '01724c36660359545e1368fc80c99f4bde44a190': {'arxiv_id': None,\n",
       "  's2_paperId': '01724c36660359545e1368fc80c99f4bde44a190',\n",
       "  'title': 'XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 64,\n",
       "  'influentialCitationCount': 87,\n",
       "  'citationCount': 376,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a09cbcaac305884f043810afc4fa4053099b5970': {'arxiv_id': None,\n",
       "  's2_paperId': 'a09cbcaac305884f043810afc4fa4053099b5970',\n",
       "  'title': 'Exploring Plain Vision Transformer Backbones for Object Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 60,\n",
       "  'influentialCitationCount': 77,\n",
       "  'citationCount': 758,\n",
       "  'publicationType': 'Not available'},\n",
       " 'adb272fbdea3631059cf88ab764bb6c2ce29f965': {'arxiv_id': None,\n",
       "  's2_paperId': 'adb272fbdea3631059cf88ab764bb6c2ce29f965',\n",
       "  'title': 'Visual Prompt Tuning',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 97,\n",
       "  'influentialCitationCount': 303,\n",
       "  'citationCount': 1463,\n",
       "  'publicationType': 'Not available'},\n",
       " '9dc481ec44178e797466bbad968071917842156b': {'arxiv_id': None,\n",
       "  's2_paperId': '9dc481ec44178e797466bbad968071917842156b',\n",
       "  'title': 'DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 191,\n",
       "  'citationCount': 1304,\n",
       "  'publicationType': 'Not available'},\n",
       " '5995504a006490f3b16cba56911ec4a13e47127d': {'arxiv_id': None,\n",
       "  's2_paperId': '5995504a006490f3b16cba56911ec4a13e47127d',\n",
       "  'title': 'Highly Accurate Dichotomous Image Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 118,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 99,\n",
       "  'publicationType': 'Not available'},\n",
       " '78d02f2909a582c624eca2d0f67c91ee91974180': {'arxiv_id': None,\n",
       "  's2_paperId': '78d02f2909a582c624eca2d0f67c91ee91974180',\n",
       "  'title': 'DN-DETR: Accelerate DETR Training by Introducing Query DeNoising',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 88,\n",
       "  'citationCount': 602,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f427ccb1d97cee3fde8abf0f5442f859531f5bf1': {'arxiv_id': None,\n",
       "  's2_paperId': 'f427ccb1d97cee3fde8abf0f5442f859531f5bf1',\n",
       "  'title': 'Mask2Former for Video Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 42,\n",
       "  'citationCount': 166,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b91d23fcbbaeeb7fec49cbe3d76907f3037339af': {'arxiv_id': None,\n",
       "  's2_paperId': 'b91d23fcbbaeeb7fec49cbe3d76907f3037339af',\n",
       "  'title': 'High Quality Segmentation for Ultra High-resolution Images',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 65,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 43,\n",
       "  'publicationType': 'Not available'},\n",
       " '3abe4ab47090768a9dd908f8aa004887b72ce74a': {'arxiv_id': None,\n",
       "  's2_paperId': '3abe4ab47090768a9dd908f8aa004887b72ce74a',\n",
       "  'title': 'Mask Transfiner for High-Quality Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 112,\n",
       "  'publicationType': 'Not available'},\n",
       " '458af0f3f03229290572a2630c75ac56e9dbec6e': {'arxiv_id': None,\n",
       "  's2_paperId': '458af0f3f03229290572a2630c75ac56e9dbec6e',\n",
       "  'title': 'CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 67,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 218,\n",
       "  'publicationType': 'Not available'},\n",
       " '96ea07447d2f9adefe03852a878517a2a6d45b96': {'arxiv_id': None,\n",
       "  's2_paperId': '96ea07447d2f9adefe03852a878517a2a6d45b96',\n",
       "  'title': 'Learning to Prompt for Vision-Language Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Journal of Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 61,\n",
       "  'influentialCitationCount': 424,\n",
       "  'citationCount': 2194,\n",
       "  'publicationType': 'Not available'},\n",
       " '76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2': {'arxiv_id': None,\n",
       "  's2_paperId': '76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2',\n",
       "  'title': 'On the Opportunities and Risks of Foundation Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 178,\n",
       "  'citationCount': 4030,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a8ca46b171467ceb2d7652fbfb67fe701ad86092': {'arxiv_id': None,\n",
       "  's2_paperId': 'a8ca46b171467ceb2d7652fbfb67fe701ad86092',\n",
       "  'title': 'LoRA: Low-Rank Adaptation of Large Language Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 65,\n",
       "  'influentialCitationCount': 1498,\n",
       "  'citationCount': 9004,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd7f041c8a0f0e8cc34c6a52f5996e906a7c7795b': {'arxiv_id': None,\n",
       "  's2_paperId': 'd7f041c8a0f0e8cc34c6a52f5996e906a7c7795b',\n",
       "  'title': 'Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 76,\n",
       "  'publicationType': 'Not available'},\n",
       " '57076b4306819203521d14dfad08693c2f5452f7': {'arxiv_id': None,\n",
       "  's2_paperId': '57076b4306819203521d14dfad08693c2f5452f7',\n",
       "  'title': 'Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 26,\n",
       "  'citationCount': 123,\n",
       "  'publicationType': 'Not available'},\n",
       " '363c260b6044bd35b0c200a4481228bbc6eb49a7': {'arxiv_id': None,\n",
       "  's2_paperId': '363c260b6044bd35b0c200a4481228bbc6eb49a7',\n",
       "  'title': 'Boundary IoU: Improving Object-Centric Image Segmentation Evaluation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 268,\n",
       "  'publicationType': 'Not available'},\n",
       " '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4': {'arxiv_id': None,\n",
       "  's2_paperId': '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4',\n",
       "  'title': 'Learning Transferable Visual Models From Natural Language Supervision',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 220,\n",
       "  'influentialCitationCount': 6398,\n",
       "  'citationCount': 26639,\n",
       "  'publicationType': 'Not available'},\n",
       " '43544230397e6ce2f051b9037cb48acedbd9506c': {'arxiv_id': None,\n",
       "  's2_paperId': '43544230397e6ce2f051b9037cb48acedbd9506c',\n",
       "  'title': 'Deep Interactive Thin Object Selection',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Workshop/Winter Conference on Applications of Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 42,\n",
       "  'publicationType': 'Not available'},\n",
       " '914a593b7f2e980470075a9955f1407641669a8f': {'arxiv_id': None,\n",
       "  's2_paperId': '914a593b7f2e980470075a9955f1407641669a8f',\n",
       "  'title': 'Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 73,\n",
       "  'influentialCitationCount': 88,\n",
       "  'citationCount': 944,\n",
       "  'publicationType': 'Not available'},\n",
       " '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a': {'arxiv_id': None,\n",
       "  's2_paperId': '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a',\n",
       "  'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 65,\n",
       "  'influentialCitationCount': 5005,\n",
       "  'citationCount': 37490,\n",
       "  'publicationType': 'Not available'},\n",
       " '7d2390de3665a067b1329cdbecc660c9353130cf': {'arxiv_id': None,\n",
       "  's2_paperId': '7d2390de3665a067b1329cdbecc660c9353130cf',\n",
       "  'title': 'SegFix: Model-Agnostic Boundary Refinement for Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 69,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 194,\n",
       "  'publicationType': 'Not available'},\n",
       " '90abbc2cf38462b954ae1b772fac9532e2ccd8b0': {'arxiv_id': '2005.14165v4',\n",
       "  's2_paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0',\n",
       "  'title': 'Language Models are Few-Shot Learners',\n",
       "  'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 146,\n",
       "  'influentialCitationCount': 4078,\n",
       "  'citationCount': 38728,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " 'd6abee937a353416cb985b7c454764b101b819df': {'arxiv_id': None,\n",
       "  's2_paperId': 'd6abee937a353416cb985b7c454764b101b819df',\n",
       "  'title': 'NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 20,\n",
       "  'publicationType': 'Not available'},\n",
       " '962dc29fdc3fbdc5930a10aba114050b82fe5a3e': {'arxiv_id': None,\n",
       "  's2_paperId': '962dc29fdc3fbdc5930a10aba114050b82fe5a3e',\n",
       "  'title': 'End-to-End Object Detection with Transformers',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 1477,\n",
       "  'citationCount': 12251,\n",
       "  'publicationType': 'Not available'},\n",
       " '1628a19ca39fd65ff4d9cb83cceceb99a3d864b5': {'arxiv_id': None,\n",
       "  's2_paperId': '1628a19ca39fd65ff4d9cb83cceceb99a3d864b5',\n",
       "  'title': 'CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 27,\n",
       "  'citationCount': 203,\n",
       "  'publicationType': 'Not available'},\n",
       " '47c92620b4b966cb320a8f8ea5b8dfc8065e8fa4': {'arxiv_id': None,\n",
       "  's2_paperId': '47c92620b4b966cb320a8f8ea5b8dfc8065e8fa4',\n",
       "  'title': 'PointRend: Image Segmentation As Rendering',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 120,\n",
       "  'citationCount': 871,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd08c7e556071e87eea263abe9393405501457ea4': {'arxiv_id': None,\n",
       "  's2_paperId': 'd08c7e556071e87eea263abe9393405501457ea4',\n",
       "  'title': 'Towards High-Resolution Salient Object Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 193,\n",
       "  'publicationType': 'Not available'},\n",
       " '0cb53a5ef780f037c1f2fd7de1f748b3f81726bc': {'arxiv_id': None,\n",
       "  's2_paperId': '0cb53a5ef780f037c1f2fd7de1f748b3f81726bc',\n",
       "  'title': 'FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 50,\n",
       "  'citationCount': 237,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f5268324962fea5d4ee4596da7e183b9172fe0ea': {'arxiv_id': None,\n",
       "  's2_paperId': 'f5268324962fea5d4ee4596da7e183b9172fe0ea',\n",
       "  'title': 'Gated-SCNN: Gated Shape CNNs for Semantic Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 60,\n",
       "  'influentialCitationCount': 66,\n",
       "  'citationCount': 599,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f902a64f7d08aaa6bfca7463e8729952ddc6134e': {'arxiv_id': None,\n",
       "  's2_paperId': 'f902a64f7d08aaa6bfca7463e8729952ddc6134e',\n",
       "  'title': 'LVIS: A Dataset for Large Vocabulary Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 296,\n",
       "  'citationCount': 1312,\n",
       "  'publicationType': 'Not available'},\n",
       " '24d70dbe19baa72a0c8481d5006d3632712ba688': {'arxiv_id': None,\n",
       "  's2_paperId': '24d70dbe19baa72a0c8481d5006d3632712ba688',\n",
       "  'title': 'Video Instance Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 184,\n",
       "  'citationCount': 496,\n",
       "  'publicationType': 'Not available'},\n",
       " '29ddc1f43f28af7c846515e32cc167bc66886d0c': {'arxiv_id': None,\n",
       "  's2_paperId': '29ddc1f43f28af7c846515e32cc167bc66886d0c',\n",
       "  'title': 'Parameter-Efficient Transfer Learning for NLP',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 681,\n",
       "  'citationCount': 4107,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e2e3c2740b7f48cab3b27f4c3c8474b1f676e1ff': {'arxiv_id': None,\n",
       "  's2_paperId': 'e2e3c2740b7f48cab3b27f4c3c8474b1f676e1ff',\n",
       "  'title': 'Semantic Segmentation Refinement by Monte Carlo Region Growing of High Confidence Detections',\n",
       "  'abstract': None,\n",
       "  'venue': 'Asian Conference on Computer Vision',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 30,\n",
       "  'publicationType': 'Not available'},\n",
       " '752771510a40076d47fd2952b72c4ae820c66081': {'arxiv_id': None,\n",
       "  's2_paperId': '752771510a40076d47fd2952b72c4ae820c66081',\n",
       "  'title': 'On Pre-Trained Image Features and Synthetic Images for Deep Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'ECCV Workshops',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 226,\n",
       "  'publicationType': 'Not available'},\n",
       " '35fe3dd3350c32467030884337dde10d5e20ff99': {'arxiv_id': None,\n",
       "  's2_paperId': '35fe3dd3350c32467030884337dde10d5e20ff99',\n",
       "  'title': 'ICNet for Real-Time Semantic Segmentation on High-Resolution Images',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 185,\n",
       "  'citationCount': 1381,\n",
       "  'publicationType': 'Not available'},\n",
       " '49e8fec24cce8b73706bc5fcd2c3f681addb9982': {'arxiv_id': None,\n",
       "  's2_paperId': '49e8fec24cce8b73706bc5fcd2c3f681addb9982',\n",
       "  'title': 'The 2017 DAVIS Challenge on Video Object Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 254,\n",
       "  'citationCount': 1166,\n",
       "  'publicationType': 'Not available'},\n",
       " 'de4ee92cfad3734ca820d004bc9ee75fc9dcfbf4': {'arxiv_id': None,\n",
       "  's2_paperId': 'de4ee92cfad3734ca820d004bc9ee75fc9dcfbf4',\n",
       "  'title': 'RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 253,\n",
       "  'citationCount': 2810,\n",
       "  'publicationType': 'Not available'},\n",
       " '7dbd0e26e1a3ec461b506fdf87a2b2d386580e2e': {'arxiv_id': None,\n",
       "  's2_paperId': '7dbd0e26e1a3ec461b506fdf87a2b2d386580e2e',\n",
       "  'title': 'Hierarchical Image Saliency Detection on Extended CSSD',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 77,\n",
       "  'citationCount': 535,\n",
       "  'publicationType': 'Not available'},\n",
       " '699d4d8c8b91b1a34dd6f47d28e3fe1236f25944': {'arxiv_id': None,\n",
       "  's2_paperId': '699d4d8c8b91b1a34dd6f47d28e3fe1236f25944',\n",
       "  'title': 'Saliency Detection via Graph-Based Manifold Ranking',\n",
       "  'abstract': None,\n",
       "  'venue': '2013 IEEE Conference on Computer Vision and Pattern Recognition',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 481,\n",
       "  'citationCount': 2294,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c81c20109c809cfc47565a9477c04ee005d424bf': {'arxiv_id': None,\n",
       "  's2_paperId': 'c81c20109c809cfc47565a9477c04ee005d424bf',\n",
       "  'title': 'Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 649,\n",
       "  'citationCount': 3421,\n",
       "  'publicationType': 'Not available'},\n",
       " '7670e3c493126c8b2f211ddfc45f89de905f1d38': {'arxiv_id': None,\n",
       "  's2_paperId': '7670e3c493126c8b2f211ddfc45f89de905f1d38',\n",
       "  'title': 'Global contrast based salient region detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 80,\n",
       "  'influentialCitationCount': 557,\n",
       "  'citationCount': 3700,\n",
       "  'publicationType': 'Not available'},\n",
       " '05f814dcce70844f250f9cd2ed4ee94aa3225ed0': {'arxiv_id': None,\n",
       "  's2_paperId': '05f814dcce70844f250f9cd2ed4ee94aa3225ed0',\n",
       "  'title': 'Fine-Grained Entity Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 60,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 12,\n",
       "  'publicationType': 'Not available'},\n",
       " '4b361a70a60a51de032c7bf8c37a67dc81340376': {'arxiv_id': None,\n",
       "  's2_paperId': '4b361a70a60a51de032c7bf8c37a67dc81340376',\n",
       "  'title': 'Class-Aware Visual Prompt Tuning for Vision-Language Pre-Trained Model',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 31,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 39,\n",
       "  'publicationType': 'Not available'},\n",
       " '53d8b356551a2361020a948f64454a6d599af69f': {'arxiv_id': None,\n",
       "  's2_paperId': '53d8b356551a2361020a948f64454a6d599af69f',\n",
       "  'title': 'Prefix-Tuning: Optimizing Continuous Prompts for Generation',\n",
       "  'abstract': 'Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.',\n",
       "  'venue': 'Annual Meeting of the Association for Computational Linguistics',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 398,\n",
       "  'citationCount': 3954,\n",
       "  'publicationType': 'Not available'},\n",
       " '4f9adfbca8bbc6c13b5345f4d9d14be1c81a9374': {'arxiv_id': '2102.06780v2',\n",
       "  's2_paperId': '4f9adfbca8bbc6c13b5345f4d9d14be1c81a9374',\n",
       "  'title': 'Newton Method over Networks is Fast up to the Statistical Precision',\n",
       "  'abstract': \"We propose a distributed cubic regularization of the Newton method for solving (constrained) empirical risk minimization problems over a network of agents, modeled as undirected graph. The algorithm employs an inexact, preconditioned Newton step at each agent's side: the gradient of the centralized loss is iteratively estimated via a gradient-tracking consensus mechanism and the Hessian is subsampled over the local data sets. No Hessian matrices are thus exchanged over the network. We derive global complexity bounds for convex and strongly convex losses. Our analysis reveals an interesting interplay between sample and iteration/communication complexity: statistically accurate solutions are achievable in roughly the same number of iterations of the centralized cubic Newton method, with a communication cost per iteration of the order of $\\\\widetilde{\\\\mathcal{O}}\\\\big(1/\\\\sqrt{1-\\\\rho}\\\\big)$, where $\\\\rho$ characterizes the connectivity of the network. This demonstrates a significant communication saving with respect to that of existing, statistically oblivious, distributed Newton-based methods over networks.\",\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 63,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 21,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'cbd997f823e5da3a0a34e2a1d7151a6af5406556': {'arxiv_id': None,\n",
       "  's2_paperId': 'cbd997f823e5da3a0a34e2a1d7151a6af5406556',\n",
       "  'title': 'An Accelerated Second-Order Method for Distributed Stochastic Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Conference on Decision and Control',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 20,\n",
       "  'publicationType': 'Not available'},\n",
       " '8bb7d430771dc7d37ba6f70c0681a3acba6ace66': {'arxiv_id': None,\n",
       "  's2_paperId': '8bb7d430771dc7d37ba6f70c0681a3acba6ace66',\n",
       "  'title': 'Hyperfast second-order local solvers for efficient statistically preconditioned distributed optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'EURO Journal on Computational Optimization',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 58,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 15,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e4779a2a916f0c425a18521d802658211d2c6653': {'arxiv_id': None,\n",
       "  's2_paperId': 'e4779a2a916f0c425a18521d802658211d2c6653',\n",
       "  'title': 'Recent theoretical advances in decentralized distributed convex optimization.',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 177,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 32,\n",
       "  'publicationType': 'Not available'},\n",
       " '5b0642ffce95c3d7c3eddb4d7a198528654130f7': {'arxiv_id': None,\n",
       "  's2_paperId': '5b0642ffce95c3d7c3eddb4d7a198528654130f7',\n",
       "  'title': 'Distributed Optimization Based on Gradient Tracking Revisited: Enhancing Convergence Rate via Surrogation',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Journal on Optimization',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 78,\n",
       "  'publicationType': 'Not available'},\n",
       " '0eea7b7cf9a0c3276a5e4de20350756868de95e7': {'arxiv_id': None,\n",
       "  's2_paperId': '0eea7b7cf9a0c3276a5e4de20350756868de95e7',\n",
       "  'title': 'Towards Accelerated Rates for Distributed Optimization over Time-Varying Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'OPTIMA',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 25,\n",
       "  'publicationType': 'Not available'},\n",
       " '96151f9274444e6c6a095570d2f4d9b01dd4d3a6': {'arxiv_id': None,\n",
       "  's2_paperId': '96151f9274444e6c6a095570d2f4d9b01dd4d3a6',\n",
       "  'title': 'A Newton Tracking Algorithm with Exact Linear Convergence Rate for Decentralized Consensus Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Conference on Decision and Control',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 3,\n",
       "  'publicationType': 'Not available'},\n",
       " '81c6dd8087685776399e6375a34b53f0d322d371': {'arxiv_id': None,\n",
       "  's2_paperId': '81c6dd8087685776399e6375a34b53f0d322d371',\n",
       "  'title': 'A Distributed Cubic-Regularized Newton Method for Smooth Convex Optimization over Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 70,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 5,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e98b4bfa54cebffdd9f21523192f86ec93492a98': {'arxiv_id': None,\n",
       "  's2_paperId': 'e98b4bfa54cebffdd9f21523192f86ec93492a98',\n",
       "  'title': 'Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 82,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bdd487bf83d99cb365b4d907885a6cc926ab06bb': {'arxiv_id': None,\n",
       "  's2_paperId': 'bdd487bf83d99cb365b4d907885a6cc926ab06bb',\n",
       "  'title': 'An Optimal Algorithm for Decentralized Finite Sum Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Journal on Optimization',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 45,\n",
       "  'publicationType': 'Not available'},\n",
       " '1420120d938b7641652303523b566099e7790d8e': {'arxiv_id': '2002.10726v1',\n",
       "  's2_paperId': '1420120d938b7641652303523b566099e7790d8e',\n",
       "  'title': 'Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization',\n",
       "  'abstract': 'We consider the setting of distributed empirical risk minimization where multiple machines compute the gradients in parallel and a centralized server updates the model parameters. In order to reduce the number of communications required to reach a given accuracy, we propose a \\\\emph{preconditioned} accelerated gradient method where the preconditioning is done by solving a local optimization problem over a subsampled dataset at the server. The convergence rate of the method depends on the square root of the relative condition number between the global and local loss functions. We estimate the relative condition number for linear prediction models by studying \\\\emph{uniform} concentration of the Hessians over a bounded domain, which allows us to derive improved convergence rates for existing preconditioned gradient methods and our accelerated method. Experiments on real-world datasets illustrate the benefits of acceleration in the ill-conditioned regime.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 58,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '12b397e8157d2f1aec991c3d39d7d1c7974333c5': {'arxiv_id': None,\n",
       "  's2_paperId': '12b397e8157d2f1aec991c3d39d7d1c7974333c5',\n",
       "  'title': 'Distributed adaptive Newton methods with global superlinear convergence',\n",
       "  'abstract': None,\n",
       "  'venue': 'at - Automatisierungstechnik',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 21,\n",
       "  'publicationType': 'Not available'},\n",
       " '14d9838b1e6144797ee5c0f27a14f938d965c533': {'arxiv_id': None,\n",
       "  's2_paperId': '14d9838b1e6144797ee5c0f27a14f938d965c533',\n",
       "  'title': 'Optimal complexity and certification of Bregman first-order methods',\n",
       "  'abstract': None,\n",
       "  'venue': 'Mathematical programming',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 72,\n",
       "  'publicationType': 'Not available'},\n",
       " '56043263214779867b57d13774e05388b810f953': {'arxiv_id': None,\n",
       "  's2_paperId': '56043263214779867b57d13774e05388b810f953',\n",
       "  'title': 'Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 97,\n",
       "  'publicationType': 'Not available'},\n",
       " '3a517ba70be8878ffce95b6ceb3d42efd2d22929': {'arxiv_id': None,\n",
       "  's2_paperId': '3a517ba70be8878ffce95b6ceb3d42efd2d22929',\n",
       "  'title': 'On Convergence of Distributed Approximate Newton Methods: Globalization, Sharper Bounds and Beyond',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 31,\n",
       "  'publicationType': 'Not available'},\n",
       " '60cb1e06b1aedbf3656e506db4d1e9986b140126': {'arxiv_id': None,\n",
       "  's2_paperId': '60cb1e06b1aedbf3656e506db4d1e9986b140126',\n",
       "  'title': 'Communication-Efficient Accurate Statistical Estimation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of the American Statistical Association',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 63,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 108,\n",
       "  'publicationType': 'Not available'},\n",
       " '56720d241cd9e1325f72724edd64e1e44e948a12': {'arxiv_id': None,\n",
       "  's2_paperId': '56720d241cd9e1325f72724edd64e1e44e948a12',\n",
       "  'title': 'DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 28,\n",
       "  'publicationType': 'Not available'},\n",
       " '15d70874bed0c2cd2ddb2f225fd62f09e9c47617': {'arxiv_id': None,\n",
       "  's2_paperId': '15d70874bed0c2cd2ddb2f225fd62f09e9c47617',\n",
       "  'title': 'Lectures on Convex Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 183,\n",
       "  'citationCount': 1013,\n",
       "  'publicationType': 'Not available'},\n",
       " '29a70dc873e66eb90e1aec2804ebe188f9158058': {'arxiv_id': None,\n",
       "  's2_paperId': '29a70dc873e66eb90e1aec2804ebe188f9158058',\n",
       "  'title': 'Efficient Distributed Hessian Free Algorithm for Large-scale Empirical Risk Minimization via Accumulating Sample Strategy',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 18,\n",
       "  'publicationType': 'Not available'},\n",
       " '9cd6c1b06dc182b25bdac67115be254e52a92b64': {'arxiv_id': None,\n",
       "  's2_paperId': '9cd6c1b06dc182b25bdac67115be254e52a92b64',\n",
       "  'title': 'A Primal-Dual Quasi-Newton Method for Exact Consensus Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Signal Processing',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 42,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd04cc3153e611d841cc14f590fb9e89ca8ea3f0c': {'arxiv_id': None,\n",
       "  's2_paperId': 'd04cc3153e611d841cc14f590fb9e89ca8ea3f0c',\n",
       "  'title': 'A Dual Approach for Optimal Algorithms in Distributed Optimization over Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Information Theory and Applications Workshop',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 100,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 137,\n",
       "  'publicationType': 'Not available'},\n",
       " '0b4283f264d3ceeb97d17ac621c0e5ad101ec1db': {'arxiv_id': None,\n",
       "  's2_paperId': '0b4283f264d3ceeb97d17ac621c0e5ad101ec1db',\n",
       "  'title': 'Optimal Algorithms for Non-Smooth Distributed Optimization in Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 166,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd96945e863d40480c0982df4a811e0e478b49b7d': {'arxiv_id': None,\n",
       "  's2_paperId': 'd96945e863d40480c0982df4a811e0e478b49b7d',\n",
       "  'title': 'Accelerated Gossip in Networks of Given Dimension Using Jacobi Polynomial Iterations',\n",
       "  'abstract': 'Consider a network of agents connected by communication links, where each agent holds a real value. The gossip problem consists in estimating the average of the values diffused in the network in a distributed manner. We develop a method solving the gossip problem that depends only on the spectral dimension of the network, that is, in the communication network set-up, the dimension of the space in which the agents live. This contrasts with previous work that required the spectral gap of the network as a parameter, or suffered from slow mixing. Our method shows an important improvement over existing algorithms in the non-asymptotic regime, i.e., when the values are far from being fully mixed in the network. Our approach stems from a polynomial-based point of view on gossip algorithms, as well as an approximation of the spectral measure of the graphs with a Jacobi measure. We show the power of the approach with simulations on various graphs, and with performance guarantees on graphs of known spectral dimension, such as grids and random percolation bonds. An extension of this work to distributed Laplacian solvers is discussed. As a side result, we also use the polynomial-based point of view to show the convergence of the message passing algorithm for gossip of Moallemi \\\\& Van Roy on regular graphs. The explicit computation of the rate of the convergence shows that message passing has a slow rate of convergence on graphs with small spectral gap.',\n",
       "  'venue': 'SIAM Journal on Mathematics of Data Science',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 45,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 29,\n",
       "  'publicationType': 'Not available'},\n",
       " '895490ae9498d620a7b69ea955c64a480ffbb4db': {'arxiv_id': None,\n",
       "  's2_paperId': '895490ae9498d620a7b69ea955c64a480ffbb4db',\n",
       "  'title': 'Convergence of Asynchronous Distributed Gradient Methods Over Stochastic Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Automatic Control',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 45,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 193,\n",
       "  'publicationType': 'Not available'},\n",
       " '2025ca4373efd5c8e5c04177bfda224364f3d682': {'arxiv_id': None,\n",
       "  's2_paperId': '2025ca4373efd5c8e5c04177bfda224364f3d682',\n",
       "  'title': 'Network Topology and Communication-Computation Tradeoffs in Decentralized Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Proceedings of the IEEE',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 123,\n",
       "  'influentialCitationCount': 45,\n",
       "  'citationCount': 500,\n",
       "  'publicationType': 'Not available'},\n",
       " '47110e1d295c419c38f5ac66bcea20f015abcdd7': {'arxiv_id': '1709.03528v5',\n",
       "  's2_paperId': '47110e1d295c419c38f5ac66bcea20f015abcdd7',\n",
       "  'title': 'GIANT: Globally Improved Approximate Newton Method for Distributed Optimization',\n",
       "  'abstract': 'For distributed computing environment, we consider the empirical risk minimization problem and propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, which is sent to the main driver. The main driver, then, averages all the ANT directions received from workers to form a {\\\\it Globally Improved ANT} (GIANT) direction. GIANT is highly communication efficient and naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. Theoretically, we show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Further, and in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. We conduct large-scale experiments on a computer cluster and, empirically, demonstrate the superior performance of GIANT.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 88,\n",
       "  'influentialCitationCount': 31,\n",
       "  'citationCount': 126,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '3f1ab8b484f7881a68c8562ff908390742e4ba90': {'arxiv_id': None,\n",
       "  's2_paperId': '3f1ab8b484f7881a68c8562ff908390742e4ba90',\n",
       "  'title': 'Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 273,\n",
       "  'citationCount': 1191,\n",
       "  'publicationType': 'Not available'},\n",
       " 'afa836396af0f989921a1eeda9521d009e595aff': {'arxiv_id': None,\n",
       "  's2_paperId': 'afa836396af0f989921a1eeda9521d009e595aff',\n",
       "  'title': 'Generalized self-concordant functions: a recipe for Newton-type methods',\n",
       "  'abstract': None,\n",
       "  'venue': 'Mathematical programming',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 87,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 60,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd4c79ceae77d2f65ccd43fd2694ccd5c8a519fa6': {'arxiv_id': None,\n",
       "  's2_paperId': 'd4c79ceae77d2f65ccd43fd2694ccd5c8a519fa6',\n",
       "  'title': 'Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 79,\n",
       "  'citationCount': 324,\n",
       "  'publicationType': 'Not available'},\n",
       " '11fa28d7c0802c2ae278e26ac13393707e4fc063': {'arxiv_id': None,\n",
       "  's2_paperId': '11fa28d7c0802c2ae278e26ac13393707e4fc063',\n",
       "  'title': 'Communication-efficient algorithms for decentralized and stochastic optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Mathematical programming',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 76,\n",
       "  'influentialCitationCount': 26,\n",
       "  'citationCount': 218,\n",
       "  'publicationType': 'Not available'},\n",
       " '2760f61ab90d815f572934b94fc410e30de54ff5': {'arxiv_id': None,\n",
       "  's2_paperId': '2760f61ab90d815f572934b94fc410e30de54ff5',\n",
       "  'title': 'Network Newton Distributed Optimization Methods',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Signal Processing',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 168,\n",
       "  'publicationType': 'Not available'},\n",
       " 'edc2e4e6308d7dfce586cb8a4441c704f8f8d41b': {'arxiv_id': None,\n",
       "  's2_paperId': 'edc2e4e6308d7dfce586cb8a4441c704f8f8d41b',\n",
       "  'title': 'AIDE: Fast and Communication Efficient Distributed Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 149,\n",
       "  'publicationType': 'Not available'},\n",
       " '4da06ee53eb97c863db1bc66c4bdb61e89f6c911': {'arxiv_id': None,\n",
       "  's2_paperId': '4da06ee53eb97c863db1bc66c4bdb61e89f6c911',\n",
       "  'title': 'Achieving Geometric Convergence for Distributed Optimization Over Time-Varying Graphs',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Journal on Optimization',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 78,\n",
       "  'influentialCitationCount': 168,\n",
       "  'citationCount': 924,\n",
       "  'publicationType': 'Not available'},\n",
       " 'eef115ce23c4939783c336746d3d07cb722e3cd8': {'arxiv_id': None,\n",
       "  's2_paperId': 'eef115ce23c4939783c336746d3d07cb722e3cd8',\n",
       "  'title': 'A Distributed Newton Method for Large Scale Consensus Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 63,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd21703674ae562bae4a849a75847cdd9ead417df': {'arxiv_id': None,\n",
       "  's2_paperId': 'd21703674ae562bae4a849a75847cdd9ead417df',\n",
       "  'title': 'Optimization Methods for Large-Scale Machine Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Review',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 183,\n",
       "  'influentialCitationCount': 429,\n",
       "  'citationCount': 3129,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c65a7911c3e7efc308b5bbb257d29b0ebcdaca1c': {'arxiv_id': '1605.07659v1',\n",
       "  's2_paperId': 'c65a7911c3e7efc308b5bbb257d29b0ebcdaca1c',\n",
       "  'title': 'Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy',\n",
       "  'abstract': \"We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton's method and reach the statistical accuracy of each training set with only one iteration of Newton's method. We show theoretically and empirically that Ada Newton can double the size of the training set in each iteration to achieve the statistical accuracy of the full training set with about two passes over the dataset.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 32,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'abf5dc2a254ff0b12a8fed6e7563ed4f75a31766': {'arxiv_id': None,\n",
       "  's2_paperId': 'abf5dc2a254ff0b12a8fed6e7563ed4f75a31766',\n",
       "  'title': 'Distributed Inexact Damped Newton Method: Data Partitioning and Load-Balancing',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 10,\n",
       "  'publicationType': 'Not available'},\n",
       " '79df60bd7a19fe22b8bdedee1bb912e9ee5bb605': {'arxiv_id': None,\n",
       "  's2_paperId': '79df60bd7a19fe22b8bdedee1bb912e9ee5bb605',\n",
       "  'title': 'A Decentralized Second-Order Method with Exact Linear Convergence Rate for Consensus Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Signal and Information Processing over Networks',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 124,\n",
       "  'publicationType': 'Not available'},\n",
       " '677a9e728b57eb470729774abfe611a00035bb20': {'arxiv_id': None,\n",
       "  's2_paperId': '677a9e728b57eb470729774abfe611a00035bb20',\n",
       "  'title': 'NEXT: In-Network Nonconvex Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Signal and Information Processing over Networks',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 61,\n",
       "  'influentialCitationCount': 52,\n",
       "  'citationCount': 494,\n",
       "  'publicationType': 'Not available'},\n",
       " '51231adf5f25ec2cc36f38ccc0d614138785b0be': {'arxiv_id': None,\n",
       "  's2_paperId': '51231adf5f25ec2cc36f38ccc0d614138785b0be',\n",
       "  'title': 'DQM: Decentralized Quadratically Approximated Alternating Direction Method of Multipliers',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Signal Processing',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 118,\n",
       "  'publicationType': 'Not available'},\n",
       " '395477b4e1bdc97c4584990bb602baebd86be52b': {'arxiv_id': None,\n",
       "  's2_paperId': '395477b4e1bdc97c4584990bb602baebd86be52b',\n",
       "  'title': 'Decentralized quadratically approximated alternating direction method of multipliers',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Global Conference on Signal and Information Processing',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 24,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fcc9a75d0cd12cfce81fcd22b22867f76b258e0c': {'arxiv_id': None,\n",
       "  's2_paperId': 'fcc9a75d0cd12cfce81fcd22b22867f76b258e0c',\n",
       "  'title': 'DiSCO: Distributed Optimization for Self-Concordant Empirical Loss',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 219,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bd2cb4546fc01074d55a183a68ce0a0f7be43a43': {'arxiv_id': None,\n",
       "  's2_paperId': 'bd2cb4546fc01074d55a183a68ce0a0f7be43a43',\n",
       "  'title': 'Communication Complexity of Distributed Convex Learning and Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 204,\n",
       "  'publicationType': 'Not available'},\n",
       " '9262925099c58cbb9550eaeca6287e8b70d2f2d0': {'arxiv_id': None,\n",
       "  's2_paperId': '9262925099c58cbb9550eaeca6287e8b70d2f2d0',\n",
       "  'title': 'Competing with the Empirical Risk Minimizer in a Single Pass',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 100,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e5f34a155fb190ec70f0f79c7bc8c9a39b1feb49': {'arxiv_id': None,\n",
       "  's2_paperId': 'e5f34a155fb190ec70f0f79c7bc8c9a39b1feb49',\n",
       "  'title': 'EXTRA: An Exact First-Order Algorithm for Decentralized Consensus Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 172,\n",
       "  'citationCount': 1087,\n",
       "  'publicationType': 'Not available'},\n",
       " '8467bb300494a1f5c13fa3248e2cd630a6033fae': {'arxiv_id': None,\n",
       "  's2_paperId': '8467bb300494a1f5c13fa3248e2cd630a6033fae',\n",
       "  'title': 'Communication-Efficient Distributed Optimization using an Approximate Newton-type Method',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 72,\n",
       "  'citationCount': 545,\n",
       "  'publicationType': 'Not available'},\n",
       " '43c05444fbc239321f6676f3cd539cac34fde7b8': {'arxiv_id': None,\n",
       "  's2_paperId': '43c05444fbc239321f6676f3cd539cac34fde7b8',\n",
       "  'title': 'Accelerating Stochastic Gradient Descent using Predictive Variance Reduction',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 6,\n",
       "  'influentialCitationCount': 543,\n",
       "  'citationCount': 2709,\n",
       "  'publicationType': 'Not available'},\n",
       " '4596a8afaf178590ad920346970dc6b9ed7aa580': {'arxiv_id': None,\n",
       "  's2_paperId': '4596a8afaf178590ad920346970dc6b9ed7aa580',\n",
       "  'title': 'A Distributed Newton Method for Network Utility Maximization—Part II: Convergence',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Automatic Control',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 31,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 66,\n",
       "  'publicationType': 'Not available'},\n",
       " '10d37edda8fc00d74789786a1abf25b3add9ac3d': {'arxiv_id': None,\n",
       "  's2_paperId': '10d37edda8fc00d74789786a1abf25b3add9ac3d',\n",
       "  'title': 'Fast Distributed Gradient Methods',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Automatic Control',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 39,\n",
       "  'citationCount': 581,\n",
       "  'publicationType': 'Not available'},\n",
       " '87a22424c8995f2d6f65b49a3f59eb1b712e8ed7': {'arxiv_id': None,\n",
       "  's2_paperId': '87a22424c8995f2d6f65b49a3f59eb1b712e8ed7',\n",
       "  'title': 'Scaling up machine learning: parallel and distributed approaches',\n",
       "  'abstract': None,\n",
       "  'venue': \"KDD '11 Tutorials\",\n",
       "  'year': 2011,\n",
       "  'referenceCount': 6,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 439,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a2ee50c951c5b8cf863d9f82ef0bcf6123806b9d': {'arxiv_id': None,\n",
       "  's2_paperId': 'a2ee50c951c5b8cf863d9f82ef0bcf6123806b9d',\n",
       "  'title': 'A distributed newton method for network optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Conference on Decision and Control',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 97,\n",
       "  'publicationType': 'Not available'},\n",
       " '7523c6f65774b4a3e44b1ea51bcb565800f74480': {'arxiv_id': None,\n",
       "  's2_paperId': '7523c6f65774b4a3e44b1ea51bcb565800f74480',\n",
       "  'title': 'Self-concordant analysis for logistic regression',\n",
       "  'abstract': 'Most of the non-asymptotic theoretical work in regression is carried out for the square loss, where estimators can be obtained through closed-form expressions. In this paper, we use and extend tools from the convex optimization literature, namely self-concordant functions, to provide simple extensions of theoretical results for the square loss to the logistic loss. We apply the extension techniques to logistic regression with regularization by the $\\\\ell_2$-norm and regularization by the $\\\\ell_1$-norm, showing that new results for binary classification through logistic regression can be easily derived from corresponding results for least-squares regression.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 46,\n",
       "  'citationCount': 206,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a4cf35b6772b57aa838f7d6d0daad83d3e36cbb6': {'arxiv_id': None,\n",
       "  's2_paperId': 'a4cf35b6772b57aa838f7d6d0daad83d3e36cbb6',\n",
       "  'title': 'Fast Rates for Regularized Objectives',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 143,\n",
       "  'publicationType': 'Not available'},\n",
       " '91a8c8058e41dae7863c30386638bdfbe77ab6b7': {'arxiv_id': None,\n",
       "  's2_paperId': '91a8c8058e41dae7863c30386638bdfbe77ab6b7',\n",
       "  'title': 'Cubic regularization of Newton method and its global performance',\n",
       "  'abstract': None,\n",
       "  'venue': 'Mathematical programming',\n",
       "  'year': 2006,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 213,\n",
       "  'citationCount': 1041,\n",
       "  'publicationType': 'Not available'},\n",
       " '1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435': {'arxiv_id': None,\n",
       "  's2_paperId': '1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435',\n",
       "  'title': 'Convexity, Classification, and Risk Bounds',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2006,\n",
       "  'referenceCount': 60,\n",
       "  'influentialCitationCount': 217,\n",
       "  'citationCount': 1453,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a5c8ca9177054499e43898ac73d1c1b6e2a45928': {'arxiv_id': None,\n",
       "  's2_paperId': 'a5c8ca9177054499e43898ac73d1c1b6e2a45928',\n",
       "  'title': 'Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of Learning Algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2002,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 109,\n",
       "  'publicationType': 'Not available'},\n",
       " '1fd2493d1ed77e0e9f29aae20e9353fec8aefadc': {'arxiv_id': None,\n",
       "  's2_paperId': '1fd2493d1ed77e0e9f29aae20e9353fec8aefadc',\n",
       "  'title': 'Efficient SVM Regression Training with SMO',\n",
       "  'abstract': None,\n",
       "  'venue': 'Machine-mediated learning',\n",
       "  'year': 2002,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 313,\n",
       "  'publicationType': 'Not available'},\n",
       " '20f63033e8775cbab0692aed92d38da7e725d64e': {'arxiv_id': None,\n",
       "  's2_paperId': '20f63033e8775cbab0692aed92d38da7e725d64e',\n",
       "  'title': 'Understanding Machine Learning - From Theory to Algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 520,\n",
       "  'citationCount': 3002,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e442218643570755880de25030670c15f78c5e68': {'arxiv_id': None,\n",
       "  's2_paperId': 'e442218643570755880de25030670c15f78c5e68',\n",
       "  'title': 'Stochastic Convex Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 56,\n",
       "  'citationCount': 326,\n",
       "  'publicationType': 'Not available'},\n",
       " '5359fb2362ee22a18a5cc1bf9ff7f69d7ce533bf': {'arxiv_id': None,\n",
       "  's2_paperId': '5359fb2362ee22a18a5cc1bf9ff7f69d7ce533bf',\n",
       "  'title': 'Distributed average consensus with least-mean-square deviation',\n",
       "  'abstract': None,\n",
       "  'venue': 'J. Parallel Distributed Comput.',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 57,\n",
       "  'citationCount': 1134,\n",
       "  'publicationType': 'Not available'},\n",
       " '8213dbed4db44e113af3ed17d6dad57471a0c048': {'arxiv_id': None,\n",
       "  's2_paperId': '8213dbed4db44e113af3ed17d6dad57471a0c048',\n",
       "  'title': 'The Nature of Statistical Learning Theory',\n",
       "  'abstract': None,\n",
       "  'venue': 'Statistics for Engineering and Information Science',\n",
       "  'year': 2000,\n",
       "  'referenceCount': 71,\n",
       "  'influentialCitationCount': 4379,\n",
       "  'citationCount': 44300,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c9501daadbd0f9b0c2ec33c133826f3e27df0cf7': {'arxiv_id': None,\n",
       "  's2_paperId': 'c9501daadbd0f9b0c2ec33c133826f3e27df0cf7',\n",
       "  'title': 'The Hebrew University',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1998,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 28,\n",
       "  'citationCount': 247,\n",
       "  'publicationType': 'Not available'},\n",
       " '68575c273ff20a1045a89771f3e347c1f8a06e25': {'arxiv_id': None,\n",
       "  's2_paperId': '68575c273ff20a1045a89771f3e347c1f8a06e25',\n",
       "  'title': 'Iterative Solution of Large Linear Systems.',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1973,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 211,\n",
       "  'citationCount': 2632,\n",
       "  'publicationType': 'Not available'},\n",
       " '3ec05b4d0b6a5d9f68b02b820e12a89f6abe1bec': {'arxiv_id': '1904.13323v2',\n",
       "  's2_paperId': '3ec05b4d0b6a5d9f68b02b820e12a89f6abe1bec',\n",
       "  'title': 'Model Comparison for Semantic Grouping',\n",
       "  'abstract': 'We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate the task of semantic similarity as a model comparison task in which we contrast a generative model which jointly models two sentences versus one that does not. We illustrate how this framework can be used for the Semantic Textual Similarity tasks using clear assumptions about how the embeddings of words are generated. We apply model comparison that utilises information criteria to address some of the shortcomings of Bayesian model comparison, whilst still penalising model complexity. We achieve competitive results by applying the proposed framework with an appropriate choice of likelihood on the STS datasets.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 1,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '117b08d70961242283afdb412d9ab9004262d9d5': {'arxiv_id': None,\n",
       "  's2_paperId': '117b08d70961242283afdb412d9ab9004262d9d5',\n",
       "  'title': \"Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors\",\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 69,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 40,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f605379a39f727dc29568f63fdf2f2454123296d': {'arxiv_id': None,\n",
       "  's2_paperId': 'f605379a39f727dc29568f63fdf2f2454123296d',\n",
       "  'title': 'Sensitivity and Specificity of Information Criteria',\n",
       "  'abstract': 'Information criteria (ICs) based on penalized likelihood, such as Akaike’s Information Criterion (AIC), the Bayesian Information Criterion (BIC), and sample-size-adjusted versions of them, are widely used for model selection in health and biological research. However, different criteria sometimes support different models, leading to discussions about which is the most trustworthy. Some researchers and fields of study habitually use one or the other, often without a clearly stated justification. They may not realize that the criteria may disagree. Others try to compare models using multiple criteria but encounter ambiguity when different criteria lead to substantively different answers, leading to questions about which criterion is best. In this paper we present an alternative perspective on these criteria that can help in interpreting their practical implications. Specifically, in some cases the comparison of two models using ICs can be viewed as equivalent to a likelihood ratio test, with the different criteria representing different alpha levels and BIC being a more conservative test than AIC. This perspective may lead to insights about how to interpret the ICs in more complex situations. For example, AIC or BIC could be preferable, depending on the relative importance one assigns to sensitivity versus specificity. Understanding the differences and similarities among the ICs can make it easier to compare their results and to use them to make informed decisions.',\n",
       "  'venue': 'bioRxiv',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 148,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 575,\n",
       "  'publicationType': 'Not available'},\n",
       " '7113bd87c3e6f727efae24ee52f20c81358da761': {'arxiv_id': None,\n",
       "  's2_paperId': '7113bd87c3e6f727efae24ee52f20c81358da761',\n",
       "  'title': 'SentEval: An Evaluation Toolkit for Universal Sentence Representations',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Language Resources and Evaluation',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 92,\n",
       "  'citationCount': 624,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e9839782321e25ed87f1a14abf7107e0d037e630': {'arxiv_id': None,\n",
       "  's2_paperId': 'e9839782321e25ed87f1a14abf7107e0d037e630',\n",
       "  'title': 'Multimodal Word Distributions',\n",
       "  'abstract': 'Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.',\n",
       "  'venue': 'Annual Meeting of the Association for Computational Linguistics',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 90,\n",
       "  'publicationType': 'Not available'},\n",
       " '3f1802d3f4f5f6d66875dac09112f978f12e1e1e': {'arxiv_id': None,\n",
       "  's2_paperId': '3f1802d3f4f5f6d66875dac09112f978f12e1e1e',\n",
       "  'title': 'A Simple but Tough-to-Beat Baseline for Sentence Embeddings',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 186,\n",
       "  'citationCount': 1287,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e2dba792360873aef125572812f3673b1a85d850': {'arxiv_id': None,\n",
       "  's2_paperId': 'e2dba792360873aef125572812f3673b1a85d850',\n",
       "  'title': 'Enriching Word Vectors with Subword Information',\n",
       "  'abstract': 'Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.',\n",
       "  'venue': 'Transactions of the Association for Computational Linguistics',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 999,\n",
       "  'citationCount': 9824,\n",
       "  'publicationType': 'Not available'},\n",
       " '395044a2e3f5624b2471fb28826e7dbb1009356e': {'arxiv_id': None,\n",
       "  's2_paperId': '395044a2e3f5624b2471fb28826e7dbb1009356e',\n",
       "  'title': 'Towards Universal Paraphrastic Sentence Embeddings',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 82,\n",
       "  'influentialCitationCount': 66,\n",
       "  'citationCount': 553,\n",
       "  'publicationType': 'Not available'},\n",
       " '958d165f8bb77838ec915d4f214a2310e3adde19': {'arxiv_id': None,\n",
       "  's2_paperId': '958d165f8bb77838ec915d4f214a2310e3adde19',\n",
       "  'title': 'Measuring Word Significance using Distributed Representations of Words',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 89,\n",
       "  'publicationType': 'Not available'},\n",
       " '30a1ef813c92eb783792d2c62e8295fbae5d74b8': {'arxiv_id': None,\n",
       "  's2_paperId': '30a1ef813c92eb783792d2c62e8295fbae5d74b8',\n",
       "  'title': 'Learning to Reweight Terms with Distributed Representations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 133,\n",
       "  'publicationType': 'Not available'},\n",
       " '66021a920001bc3e6258bffe7076d647614147b7': {'arxiv_id': None,\n",
       "  's2_paperId': '66021a920001bc3e6258bffe7076d647614147b7',\n",
       "  'title': 'From Word Embeddings To Document Distances',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 386,\n",
       "  'citationCount': 2066,\n",
       "  'publicationType': 'Not available'},\n",
       " '6e795c6e9916174ae12349f5dc3f516570c17ce8': {'arxiv_id': '1506.06726v1',\n",
       "  's2_paperId': '6e795c6e9916174ae12349f5dc3f516570c17ce8',\n",
       "  'title': 'Skip-Thought Vectors',\n",
       "  'abstract': 'We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 268,\n",
       "  'citationCount': 2391,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '6346f4fb03a25ff3a4bd887ba45d627a01983ea4': {'arxiv_id': None,\n",
       "  's2_paperId': '6346f4fb03a25ff3a4bd887ba45d627a01983ea4',\n",
       "  'title': 'SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability',\n",
       "  'abstract': 'In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new datasets in English and Spanish. The annotations for both subtasks leveraged crowdsourcing. The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs. In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair. The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years. 7 teams participated with 29 runs.',\n",
       "  'venue': 'International Workshop on Semantic Evaluation',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 46,\n",
       "  'citationCount': 599,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f37e1b62a767a307c046404ca96bc140b3e68cb5': {'arxiv_id': None,\n",
       "  's2_paperId': 'f37e1b62a767a307c046404ca96bc140b3e68cb5',\n",
       "  'title': 'GloVe: Global Vectors for Word Representation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Conference on Empirical Methods in Natural Language Processing',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 3876,\n",
       "  'citationCount': 31573,\n",
       "  'publicationType': 'Not available'},\n",
       " '16084914bc3729f86f46ac6267ea7a42e7951d41': {'arxiv_id': None,\n",
       "  's2_paperId': '16084914bc3729f86f46ac6267ea7a42e7951d41',\n",
       "  'title': 'SemEval-2014 Task 10: Multilingual Semantic Textual Similarity',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Workshop on Semantic Evaluation',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 40,\n",
       "  'citationCount': 560,\n",
       "  'publicationType': 'Not available'},\n",
       " '87f40e6f3022adbc1f1905e3e506abad05a9964f': {'arxiv_id': None,\n",
       "  's2_paperId': '87f40e6f3022adbc1f1905e3e506abad05a9964f',\n",
       "  'title': 'Distributed Representations of Words and Phrases and their Compositionality',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 4059,\n",
       "  'citationCount': 33118,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ff5cbefc6766df788919db4c060daedb303ed3e3': {'arxiv_id': None,\n",
       "  's2_paperId': 'ff5cbefc6766df788919db4c060daedb303ed3e3',\n",
       "  'title': '*SEM 2013 shared task: Semantic Textual Similarity',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Workshop on Semantic Evaluation',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 12,\n",
       "  'influentialCitationCount': 58,\n",
       "  'citationCount': 747,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f6b51c8753a871dc94ff32152c00c01e94f90f09': {'arxiv_id': None,\n",
       "  's2_paperId': 'f6b51c8753a871dc94ff32152c00c01e94f90f09',\n",
       "  'title': 'Efficient Estimation of Word Representations in Vector Space',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 4151,\n",
       "  'citationCount': 30940,\n",
       "  'publicationType': 'Not available'},\n",
       " '7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917': {'arxiv_id': None,\n",
       "  's2_paperId': '7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917',\n",
       "  'title': 'A Comparison of Vector-based Representations for Semantic Composition',\n",
       "  'abstract': None,\n",
       "  'venue': 'Conference on Empirical Methods in Natural Language Processing',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 294,\n",
       "  'publicationType': 'Not available'},\n",
       " '528fa9bb03644ba752fb9491be49b9dd1bce1d52': {'arxiv_id': None,\n",
       "  's2_paperId': '528fa9bb03644ba752fb9491be49b9dd1bce1d52',\n",
       "  'title': 'SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Workshop on Semantic Evaluation',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 10,\n",
       "  'influentialCitationCount': 100,\n",
       "  'citationCount': 895,\n",
       "  'publicationType': 'Not available'},\n",
       " '225f78ae8a44723c136646044fd5c5d7f1d3d15a': {'arxiv_id': None,\n",
       "  's2_paperId': '225f78ae8a44723c136646044fd5c5d7f1d3d15a',\n",
       "  'title': 'A Kernel Two-Sample Test',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 97,\n",
       "  'influentialCitationCount': 1037,\n",
       "  'citationCount': 5015,\n",
       "  'publicationType': 'Not available'},\n",
       " '745d86adca56ec50761591733e157f84cfb19671': {'arxiv_id': None,\n",
       "  's2_paperId': '745d86adca56ec50761591733e157f84cfb19671',\n",
       "  'title': 'Composition in Distributional Models of Semantics',\n",
       "  'abstract': None,\n",
       "  'venue': 'Cognitive Sciences',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 250,\n",
       "  'influentialCitationCount': 102,\n",
       "  'citationCount': 1005,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b5d67d1dc671bce42a9daac0c3605adb3fcfc697': {'arxiv_id': None,\n",
       "  's2_paperId': 'b5d67d1dc671bce42a9daac0c3605adb3fcfc697',\n",
       "  'title': 'Vector-based Models of Semantic Composition',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Meeting of the Association for Computational Linguistics',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 126,\n",
       "  'influentialCitationCount': 86,\n",
       "  'citationCount': 784,\n",
       "  'publicationType': 'Not available'},\n",
       " '0c9976a0360654b4508a8dc78d039ab73b32d57f': {'arxiv_id': None,\n",
       "  's2_paperId': '0c9976a0360654b4508a8dc78d039ab73b32d57f',\n",
       "  'title': 'Information Criteria and Statistical Modeling',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 63,\n",
       "  'citationCount': 870,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ffbd6c56c3c8a99bad668746cf41c34877201e55': {'arxiv_id': None,\n",
       "  's2_paperId': 'ffbd6c56c3c8a99bad668746cf41c34877201e55',\n",
       "  'title': 'Bayesian Sets',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 3,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 157,\n",
       "  'publicationType': 'Not available'},\n",
       " '11de3f9770b08484b28597deb17714fb107caafe': {'arxiv_id': None,\n",
       "  's2_paperId': '11de3f9770b08484b28597deb17714fb107caafe',\n",
       "  'title': 'Clustering on the Unit Hypersphere using von Mises-Fisher Distributions',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 67,\n",
       "  'influentialCitationCount': 114,\n",
       "  'citationCount': 967,\n",
       "  'publicationType': 'Not available'},\n",
       " '1f3ba8f37791f548f5e2f82d3376c40e44133566': {'arxiv_id': None,\n",
       "  's2_paperId': '1f3ba8f37791f548f5e2f82d3376c40e44133566',\n",
       "  'title': 'Bayesian evidence as a tool for comparing datasets',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 82,\n",
       "  'publicationType': 'Not available'},\n",
       " 'caa359864efb289d43afc50f2020cf38dcf6fad2': {'arxiv_id': None,\n",
       "  's2_paperId': 'caa359864efb289d43afc50f2020cf38dcf6fad2',\n",
       "  'title': 'Generalization, similarity, and Bayesian inference.',\n",
       "  'abstract': None,\n",
       "  'venue': 'Behavioral and Brain Sciences',\n",
       "  'year': 2001,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 53,\n",
       "  'citationCount': 745,\n",
       "  'publicationType': 'Not available'},\n",
       " '1ff21f3780dec0d019f5ce5ba648fdbc2d1a3767': {'arxiv_id': None,\n",
       "  's2_paperId': '1ff21f3780dec0d019f5ce5ba648fdbc2d1a3767',\n",
       "  'title': 'Bayesian Analysis: A Look at Today and Thoughts of Tomorrow',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2000,\n",
       "  'referenceCount': 118,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 138,\n",
       "  'publicationType': 'Not available'},\n",
       " '41be484549d47db42bf3baea98b7997eab6e7125': {'arxiv_id': None,\n",
       "  's2_paperId': '41be484549d47db42bf3baea98b7997eab6e7125',\n",
       "  'title': 'Statistical Analysis of Spherical Data.',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1987,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 105,\n",
       "  'citationCount': 1301,\n",
       "  'publicationType': 'Not available'},\n",
       " '9361fcb56fa47a735db40c7d53f1a72eecfc766d': {'arxiv_id': None,\n",
       "  's2_paperId': '9361fcb56fa47a735db40c7d53f1a72eecfc766d',\n",
       "  'title': 'Likelihood of a model and information criteria',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1981,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 154,\n",
       "  'citationCount': 1246,\n",
       "  'publicationType': 'Not available'},\n",
       " '37e44d1de8003d8394d158ec6afd1ff0e87e595b': {'arxiv_id': None,\n",
       "  's2_paperId': '37e44d1de8003d8394d158ec6afd1ff0e87e595b',\n",
       "  'title': 'Estimating the Dimension of a Model',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1978,\n",
       "  'referenceCount': 4,\n",
       "  'influentialCitationCount': 5316,\n",
       "  'citationCount': 45761,\n",
       "  'publicationType': 'Not available'},\n",
       " '50a42ed2f81b9fe150883a6c89194c88a9647106': {'arxiv_id': None,\n",
       "  's2_paperId': '50a42ed2f81b9fe150883a6c89194c88a9647106',\n",
       "  'title': 'A new look at the statistical model identification',\n",
       "  'abstract': 'The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.',\n",
       "  'venue': '',\n",
       "  'year': 1974,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 6801,\n",
       "  'citationCount': 48786,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c79f5e192f892eb3bb0cf4d855876e1c6657bf00': {'arxiv_id': None,\n",
       "  's2_paperId': 'c79f5e192f892eb3bb0cf4d855876e1c6657bf00',\n",
       "  'title': \"A comment on D. V. Lindley's statistical paradox\",\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1957,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 314,\n",
       "  'publicationType': 'Not available'},\n",
       " '3e555b6054aa6fd4b46f87cd632668aaf8a91db6': {'arxiv_id': None,\n",
       "  's2_paperId': '3e555b6054aa6fd4b46f87cd632668aaf8a91db6',\n",
       "  'title': 'SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation',\n",
       "  'abstract': 'Comunicacio presentada al 10th International Workshop on Semantic Evaluation (SemEval-2016), celebrat els dies 16 i 17 de juny de 2016 a San Diego, California.',\n",
       "  'venue': 'International Workshop on Semantic Evaluation',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 46,\n",
       "  'citationCount': 611,\n",
       "  'publicationType': 'Not available'},\n",
       " '84693723402885c38a5408a123ca82fa8f742f8d': {'arxiv_id': None,\n",
       "  's2_paperId': '84693723402885c38a5408a123ca82fa8f742f8d',\n",
       "  'title': 'Distribution Theory for the Von Mises-Fisher Distribution and Its Application',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1975,\n",
       "  'referenceCount': 8,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 33,\n",
       "  'publicationType': 'Not available'},\n",
       " '7316596b1f02f288e3b76546d90646524e35fd40': {'arxiv_id': '2305.12529v3',\n",
       "  's2_paperId': '7316596b1f02f288e3b76546d90646524e35fd40',\n",
       "  'title': 'DreamWaltz: Make a Scene with Complex 3D Animatable Avatars',\n",
       "  'abstract': 'We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable and generalizable avatar representation which could map arbitrary poses to the canonical pose representation. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The proposed framework further enables the creation of complex scenes with diverse compositions, including avatar-avatar, avatar-object and avatar-scene interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and animation results.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 69,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '32d3048a4fe4becc7c4638afd05f2354b631cfca': {'arxiv_id': None,\n",
       "  's2_paperId': '32d3048a4fe4becc7c4638afd05f2354b631cfca',\n",
       "  'title': 'SMPL: A Skinned Multi-Person Linear Model',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 569,\n",
       "  'citationCount': 3431,\n",
       "  'publicationType': 'Not available'},\n",
       " '8dfe271d2186d5746d034b3cce12131f4d3f45f7': {'arxiv_id': None,\n",
       "  's2_paperId': '8dfe271d2186d5746d034b3cce12131f4d3f45f7',\n",
       "  'title': 'DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 63,\n",
       "  'publicationType': 'Not available'},\n",
       " '1ea2140567bbed461c19ff02d0dd193c6709f4da': {'arxiv_id': None,\n",
       "  's2_paperId': '1ea2140567bbed461c19ff02d0dd193c6709f4da',\n",
       "  'title': 'HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 83,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b5fb909d436856ba7c4d5e15bfdb83a847e7ff8a': {'arxiv_id': None,\n",
       "  's2_paperId': 'b5fb909d436856ba7c4d5e15bfdb83a847e7ff8a',\n",
       "  'title': 'MonoHuman: Animatable Human Neural Field from Monocular Video',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 64,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 72,\n",
       "  'publicationType': 'Not available'},\n",
       " '0fa1501c7378a0dca2ac913fce9dcdcc2b1958a7': {'arxiv_id': None,\n",
       "  's2_paperId': '0fa1501c7378a0dca2ac913fce9dcdcc2b1958a7',\n",
       "  'title': 'DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 72,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 131,\n",
       "  'publicationType': 'Not available'},\n",
       " '836f0d803332853bb12a89495ea30f0e91c97bf6': {'arxiv_id': None,\n",
       "  's2_paperId': '836f0d803332853bb12a89495ea30f0e91c97bf6',\n",
       "  'title': 'AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 64,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 73,\n",
       "  'publicationType': 'Not available'},\n",
       " '0cbb518c364067200476a51e5ce7476a4f582770': {'arxiv_id': None,\n",
       "  's2_paperId': '0cbb518c364067200476a51e5ce7476a4f582770',\n",
       "  'title': 'Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 85,\n",
       "  'citationCount': 539,\n",
       "  'publicationType': 'Not available'},\n",
       " '26e5b933b8f60bd749d428b5ff813b2abcd765d8': {'arxiv_id': None,\n",
       "  's2_paperId': '26e5b933b8f60bd749d428b5ff813b2abcd765d8',\n",
       "  'title': 'Composer: Creative and Controllable Image Synthesis with Composable Conditions',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 268,\n",
       "  'publicationType': 'Not available'},\n",
       " 'efbe97d20c4ffe356e8826c01dc550bacc405add': {'arxiv_id': None,\n",
       "  's2_paperId': 'efbe97d20c4ffe356e8826c01dc550bacc405add',\n",
       "  'title': 'Adding Conditional Control to Text-to-Image Diffusion Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 120,\n",
       "  'influentialCitationCount': 671,\n",
       "  'citationCount': 3748,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fc011ed5ee986332523a62d2783adee1179dc1ed': {'arxiv_id': None,\n",
       "  's2_paperId': 'fc011ed5ee986332523a62d2783adee1179dc1ed',\n",
       "  'title': 'Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 74,\n",
       "  'influentialCitationCount': 49,\n",
       "  'citationCount': 517,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bdf4af8311637c681904e71cf50f96fd0026f578': {'arxiv_id': None,\n",
       "  's2_paperId': 'bdf4af8311637c681904e71cf50f96fd0026f578',\n",
       "  'title': 'Magic3D: High-Resolution Text-to-3D Content Creation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 132,\n",
       "  'citationCount': 1081,\n",
       "  'publicationType': 'Not available'},\n",
       " '793939b83e10903f58d8edbb7534963df627a1fe': {'arxiv_id': None,\n",
       "  's2_paperId': '793939b83e10903f58d8edbb7534963df627a1fe',\n",
       "  'title': 'Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 66,\n",
       "  'citationCount': 437,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9': {'arxiv_id': None,\n",
       "  's2_paperId': 'e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9',\n",
       "  'title': 'LAION-5B: An open large-scale dataset for training next generation image-text models',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 109,\n",
       "  'influentialCitationCount': 326,\n",
       "  'citationCount': 3161,\n",
       "  'publicationType': 'Not available'},\n",
       " '4c94d04afa4309ec2f06bdd0fe3781f91461b362': {'arxiv_id': None,\n",
       "  's2_paperId': '4c94d04afa4309ec2f06bdd0fe3781f91461b362',\n",
       "  'title': 'DreamFusion: Text-to-3D using 2D Diffusion',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 76,\n",
       "  'influentialCitationCount': 514,\n",
       "  'citationCount': 2244,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a422900dbccc0b01bbbcd735ea54001a44f67615': {'arxiv_id': None,\n",
       "  's2_paperId': 'a422900dbccc0b01bbbcd735ea54001a44f67615',\n",
       "  'title': 'Neural Capture of Animatable 3D Human from Monocular Video',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 24,\n",
       "  'publicationType': 'Not available'},\n",
       " '9695824d7a01fad57ba9c01d7d76a519d78d65e7': {'arxiv_id': '2205.11487v1',\n",
       "  's2_paperId': '9695824d7a01fad57ba9c01d7d76a519d78d65e7',\n",
       "  'title': 'Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding',\n",
       "  'abstract': 'We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 108,\n",
       "  'influentialCitationCount': 395,\n",
       "  'citationCount': 5641,\n",
       "  'publicationType': 'JournalArticle | Review'},\n",
       " 'c57293882b2561e1ba03017902df9fc2f289dea2': {'arxiv_id': None,\n",
       "  's2_paperId': 'c57293882b2561e1ba03017902df9fc2f289dea2',\n",
       "  'title': 'Hierarchical Text-Conditional Image Generation with CLIP Latents',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 471,\n",
       "  'citationCount': 6454,\n",
       "  'publicationType': 'Not available'},\n",
       " '8941e477b2f39eb92712f04400412da60d349ec1': {'arxiv_id': None,\n",
       "  's2_paperId': '8941e477b2f39eb92712f04400412da60d349ec1',\n",
       "  'title': 'CLIP-Mesh: Generating textured meshes from text using pretrained image-text models',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 292,\n",
       "  'publicationType': 'Not available'},\n",
       " '17df7e87a5d25e6e83d773e6f686eb0a85f6827e': {'arxiv_id': None,\n",
       "  's2_paperId': '17df7e87a5d25e6e83d773e6f686eb0a85f6827e',\n",
       "  'title': 'NeuMan: Neural Human Radiance Field from a Single Video',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 21,\n",
       "  'citationCount': 209,\n",
       "  'publicationType': 'Not available'},\n",
       " '32da58aa252700339b118b1e4f03bd721d2d5b55': {'arxiv_id': None,\n",
       "  's2_paperId': '32da58aa252700339b118b1e4f03bd721d2d5b55',\n",
       "  'title': 'DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 65,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 44,\n",
       "  'publicationType': 'Not available'},\n",
       " '17a4c0e0e859b8e36a0591f4b5ff26b62e83ea60': {'arxiv_id': None,\n",
       "  's2_paperId': '17a4c0e0e859b8e36a0591f4b5ff26b62e83ea60',\n",
       "  'title': 'SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 148,\n",
       "  'publicationType': 'Not available'},\n",
       " '60e69982ef2920596c6f31d6fd3ca5e9591f3db6': {'arxiv_id': None,\n",
       "  's2_paperId': '60e69982ef2920596c6f31d6fd3ca5e9591f3db6',\n",
       "  'title': 'Instant neural graphics primitives with a multiresolution hash encoding',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM Transactions on Graphics',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 600,\n",
       "  'citationCount': 3787,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f763a59644e27a2215095943224f2564e670a504': {'arxiv_id': None,\n",
       "  's2_paperId': 'f763a59644e27a2215095943224f2564e670a504',\n",
       "  'title': 'HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 77,\n",
       "  'influentialCitationCount': 77,\n",
       "  'citationCount': 444,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c10075b3746a9f3dd5811970e93c8ca3ad39b39d': {'arxiv_id': None,\n",
       "  's2_paperId': 'c10075b3746a9f3dd5811970e93c8ca3ad39b39d',\n",
       "  'title': 'High-Resolution Image Synthesis with Latent Diffusion Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 110,\n",
       "  'influentialCitationCount': 3765,\n",
       "  'citationCount': 14348,\n",
       "  'publicationType': 'Not available'},\n",
       " '7002ae048e4b8c9133a55428441e8066070995cb': {'arxiv_id': None,\n",
       "  's2_paperId': '7002ae048e4b8c9133a55428441e8066070995cb',\n",
       "  'title': 'GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 281,\n",
       "  'citationCount': 3366,\n",
       "  'publicationType': 'Not available'},\n",
       " '03e1c3b5fdad9b21bbed3d13af7e8d6c73cbcfa6': {'arxiv_id': None,\n",
       "  's2_paperId': '03e1c3b5fdad9b21bbed3d13af7e8d6c73cbcfa6',\n",
       "  'title': 'Zero-Shot Text-Guided Object Generation with Dream Fields',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 546,\n",
       "  'publicationType': 'Not available'},\n",
       " '64ea8f180d0682e6c18d1eb688afdb2027c02794': {'arxiv_id': None,\n",
       "  's2_paperId': '64ea8f180d0682e6c18d1eb688afdb2027c02794',\n",
       "  'title': 'Diffusion Models Beat GANs on Image Synthesis',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 81,\n",
       "  'influentialCitationCount': 818,\n",
       "  'citationCount': 7166,\n",
       "  'publicationType': 'Not available'},\n",
       " '35e940ace815548f620709d9c1803da34c581e86': {'arxiv_id': None,\n",
       "  's2_paperId': '35e940ace815548f620709d9c1803da34c581e86',\n",
       "  'title': 'Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 68,\n",
       "  'influentialCitationCount': 63,\n",
       "  'citationCount': 364,\n",
       "  'publicationType': 'Not available'},\n",
       " '21336e57dc2ab9ae2171a0f6c35f7d1aba584796': {'arxiv_id': None,\n",
       "  's2_paperId': '21336e57dc2ab9ae2171a0f6c35f7d1aba584796',\n",
       "  'title': 'Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 208,\n",
       "  'citationCount': 1858,\n",
       "  'publicationType': 'Not available'},\n",
       " 'de18baa4964804cf471d85a5a090498242d2e79f': {'arxiv_id': None,\n",
       "  's2_paperId': 'de18baa4964804cf471d85a5a090498242d2e79f',\n",
       "  'title': 'Improved Denoising Diffusion Probabilistic Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 347,\n",
       "  'citationCount': 3375,\n",
       "  'publicationType': 'Not available'},\n",
       " '394be105b87e9bfe72c20efe6338de10604e1a11': {'arxiv_id': None,\n",
       "  's2_paperId': '394be105b87e9bfe72c20efe6338de10604e1a11',\n",
       "  'title': 'Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 99,\n",
       "  'influentialCitationCount': 152,\n",
       "  'citationCount': 1053,\n",
       "  'publicationType': 'Not available'},\n",
       " '014576b866078524286802b1d0e18628520aa886': {'arxiv_id': None,\n",
       "  's2_paperId': '014576b866078524286802b1d0e18628520aa886',\n",
       "  'title': 'Denoising Diffusion Implicit Models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 1188,\n",
       "  'citationCount': 6659,\n",
       "  'publicationType': 'Not available'},\n",
       " '5c126ae3421f05768d8edd97ecd44b1364e2c99a': {'arxiv_id': '2006.11239v2',\n",
       "  's2_paperId': '5c126ae3421f05768d8edd97ecd44b1364e2c99a',\n",
       "  'title': 'Denoising Diffusion Probabilistic Models',\n",
       "  'abstract': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 73,\n",
       "  'influentialCitationCount': 3100,\n",
       "  'citationCount': 16249,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '428b663772dba998f5dc6a24488fff1858a0899f': {'arxiv_id': None,\n",
       "  's2_paperId': '428b663772dba998f5dc6a24488fff1858a0899f',\n",
       "  'title': 'NeRF',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 761,\n",
       "  'citationCount': 2612,\n",
       "  'publicationType': 'Not available'},\n",
       " '343da6d4cff7ce8c04270487a1f7a037ea0572d6': {'arxiv_id': None,\n",
       "  's2_paperId': '343da6d4cff7ce8c04270487a1f7a037ea0572d6',\n",
       "  'title': 'PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 77,\n",
       "  'influentialCitationCount': 210,\n",
       "  'citationCount': 1231,\n",
       "  'publicationType': 'Not available'},\n",
       " '4be4707aba8d622a0553aa159dc92ae7f9af9c5e': {'arxiv_id': None,\n",
       "  's2_paperId': '4be4707aba8d622a0553aa159dc92ae7f9af9c5e',\n",
       "  'title': 'Expressive Body Capture: 3D Hands, Face, and Body From a Single Image',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 81,\n",
       "  'influentialCitationCount': 303,\n",
       "  'citationCount': 1624,\n",
       "  'publicationType': 'Not available'},\n",
       " '690c817ab5be8017ff713fa1028669debde205af': {'arxiv_id': None,\n",
       "  's2_paperId': '690c817ab5be8017ff713fa1028669debde205af',\n",
       "  'title': 'AMASS: Archive of Motion Capture As Surface Shapes',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 170,\n",
       "  'citationCount': 1182,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b4df354db88a70183a64dbc9e56cf14e7669a6c0': {'arxiv_id': None,\n",
       "  's2_paperId': 'b4df354db88a70183a64dbc9e56cf14e7669a6c0',\n",
       "  'title': 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning',\n",
       "  'abstract': 'We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.',\n",
       "  'venue': 'Annual Meeting of the Association for Computational Linguistics',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 370,\n",
       "  'citationCount': 2381,\n",
       "  'publicationType': 'Not available'},\n",
       " '4233b07033a1ef8af188383f30602a5fd0aa2181': {'arxiv_id': None,\n",
       "  's2_paperId': '4233b07033a1ef8af188383f30602a5fd0aa2181',\n",
       "  'title': 'Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 226,\n",
       "  'citationCount': 1559,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fdada4e2e659a72fd855f578150c1837e1a377d2': {'arxiv_id': None,\n",
       "  's2_paperId': 'fdada4e2e659a72fd855f578150c1837e1a377d2',\n",
       "  'title': 'Building efficient, accurate character skins from examples',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM Transactions on Graphics',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 333,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c8a679622d3ac764aec5a928b9b84910133a1d78': {'arxiv_id': None,\n",
       "  's2_paperId': 'c8a679622d3ac764aec5a928b9b84910133a1d78',\n",
       "  'title': 'Occlusion Culling Algorithms: A Comprehensive Survey',\n",
       "  'abstract': None,\n",
       "  'venue': 'J. Intell. Robotic Syst.',\n",
       "  'year': 2002,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 31,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f36e88b2052b2be089406dde7521090b00c37f63': {'arxiv_id': None,\n",
       "  's2_paperId': 'f36e88b2052b2be089406dde7521090b00c37f63',\n",
       "  'title': 'ECON: Explicit Clothed humans Obtained from Normals',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 78,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 24,\n",
       "  'publicationType': 'Not available'},\n",
       " '5d2289dea222330dcab41b3fbec12a6de9c91365': {'arxiv_id': None,\n",
       "  's2_paperId': '5d2289dea222330dcab41b3fbec12a6de9c91365',\n",
       "  'title': 'Learn to Dance with AIST++: Music Conditioned 3D Dance Generation',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 99,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 120,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c19ed5102ecd953d5c78d5a0b87eaa51658e07d8': {'arxiv_id': None,\n",
       "  's2_paperId': 'c19ed5102ecd953d5c78d5a0b87eaa51658e07d8',\n",
       "  'title': 'Supplementary Material to: Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 5,\n",
       "  'influentialCitationCount': 113,\n",
       "  'citationCount': 728,\n",
       "  'publicationType': 'Not available'},\n",
       " '153f01e2e9bb1a806d6372994ec14930a8dda66b': {'arxiv_id': '2009.14774v2',\n",
       "  's2_paperId': '153f01e2e9bb1a806d6372994ec14930a8dda66b',\n",
       "  'title': 'Consistent regression when oblivious outliers overwhelm',\n",
       "  'abstract': 'We consider a robust linear regression model y = Xβ∗ + η, where an adversary oblivious to the design X ∈ Rn×d may choose η to corrupt all but an α fraction of the observations y in an arbitrary way. Prior to our work, even for Gaussian X , no estimator for β∗ was known to be consistent in this model except for quadratic sample size n & (d/α)2 or for logarithmic inlier fraction α ≥ 1/log n. We show that consistent estimation is possible with nearly linear sample size and inverse-polynomial inlier fraction. Concretely, we show that the Huber loss estimator is consistent for every sample size n = ω(d/α2) and achieves an error rate of O(d/α2n)1/2 (both bounds are optimal up to constant factors). Our results extend to designs far beyond the Gaussian case and only require the column span of X to not contain approximately sparse vectors (similar to the kind of assumption commonly made about the kernel space for compressed sensing). We provide two technically similar proofs. One proof is phrased in terms of strong convexity, extending work of (Tsakonas et al., 2014), and particularly short. The other proof highlights a connection between the Huber loss estimator and high-dimensional median computations. In the special case of Gaussian designs, this connection leads us to a strikingly simple algorithm based on computing coordinate-wise medians that achieves nearly optimal guarantees in linear time, and that can exploit sparsity of β∗. The model studied here also captures heavy-tailed noise distributions that may not even have a first moment. *Equal contribution 1Department of Computer Science, ETH Zürich, Switzerland. Correspondence to: Tommaso d’Orsi <tommaso.dorsi@inf.ethz.ch>, Gleb Novikov <gleb.novikov@inf.ethz.ch>, David Steurer <david.steurer@inf.ethz.ch>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 15,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'dd7382eaccc007119084e94732a839f212a03664': {'arxiv_id': None,\n",
       "  's2_paperId': 'dd7382eaccc007119084e94732a839f212a03664',\n",
       "  'title': 'High dimensional robust M-estimation : arbitrary corruption and heavy tails',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 94,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 16,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fa5853fdef7d2f6bb68203d187ddacbbddc63a8b': {'arxiv_id': None,\n",
       "  's2_paperId': 'fa5853fdef7d2f6bb68203d187ddacbbddc63a8b',\n",
       "  'title': 'High-Dimensional Probability: An Introduction with Applications in Data Science',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 4,\n",
       "  'influentialCitationCount': 965,\n",
       "  'citationCount': 2931,\n",
       "  'publicationType': 'Not available'},\n",
       " '47628dde478a58668ea8ba1ea9a432c5f5ddb652': {'arxiv_id': None,\n",
       "  's2_paperId': '47628dde478a58668ea8ba1ea9a432c5f5ddb652',\n",
       "  'title': 'High‐dimensional Statistics: A Non‐asymptotic Viewpoint, Martin J.Wainwright, Cambridge University Press, 2019, xvii 552 pages, £57.99, hardback ISBN: 978‐1‐1084‐9802‐9',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Statistical Review',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 321,\n",
       "  'citationCount': 1285,\n",
       "  'publicationType': 'Not available'},\n",
       " '26bb8d49c51a4df70b2fb506aad1e31bed53d6f8': {'arxiv_id': None,\n",
       "  's2_paperId': '26bb8d49c51a4df70b2fb506aad1e31bed53d6f8',\n",
       "  'title': 'List Decodable Learning via Sum of Squares',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM-SIAM Symposium on Discrete Algorithms',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 66,\n",
       "  'publicationType': 'Not available'},\n",
       " '438b3d926426531954f22a2e6ac88722150be2e1': {'arxiv_id': None,\n",
       "  's2_paperId': '438b3d926426531954f22a2e6ac88722150be2e1',\n",
       "  'title': 'List-Decodable Linear Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 62,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 74,\n",
       "  'publicationType': 'Not available'},\n",
       " '812ca15efc50210778ad2229b1fba551dfe3c414': {'arxiv_id': None,\n",
       "  's2_paperId': '812ca15efc50210778ad2229b1fba551dfe3c414',\n",
       "  'title': \"Outlier-robust estimation of a sparse linear model using 𝓁1-penalized Huber's M-estimator\",\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 65,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bf640c289c6d9bf315040c19275e7df8fcc88417': {'arxiv_id': None,\n",
       "  's2_paperId': 'bf640c289c6d9bf315040c19275e7df8fcc88417',\n",
       "  'title': 'Adaptive Hard Thresholding for Near-optimal Consistent Robust Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 47,\n",
       "  'publicationType': 'Not available'},\n",
       " '6f9fcd930c267d5ea31fd085508dbe138524a184': {'arxiv_id': None,\n",
       "  's2_paperId': '6f9fcd930c267d5ea31fd085508dbe138524a184',\n",
       "  'title': 'Robust Sparse Reduced Rank Regression in High Dimensions',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 8,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e11f8ef12bce2f23b083865759ddf9b5ac021fb4': {'arxiv_id': None,\n",
       "  's2_paperId': 'e11f8ef12bce2f23b083865759ddf9b5ac021fb4',\n",
       "  'title': 'Compressed Sensing with Adversarial Sparse Noise via L1 Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'SIAM Symposium on Simplicity in Algorithms',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 34,\n",
       "  'publicationType': 'Not available'},\n",
       " '8904cdbe394593cdef96405b238fe5d3a4bdc03a': {'arxiv_id': None,\n",
       "  's2_paperId': '8904cdbe394593cdef96405b238fe5d3a4bdc03a',\n",
       "  'title': 'Efficient Algorithms and Lower Bounds for Robust Linear Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM-SIAM Symposium on Discrete Algorithms',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 161,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ad901015a61585f967d87e6acb847be9d18d7df3': {'arxiv_id': None,\n",
       "  's2_paperId': 'ad901015a61585f967d87e6acb847be9d18d7df3',\n",
       "  'title': 'High Dimensional Robust Sparse Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 69,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c49f1881dcb84113fe3cd8c9dbe539ad1111e2f0': {'arxiv_id': None,\n",
       "  's2_paperId': 'c49f1881dcb84113fe3cd8c9dbe539ad1111e2f0',\n",
       "  'title': 'Efficient Algorithms for Outlier-Robust Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 154,\n",
       "  'publicationType': 'Not available'},\n",
       " '3ed81ee5a442944be51fc2dadc708ec1e2cef07c': {'arxiv_id': None,\n",
       "  's2_paperId': '3ed81ee5a442944be51fc2dadc708ec1e2cef07c',\n",
       "  'title': 'Consistent Robust Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 98,\n",
       "  'publicationType': 'Not available'},\n",
       " 'dcf586a27e6237e031efd699b9cd7e6ca89a3d02': {'arxiv_id': None,\n",
       "  's2_paperId': 'dcf586a27e6237e031efd699b9cd7e6ca89a3d02',\n",
       "  'title': 'Adaptive Huber Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of the American Statistical Association',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 72,\n",
       "  'influentialCitationCount': 29,\n",
       "  'citationCount': 275,\n",
       "  'publicationType': 'Not available'},\n",
       " '842ba35a690282acde75643bb1935a9b8b630b1e': {'arxiv_id': None,\n",
       "  's2_paperId': '842ba35a690282acde75643bb1935a9b8b630b1e',\n",
       "  'title': 'Learning from untrusted data',\n",
       "  'abstract': None,\n",
       "  'venue': 'Symposium on the Theory of Computing',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 30,\n",
       "  'citationCount': 288,\n",
       "  'publicationType': 'Not available'},\n",
       " '842671f3ad4502836041bb4e25f31ea07072a5de': {'arxiv_id': None,\n",
       "  's2_paperId': '842671f3ad4502836041bb4e25f31ea07072a5de',\n",
       "  'title': 'Robust Estimators in High Dimensions without the Computational Intractability',\n",
       "  'abstract': 'We study high-dimensional distribution learning in an agnostic setting where an adversary is allowed to arbitrarily corrupt an epsilon fraction of the samples. Such questions have a rich history spanning statistics, machine learning and theoretical computer science. Even in the most basic settings, the only known approaches are either computationally inefficient or lose dimension dependent factors in their error guarantees. This raises the following question: Is high-dimensional agnostic distribution learning even possible, algorithmically? In this work, we obtain the first computationally efficient algorithms for agnostically learning several fundamental classes of high-dimensional distributions: (1) a single Gaussian, (2) a product distribution on the hypercube, (3) mixtures of two product distributions (under a natural balancedness condition), and (4) mixtures of k Gaussians with identical spherical covariances. All our algorithms achieve error that is independent of the dimension, and in many cases depends nearly-linearly on the fraction of adversarially corrupted samples. Moreover, we develop a general recipe for detecting and correcting corruptions in high-dimensions, that may be applicable to many other problems.',\n",
       "  'venue': 'IEEE Annual Symposium on Foundations of Computer Science',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 87,\n",
       "  'influentialCitationCount': 61,\n",
       "  'citationCount': 500,\n",
       "  'publicationType': 'Not available'},\n",
       " '525ab3a2d804a5ec7c02e35545c6288fb00c0216': {'arxiv_id': None,\n",
       "  's2_paperId': '525ab3a2d804a5ec7c02e35545c6288fb00c0216',\n",
       "  'title': 'Robust low-rank matrix estimation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annals of Statistics',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 45,\n",
       "  'publicationType': 'Not available'},\n",
       " '52da481acdd929c12f14ec613cb3aa3dee9a679e': {'arxiv_id': None,\n",
       "  's2_paperId': '52da481acdd929c12f14ec613cb3aa3dee9a679e',\n",
       "  'title': 'Convergence of the Huber Regression M-Estimate in the Presence of Dense Outliers',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Signal Processing Letters',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 25,\n",
       "  'publicationType': 'Not available'},\n",
       " '7a825749d1a2c8467d934b2976ce372ba878ba37': {'arxiv_id': None,\n",
       "  's2_paperId': '7a825749d1a2c8467d934b2976ce372ba878ba37',\n",
       "  'title': 'Exact Recoverability From Dense Corrupted Observations via $\\\\ell _{1}$-Minimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 116,\n",
       "  'publicationType': 'Not available'},\n",
       " '5f56320c5979faeab78dbd9ddb7db755ba4550f3': {'arxiv_id': None,\n",
       "  's2_paperId': '5f56320c5979faeab78dbd9ddb7db755ba4550f3',\n",
       "  'title': 'A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 105,\n",
       "  'influentialCitationCount': 235,\n",
       "  'citationCount': 1369,\n",
       "  'publicationType': 'Not available'},\n",
       " '842b12cee678cdc2856fa1fd9df3fd9970222b28': {'arxiv_id': None,\n",
       "  's2_paperId': '842b12cee678cdc2856fa1fd9df3fd9970222b28',\n",
       "  'title': 'Recovering Low-Rank Matrices From Few Coefficients in Any Basis',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 110,\n",
       "  'citationCount': 923,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f4106ee9cde073f89263aa417434bea09c094e71': {'arxiv_id': None,\n",
       "  's2_paperId': 'f4106ee9cde073f89263aa417434bea09c094e71',\n",
       "  'title': 'Robust Face Recognition via Sparse Representation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 84,\n",
       "  'influentialCitationCount': 1507,\n",
       "  'citationCount': 9623,\n",
       "  'publicationType': 'Not available'},\n",
       " '8f338eb4fa938890f74d054de50e99ecfb7d6201': {'arxiv_id': None,\n",
       "  's2_paperId': '8f338eb4fa938890f74d054de50e99ecfb7d6201',\n",
       "  'title': 'Euclidean Sections of with Sublinear Randomness and Error-Correction over the Reals',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 35,\n",
       "  'publicationType': 'Not available'},\n",
       " '77a5022598172d0b79b6e5e63ac26da7756e1f35': {'arxiv_id': None,\n",
       "  's2_paperId': '77a5022598172d0b79b6e5e63ac26da7756e1f35',\n",
       "  'title': 'Compressed Sensing for Networked Data',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Signal Processing Magazine',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 549,\n",
       "  'publicationType': 'Not available'},\n",
       " '97ea3dc5fd073d609d87fd843ed3d6dd3fbd8267': {'arxiv_id': None,\n",
       "  's2_paperId': '97ea3dc5fd073d609d87fd843ed3d6dd3fbd8267',\n",
       "  'title': 'A remark on Compressed Sensing',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 11,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 101,\n",
       "  'publicationType': 'Not available'},\n",
       " '915df1a8dda45221204f3ecbf70b07d8b34d7ba8': {'arxiv_id': None,\n",
       "  's2_paperId': '915df1a8dda45221204f3ecbf70b07d8b34d7ba8',\n",
       "  'title': 'Stable signal recovery from incomplete and inaccurate measurements',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 520,\n",
       "  'citationCount': 7494,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b5e853572b2f3134acafa76d5ae80b9f28c7dca8': {'arxiv_id': None,\n",
       "  's2_paperId': 'b5e853572b2f3134acafa76d5ae80b9f28c7dca8',\n",
       "  'title': 'Decoding by linear programming',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 1012,\n",
       "  'citationCount': 7323,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ae3f31c841c460b15a81bf51655f4c0e39cacc79': {'arxiv_id': None,\n",
       "  's2_paperId': 'ae3f31c841c460b15a81bf51655f4c0e39cacc79',\n",
       "  'title': 'Robust Regression and Outlier Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'Wiley Series in Probability and Statistics',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 635,\n",
       "  'citationCount': 8459,\n",
       "  'publicationType': 'Not available'},\n",
       " '819562c634d2d4e1369249e94086da8626e75f56': {'arxiv_id': None,\n",
       "  's2_paperId': '819562c634d2d4e1369249e94086da8626e75f56',\n",
       "  'title': 'Asymptotics for Least Absolute Deviation Regression Estimators',\n",
       "  'abstract': None,\n",
       "  'venue': 'Econometric Theory',\n",
       "  'year': 1991,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 104,\n",
       "  'citationCount': 712,\n",
       "  'publicationType': 'Not available'},\n",
       " '639bb2d08557584d28c19777b0c6732e498e82a5': {'arxiv_id': None,\n",
       "  's2_paperId': '639bb2d08557584d28c19777b0c6732e498e82a5',\n",
       "  'title': 'Time Bounds for Selection',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of computer and system sciences (Print)',\n",
       "  'year': 1973,\n",
       "  'referenceCount': 9,\n",
       "  'influentialCitationCount': 86,\n",
       "  'citationCount': 1352,\n",
       "  'publicationType': 'Not available'},\n",
       " '94d6a5ed17042dfac7a756202bfb019ff84080fb': {'arxiv_id': None,\n",
       "  's2_paperId': '94d6a5ed17042dfac7a756202bfb019ff84080fb',\n",
       "  'title': 'Asymptotic Properties of Non-Linear Least Squares Estimators',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1969,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 143,\n",
       "  'citationCount': 1422,\n",
       "  'publicationType': 'Not available'},\n",
       " '95c1225ffde189c7b3146938beba4e1cd8a915b6': {'arxiv_id': None,\n",
       "  's2_paperId': '95c1225ffde189c7b3146938beba4e1cd8a915b6',\n",
       "  'title': 'Algorithm 65: find',\n",
       "  'abstract': None,\n",
       "  'venue': 'Communications of the ACM',\n",
       "  'year': 1961,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 28,\n",
       "  'citationCount': 394,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e7321ab0f3be0b29aaf5f073fd7de7da5fed2f92': {'arxiv_id': None,\n",
       "  's2_paperId': 'e7321ab0f3be0b29aaf5f073fd7de7da5fed2f92',\n",
       "  'title': 'Compressed Sensing',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 1002,\n",
       "  'citationCount': 16580,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c33ded240dc564a0eae814feb0d7493a4c0cc59c': {'arxiv_id': None,\n",
       "  's2_paperId': 'c33ded240dc564a0eae814feb0d7493a4c0cc59c',\n",
       "  'title': 'Almost Euclidean subspaces of $\\\\ell_1^N$ via expander codes.',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 8,\n",
       "  'publicationType': 'Not available'},\n",
       " '3ead228b4061d7053ebbeb0603555bd288a83656': {'arxiv_id': None,\n",
       "  's2_paperId': '3ead228b4061d7053ebbeb0603555bd288a83656',\n",
       "  'title': 'ADAPTIVE ESTIMATION OF A QUADRATIC FUNCTIONAL BY MODEL SELECTION',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2001,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 142,\n",
       "  'citationCount': 1325,\n",
       "  'publicationType': 'Not available'},\n",
       " '4715dc26278d596d3981927a86706fafaaf0e83b': {'arxiv_id': None,\n",
       "  's2_paperId': '4715dc26278d596d3981927a86706fafaaf0e83b',\n",
       "  'title': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 20, NO. 7, JULY 1998 Comments on \"Geodesic Saliency of',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1998,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 62,\n",
       "  'citationCount': 896,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c87d57da3b1f2b467ef4995d30df832ee2281107': {'arxiv_id': None,\n",
       "  's2_paperId': 'c87d57da3b1f2b467ef4995d30df832ee2281107',\n",
       "  'title': 'On robust estimation of the location parameter',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1980,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 676,\n",
       "  'citationCount': 5845,\n",
       "  'publicationType': 'Not available'},\n",
       " '746546336375a9129b856d44f0740e6609024212': {'arxiv_id': None,\n",
       "  's2_paperId': '746546336375a9129b856d44f0740e6609024212',\n",
       "  'title': 'Mathematics and the Picturing of Data',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1975,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 105,\n",
       "  'citationCount': 1117,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e57d9aa9cf3e292e9eecd2f24b21677fa090d988': {'arxiv_id': '2111.03602v1',\n",
       "  's2_paperId': 'e57d9aa9cf3e292e9eecd2f24b21677fa090d988',\n",
       "  'title': 'NAS-Bench-x11 and the Power of Learning Curves',\n",
       "  'abstract': 'While early research in neural architecture search (NAS) required extreme computational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of NAS research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to run many types of multi-fidelity techniques, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate benchmarks, NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, that output the full training information for each architecture, rather than just the final validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-fidelity algorithms which claimed to be state-of-the-art upon release. Our code and pretrained models are available at https://github.com/automl/nas-bench-x11.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 102,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 27,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '05237f45140c5d48ec44a366aff52d49bf9c4c45': {'arxiv_id': None,\n",
       "  's2_paperId': '05237f45140c5d48ec44a366aff52d49bf9c4c45',\n",
       "  'title': 'NPENAS: Neural Predictor Guided Evolution for Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Neural Networks and Learning Systems',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 68,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 95,\n",
       "  'publicationType': 'Not available'},\n",
       " '7a16d9b4e04300d034502dc7dd58428714594e2c': {'arxiv_id': None,\n",
       "  's2_paperId': '7a16d9b4e04300d034502dc7dd58428714594e2c',\n",
       "  'title': 'Carbon Emissions and Large Neural Network Training',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 38,\n",
       "  'citationCount': 617,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b1d5e323d7303416dbb832ca70bc554d1a11f8c1': {'arxiv_id': None,\n",
       "  's2_paperId': 'b1d5e323d7303416dbb832ca70bc554d1a11f8c1',\n",
       "  'title': 'Landmark Regularization: Ranking Guided Super-Net Training in Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 17,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c13f5dbf6fd48ae339dd3f3394e0cbe9661756d3': {'arxiv_id': '2104.01177v2',\n",
       "  's2_paperId': 'c13f5dbf6fd48ae339dd3f3394e0cbe9661756d3',\n",
       "  'title': 'How Powerful are Performance Predictors in Neural Architecture Search?',\n",
       "  'abstract': 'Early methods in the rapidly developing field of neural architecture search (NAS) required fully training thousands of neural networks. To reduce this extreme computational cost, dozens of techniques have since been proposed to predict the final performance of neural architectures. Despite the success of such performance prediction methods, it is not well-understood how different families of techniques compare to one another, due to the lack of an agreed-upon evaluation metric and optimization for different constraints on the initialization time and query time. In this work, we give the first large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to\"zero-cost\"proxies. We test a number of correlation- and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. Our results act as recommendations for the best predictors to use in different settings, and we show that certain families of predictors can be combined to achieve even better predictive power, opening up promising research directions. Our code, featuring a library of 31 performance predictors, is available at https://github.com/automl/naslib.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 89,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 123,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '8279cee52ae62fd819e1380b5e6ee200dab09aad': {'arxiv_id': None,\n",
       "  's2_paperId': '8279cee52ae62fd819e1380b5e6ee200dab09aad',\n",
       "  'title': 'CATE: Computation-aware Neural Architecture Encoding with Transformers',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 87,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 24,\n",
       "  'publicationType': 'Not available'},\n",
       " '39e3c9d9546c055488c6e643ac3e793ceb80590e': {'arxiv_id': None,\n",
       "  's2_paperId': '39e3c9d9546c055488c6e643ac3e793ceb80590e',\n",
       "  'title': 'Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 66,\n",
       "  'publicationType': 'Not available'},\n",
       " '905ca29530fc755da13151f1ddf604ad6615d644': {'arxiv_id': None,\n",
       "  's2_paperId': '905ca29530fc755da13151f1ddf604ad6615d644',\n",
       "  'title': 'Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 117,\n",
       "  'publicationType': 'Not available'},\n",
       " '39fb6f086039ea1b0fca1c8c556fd86f2abad4c2': {'arxiv_id': None,\n",
       "  's2_paperId': '39fb6f086039ea1b0fca1c8c556fd86f2abad4c2',\n",
       "  'title': 'NASLib: A Modular and Flexible Neural Architecture Search Library',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 22,\n",
       "  'publicationType': 'Not available'},\n",
       " '3259c9ab1714a4cfdf6439cca6bdc5f78d78fda3': {'arxiv_id': None,\n",
       "  's2_paperId': '3259c9ab1714a4cfdf6439cca6bdc5f78d78fda3',\n",
       "  'title': 'NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 36,\n",
       "  'citationCount': 159,\n",
       "  'publicationType': 'Not available'},\n",
       " 'faef3261446023bed7e59fda75a0b819986903bd': {'arxiv_id': None,\n",
       "  's2_paperId': 'faef3261446023bed7e59fda75a0b819986903bd',\n",
       "  'title': 'NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 79,\n",
       "  'influentialCitationCount': 24,\n",
       "  'citationCount': 144,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b1e143af02e87d27c790cad6034332fc5079b791': {'arxiv_id': None,\n",
       "  's2_paperId': 'b1e143af02e87d27c790cad6034332fc5079b791',\n",
       "  'title': 'A Surgery of the Neural Architecture Evaluators',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 10,\n",
       "  'publicationType': 'Not available'},\n",
       " 'cac2c35d21b84ee1042886962d40e5f8737603e3': {'arxiv_id': None,\n",
       "  's2_paperId': 'cac2c35d21b84ee1042886962d40e5f8737603e3',\n",
       "  'title': 'Weight-Sharing Neural Architecture Search: A Battle to Shrink the Optimization Gap',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM Computing Surveys',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 522,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 108,\n",
       "  'publicationType': 'Not available'},\n",
       " '595891ca6cf85f04d922f22046807fc17419168f': {'arxiv_id': None,\n",
       "  's2_paperId': '595891ca6cf85f04d922f22046807fc17419168f',\n",
       "  'title': 'An Asymptotically Optimal Multi-Armed Bandit Algorithm and Hyperparameter Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 76,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 7,\n",
       "  'publicationType': 'Not available'},\n",
       " '24db2d47dfd90272c06675548460d44176cb60be': {'arxiv_id': None,\n",
       "  's2_paperId': '24db2d47dfd90272c06675548460d44176cb60be',\n",
       "  'title': 'A Study on Encodings for Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 71,\n",
       "  'publicationType': 'Not available'},\n",
       " '11b430ae91c5b7a3e51f86235818f74fd8b449ef': {'arxiv_id': None,\n",
       "  's2_paperId': '11b430ae91c5b7a3e51f86235818f74fd8b449ef',\n",
       "  'title': 'DrNAS: Dirichlet Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 97,\n",
       "  'publicationType': 'Not available'},\n",
       " '663bc61408a758872afed9c132923eb1e57c3544': {'arxiv_id': None,\n",
       "  's2_paperId': '663bc61408a758872afed9c132923eb1e57c3544',\n",
       "  'title': 'Neural Architecture Search using Bayesian Optimisation with Weisfeiler-Lehman Kernel',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 77,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 21,\n",
       "  'publicationType': 'Not available'},\n",
       " '48167fbde4ef8d5caa69e1c0c9d6e2ac7fd48c3b': {'arxiv_id': None,\n",
       "  's2_paperId': '48167fbde4ef8d5caa69e1c0c9d6e2ac7fd48c3b',\n",
       "  'title': 'NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing',\n",
       "  'abstract': 'Neural Architecture Search (NAS) is a promising and rapidly evolving research area. Training a large number of neural networks requires an exceptional amount of computational power, which makes NAS unreachable for those researchers who have limited or no access to high-performance clusters and supercomputers. A few benchmarks with precomputed neural architectures performances have been recently introduced to overcome this problem and ensure reproducible experiments. However, these benchmarks are only for the computer vision domain and, thus, are built from the image datasets and convolution-derived architectures. In this work, we step outside the computer vision domain by leveraging the language modeling task, which is the core of natural language processing (NLP). Our main contribution is as follows: we have provided search space of recurrent neural networks on the text datasets and trained 14k architectures within it; we have conducted both intrinsic and extrinsic evaluation of the trained models using datasets for semantic relatedness and language understanding evaluation; finally, we have tested several NAS algorithms to demonstrate how the precomputed results can be utilized. We consider that the benchmark will provide more reliable empirical findings in the community and stimulate progress in developing new NAS methods well suited for recurrent architectures.',\n",
       "  'venue': 'IEEE Access',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 99,\n",
       "  'publicationType': 'Not available'},\n",
       " '64f5c12dafeeface4504e1f7a585acda09c0d83a': {'arxiv_id': None,\n",
       "  's2_paperId': '64f5c12dafeeface4504e1f7a585acda09c0d83a',\n",
       "  'title': 'Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 74,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 98,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd27da8224b3c884be0890b4eb75d492e1b3e0b79': {'arxiv_id': None,\n",
       "  's2_paperId': 'd27da8224b3c884be0890b4eb75d492e1b3e0b79',\n",
       "  'title': 'Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 15,\n",
       "  'publicationType': 'Not available'},\n",
       " '6c2b5a50f903c873db7c18d9b9580560d8c74c3a': {'arxiv_id': None,\n",
       "  's2_paperId': '6c2b5a50f903c873db7c18d9b9580560d8c74c3a',\n",
       "  'title': 'Local Search is State of the Art for NAS Benchmarks',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 28,\n",
       "  'publicationType': 'Not available'},\n",
       " '2776d874840fa1b9d94ad9365ee5693bf98be8e0': {'arxiv_id': None,\n",
       "  's2_paperId': '2776d874840fa1b9d94ad9365ee5693bf98be8e0',\n",
       "  'title': 'Local Search is a Remarkably Strong Baseline for Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Evolutionary Multi-Criterion Optimization',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 37,\n",
       "  'publicationType': 'Not available'},\n",
       " '301c48248ba3a4b6d726aac8ec4c3f335e3712e9': {'arxiv_id': None,\n",
       "  's2_paperId': '301c48248ba3a4b6d726aac8ec4c3f335e3712e9',\n",
       "  'title': 'Geometry-Aware Gradient Algorithms for Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 66,\n",
       "  'publicationType': 'Not available'},\n",
       " '67cf81f5badb5d8dc28baf769ec07931a206ed72': {'arxiv_id': None,\n",
       "  's2_paperId': '67cf81f5badb5d8dc28baf769ec07931a206ed72',\n",
       "  'title': 'Model-based Asynchronous Hyperparameter and Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 42,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f3b3a921762a40f929fa6040cc607b503f9ab8b5': {'arxiv_id': None,\n",
       "  's2_paperId': 'f3b3a921762a40f929fa6040cc607b503f9ab8b5',\n",
       "  'title': 'Semi-Supervised Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 86,\n",
       "  'publicationType': 'Not available'},\n",
       " '40744235481391b7d6baf8f9d590dfa32da1f4d4': {'arxiv_id': None,\n",
       "  's2_paperId': '40744235481391b7d6baf8f9d590dfa32da1f4d4',\n",
       "  'title': 'NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 26,\n",
       "  'citationCount': 145,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a96f8601a1eddc09a8896830bd3d597cd50596fb': {'arxiv_id': None,\n",
       "  's2_paperId': 'a96f8601a1eddc09a8896830bd3d597cd50596fb',\n",
       "  'title': 'Deeper Insights into Weight Sharing in Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 48,\n",
       "  'publicationType': 'Not available'},\n",
       " '69599593f93023e2f91ef6673ee9860f85777d98': {'arxiv_id': None,\n",
       "  's2_paperId': '69599593f93023e2f91ef6673ee9860f85777d98',\n",
       "  'title': 'NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 200,\n",
       "  'citationCount': 681,\n",
       "  'publicationType': 'Not available'},\n",
       " '645a24296f96f325f4a6fd324cef85661a8987da': {'arxiv_id': None,\n",
       "  's2_paperId': '645a24296f96f325f4a6fd324cef85661a8987da',\n",
       "  'title': 'NAS evaluation is frustratingly hard',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 166,\n",
       "  'publicationType': 'Not available'},\n",
       " '2af12bb2fc283bdc6cd3a92e3a7642a57bc3dcac': {'arxiv_id': None,\n",
       "  's2_paperId': '2af12bb2fc283bdc6cd3a92e3a7642a57bc3dcac',\n",
       "  'title': 'Neural Predictor for Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 41,\n",
       "  'citationCount': 193,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a66d71cfbe48af6b3a2e38e60ec5549076722530': {'arxiv_id': None,\n",
       "  's2_paperId': 'a66d71cfbe48af6b3a2e38e60ec5549076722530',\n",
       "  'title': 'Probabilistic Rollouts for Learning Curve Extrapolation Across Hyperparameter Settings',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 9,\n",
       "  'publicationType': 'Not available'},\n",
       " '013a741927569ae9b40875a9e58d2c5ba6dbb3a8': {'arxiv_id': None,\n",
       "  's2_paperId': '013a741927569ae9b40875a9e58d2c5ba6dbb3a8',\n",
       "  'title': 'BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search',\n",
       "  'abstract': 'Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance.\\n\\nIn this work, we give a thorough analysis of the \"BO + neural predictor framework\" by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition function optimization. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.',\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 94,\n",
       "  'influentialCitationCount': 48,\n",
       "  'citationCount': 308,\n",
       "  'publicationType': 'Not available'},\n",
       " '3242bf8767179c13c7322ccfdbe18c66c1e25a99': {'arxiv_id': None,\n",
       "  's2_paperId': '3242bf8767179c13c7322ccfdbe18c66c1e25a99',\n",
       "  'title': 'Understanding and Robustifying Differentiable Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 66,\n",
       "  'citationCount': 363,\n",
       "  'publicationType': 'Not available'},\n",
       " '922aca1c0621d7828cf49e409093b064ae9c2f04': {'arxiv_id': None,\n",
       "  's2_paperId': '922aca1c0621d7828cf49e409093b064ae9c2f04',\n",
       "  'title': 'Bayesian Optimization for Iterative Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 31,\n",
       "  'publicationType': 'Not available'},\n",
       " '78dbb9334215194020437c2c2cfdfce478f30060': {'arxiv_id': None,\n",
       "  's2_paperId': '78dbb9334215194020437c2c2cfdfce478f30060',\n",
       "  'title': 'Best Practices for Scientific Research on Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 140,\n",
       "  'publicationType': 'Not available'},\n",
       " '4ebce2425e231031f89a4a68dc52a151cd735d03': {'arxiv_id': None,\n",
       "  's2_paperId': '4ebce2425e231031f89a4a68dc52a151cd735d03',\n",
       "  'title': 'PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 133,\n",
       "  'citationCount': 595,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fe8907302f9d14233cd03cc2948a1c4e2a50bdb6': {'arxiv_id': None,\n",
       "  's2_paperId': 'fe8907302f9d14233cd03cc2948a1c4e2a50bdb6',\n",
       "  'title': 'Searching for a Robust Neural Architecture in Four GPU Hours',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 113,\n",
       "  'citationCount': 637,\n",
       "  'publicationType': 'Not available'},\n",
       " '6f561c0a27827e699db0353a0caa9075a03e9701': {'arxiv_id': None,\n",
       "  's2_paperId': '6f561c0a27827e699db0353a0caa9075a03e9701',\n",
       "  'title': 'Efficient Forward Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 40,\n",
       "  'publicationType': 'Not available'},\n",
       " '6e4fd9b4b2b673c981cda528d8039a221ad35225': {'arxiv_id': None,\n",
       "  's2_paperId': '6e4fd9b4b2b673c981cda528d8039a221ad35225',\n",
       "  'title': 'NAS-Bench-101: Towards Reproducible Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 153,\n",
       "  'citationCount': 660,\n",
       "  'publicationType': 'Not available'},\n",
       " '6eb3a62cd365e4f9792eedca43c90595e1a862ba': {'arxiv_id': None,\n",
       "  's2_paperId': '6eb3a62cd365e4f9792eedca43c90595e1a862ba',\n",
       "  'title': 'Evaluating the Search Phase of Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 341,\n",
       "  'publicationType': 'Not available'},\n",
       " '35a59bd09974c7fc78cf681f77f7301e180fd23c': {'arxiv_id': None,\n",
       "  's2_paperId': '35a59bd09974c7fc78cf681f77f7301e180fd23c',\n",
       "  'title': 'Random Search and Reproducibility for Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'Conference on Uncertainty in Artificial Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 81,\n",
       "  'citationCount': 707,\n",
       "  'publicationType': 'Not available'},\n",
       " '65a854702fea5b32d02e13515cae7fc7460e1510': {'arxiv_id': None,\n",
       "  's2_paperId': '65a854702fea5b32d02e13515cae7fc7460e1510',\n",
       "  'title': 'Combinatorial Bayesian Optimization using the Graph Cartesian Product',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 102,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a2403c1ce02120f7bd383e395b561ff7c64d52ec': {'arxiv_id': None,\n",
       "  's2_paperId': 'a2403c1ce02120f7bd383e395b561ff7c64d52ec',\n",
       "  'title': 'A System for Massively Parallel Hyperparameter Tuning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Conference on Machine Learning and Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 37,\n",
       "  'citationCount': 364,\n",
       "  'publicationType': 'Not available'},\n",
       " '93436a26d744e0417e21df10abdfce2cc74b1e58': {'arxiv_id': '1807.01774v1',\n",
       "  's2_paperId': '93436a26d744e0417e21df10abdfce2cc74b1e58',\n",
       "  'title': 'BOHB: Robust and Efficient Hyperparameter Optimization at Scale',\n",
       "  'abstract': 'Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 147,\n",
       "  'citationCount': 1055,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '45b7b5514a65126d39a51d5a68da53e7aa244c1f': {'arxiv_id': None,\n",
       "  's2_paperId': '45b7b5514a65126d39a51d5a68da53e7aa244c1f',\n",
       "  'title': 'Understanding and Simplifying One-Shot Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 113,\n",
       "  'citationCount': 762,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c1f457e31b611da727f9aef76c283a18157dfa83': {'arxiv_id': None,\n",
       "  's2_paperId': 'c1f457e31b611da727f9aef76c283a18157dfa83',\n",
       "  'title': 'DARTS: Differentiable Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 1281,\n",
       "  'citationCount': 4251,\n",
       "  'publicationType': 'Not available'},\n",
       " '7864c8cd08ff4da9acc37de2576e9cdbabe03107': {'arxiv_id': None,\n",
       "  's2_paperId': '7864c8cd08ff4da9acc37de2576e9cdbabe03107',\n",
       "  'title': 'Neural Architecture Search with Bayesian Optimisation and Optimal Transport',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 61,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 593,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fe9b8aac9fa3bfd9724db5a881a578e471e612d7': {'arxiv_id': None,\n",
       "  's2_paperId': 'fe9b8aac9fa3bfd9724db5a881a578e471e612d7',\n",
       "  'title': 'Efficient Neural Architecture Search via Parameter Sharing',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 490,\n",
       "  'citationCount': 2719,\n",
       "  'publicationType': 'Not available'},\n",
       " '50bdda28de3dcf82a0e10f9ec13eea248b19edb5': {'arxiv_id': None,\n",
       "  's2_paperId': '50bdda28de3dcf82a0e10f9ec13eea248b19edb5',\n",
       "  'title': 'Regularized Evolution for Image Classifier Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 90,\n",
       "  'influentialCitationCount': 408,\n",
       "  'citationCount': 2972,\n",
       "  'publicationType': 'Not available'},\n",
       " '497e4b08279d69513e4d2313a7fd9a55dfb73273': {'arxiv_id': None,\n",
       "  's2_paperId': '497e4b08279d69513e4d2313a7fd9a55dfb73273',\n",
       "  'title': 'LightGBM: A Highly Efficient Gradient Boosting Decision Tree',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 989,\n",
       "  'citationCount': 9924,\n",
       "  'publicationType': 'Not available'},\n",
       " '5f79398057bf0bbda9ff50067bc1f2950c2a2266': {'arxiv_id': None,\n",
       "  's2_paperId': '5f79398057bf0bbda9ff50067bc1f2950c2a2266',\n",
       "  'title': 'Progressive Neural Architecture Search',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 288,\n",
       "  'citationCount': 1971,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fd242002c786c7db5c80ac885e490514ff90a721': {'arxiv_id': None,\n",
       "  's2_paperId': 'fd242002c786c7db5c80ac885e490514ff90a721',\n",
       "  'title': 'Speeding up Hyper-parameter Optimization by Extrapolation of Learning Curves Using Previous Builds',\n",
       "  'abstract': None,\n",
       "  'venue': 'ECML/PKDD',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 30,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e644a409b4a4c6eaedffe27efbc5c76280b34c61': {'arxiv_id': None,\n",
       "  's2_paperId': 'e644a409b4a4c6eaedffe27efbc5c76280b34c61',\n",
       "  'title': 'A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 119,\n",
       "  'citationCount': 624,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd0611891b9e8a7c5731146097b6f201578f47b2f': {'arxiv_id': None,\n",
       "  's2_paperId': 'd0611891b9e8a7c5731146097b6f201578f47b2f',\n",
       "  'title': 'Learning Transferable Architectures for Scalable Image Recognition',\n",
       "  'abstract': None,\n",
       "  'venue': '2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 77,\n",
       "  'influentialCitationCount': 834,\n",
       "  'citationCount': 5509,\n",
       "  'publicationType': 'Not available'},\n",
       " '2ddc07ca7af4578fde868f102d178dc3ba6a0751': {'arxiv_id': None,\n",
       "  's2_paperId': '2ddc07ca7af4578fde868f102d178dc3ba6a0751',\n",
       "  'title': 'Accelerating Neural Architecture Search using Performance Prediction',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 38,\n",
       "  'citationCount': 362,\n",
       "  'publicationType': 'Not available'},\n",
       " '71a80e7342e56f33fd120246e907151a0cf1b4d0': {'arxiv_id': None,\n",
       "  's2_paperId': '71a80e7342e56f33fd120246e907151a0cf1b4d0',\n",
       "  'title': 'DeepArchitect: Automatically Designing and Training Deep Architectures',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 184,\n",
       "  'publicationType': 'Not available'},\n",
       " '5f2060ad5ba828288c9daa13f3bbaf0a6ac0e6f6': {'arxiv_id': None,\n",
       "  's2_paperId': '5f2060ad5ba828288c9daa13f3bbaf0a6ac0e6f6',\n",
       "  'title': 'Multi-fidelity Bayesian Optimisation with Continuous Approximations',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 24,\n",
       "  'citationCount': 217,\n",
       "  'publicationType': 'Not available'},\n",
       " '67d968c7450878190e45ac7886746de867bf673d': {'arxiv_id': None,\n",
       "  's2_paperId': '67d968c7450878190e45ac7886746de867bf673d',\n",
       "  'title': 'Neural Architecture Search with Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 73,\n",
       "  'influentialCitationCount': 600,\n",
       "  'citationCount': 5282,\n",
       "  'publicationType': 'Not available'},\n",
       " '925cab60b1795c94ae6f488fda7ad71be71b5822': {'arxiv_id': None,\n",
       "  's2_paperId': '925cab60b1795c94ae6f488fda7ad71be71b5822',\n",
       "  'title': 'Learning Curve Prediction with Bayesian Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 250,\n",
       "  'publicationType': 'Not available'},\n",
       " 'cf4e443c4268a3b6face07a30599529e6f47410c': {'arxiv_id': None,\n",
       "  's2_paperId': 'cf4e443c4268a3b6face07a30599529e6f47410c',\n",
       "  'title': 'Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 46,\n",
       "  'citationCount': 540,\n",
       "  'publicationType': 'Not available'},\n",
       " '892f9a2f69241feec647856cd26bed37e04fd747': {'arxiv_id': None,\n",
       "  's2_paperId': '892f9a2f69241feec647856cd26bed37e04fd747',\n",
       "  'title': 'Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 222,\n",
       "  'citationCount': 2256,\n",
       "  'publicationType': 'Not available'},\n",
       " '68cf30a0b311bf4c776b234dfc4331a1310659e9': {'arxiv_id': None,\n",
       "  's2_paperId': '68cf30a0b311bf4c776b234dfc4331a1310659e9',\n",
       "  'title': 'Multi-fidelity Gaussian Process Bandit Optimisation',\n",
       "  'abstract': 'In many scientific and engineering applications, we are tasked with the maximisation of an expensive to evaluate black box function f. Traditional settings for this problem assume just the availability of this single function. However, in many cases, cheap approximations to f may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of f in a small but promising region and speedily identify the optimum. We formalise this task as a multi-fidelity bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a novel method based on upper confidence bound techniques. In our theoretical analysis we demonstrate that it exhibits precisely the above behaviour and achieves better bounds on the regret than strategies which ignore multi-fidelity information. Empirically, MF-GP-UCB outperforms such naive strategies and other multi-fidelity methods on several synthetic and real experiments.',\n",
       "  'venue': 'Journal of Artificial Intelligence Research',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 72,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 76,\n",
       "  'publicationType': 'Not available'},\n",
       " '26bc9195c6343e4d7f434dd65b4ad67efe2be27a': {'arxiv_id': None,\n",
       "  's2_paperId': '26bc9195c6343e4d7f434dd65b4ad67efe2be27a',\n",
       "  'title': 'XGBoost: A Scalable Tree Boosting System',\n",
       "  'abstract': None,\n",
       "  'venue': 'Knowledge Discovery and Data Mining',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 3058,\n",
       "  'citationCount': 37004,\n",
       "  'publicationType': 'Not available'},\n",
       " 'efb4431579a46d9cfa51b4ebbd4ddb9f44a30246': {'arxiv_id': None,\n",
       "  's2_paperId': 'efb4431579a46d9cfa51b4ebbd4ddb9f44a30246',\n",
       "  'title': 'Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Joint Conference on Artificial Intelligence',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 57,\n",
       "  'citationCount': 573,\n",
       "  'publicationType': 'Not available'},\n",
       " 'aa7cf530ae11df4135026e9003288cdc40f75b08': {'arxiv_id': None,\n",
       "  's2_paperId': 'aa7cf530ae11df4135026e9003288cdc40f75b08',\n",
       "  'title': 'Efficient Benchmarking of Hyperparameter Optimizers via Surrogates',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 125,\n",
       "  'publicationType': 'Not available'},\n",
       " '52d97890dbc290108136739ec2afe0f2b6c4f570': {'arxiv_id': None,\n",
       "  's2_paperId': '52d97890dbc290108136739ec2afe0f2b6c4f570',\n",
       "  'title': 'Freeze-Thaw Bayesian Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 26,\n",
       "  'citationCount': 264,\n",
       "  'publicationType': 'Not available'},\n",
       " '03911c85305d42aa2eeb02be82ef6fb7da644dd0': {'arxiv_id': None,\n",
       "  's2_paperId': '03911c85305d42aa2eeb02be82ef6fb7da644dd0',\n",
       "  'title': 'Algorithms for Hyper-Parameter Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 392,\n",
       "  'citationCount': 4515,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd03c916d49268d48fde3b76a68e64af7761835e7': {'arxiv_id': None,\n",
       "  's2_paperId': 'd03c916d49268d48fde3b76a68e64af7761835e7',\n",
       "  'title': 'Evolving Neural Networks through Augmenting Topologies',\n",
       "  'abstract': None,\n",
       "  'venue': 'Evolutionary Computation',\n",
       "  'year': 2002,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 455,\n",
       "  'citationCount': 3475,\n",
       "  'publicationType': 'Not available'},\n",
       " 'aae7c875fc7531233c2a3ebefa31a33f1a0d7f49': {'arxiv_id': None,\n",
       "  's2_paperId': 'aae7c875fc7531233c2a3ebefa31a33f1a0d7f49',\n",
       "  'title': 'Multivariate Density Estimation, Theory, Practice and Visualization',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1992,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 492,\n",
       "  'citationCount': 5045,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e89cb97bc83badf8c6cc0e2439ee4a035cba72d9': {'arxiv_id': None,\n",
       "  's2_paperId': 'e89cb97bc83badf8c6cc0e2439ee4a035cba72d9',\n",
       "  'title': 'Designing Neural Networks using Genetic Algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Genetic Algorithms',\n",
       "  'year': 1989,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 37,\n",
       "  'citationCount': 982,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e8735c4982e35b9de686f00823d1a1ca4289c27e': {'arxiv_id': None,\n",
       "  's2_paperId': 'e8735c4982e35b9de686f00823d1a1ca4289c27e',\n",
       "  'title': 'Correlation and Causation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Wilmott Magazine',\n",
       "  'year': 1937,\n",
       "  'referenceCount': 5,\n",
       "  'influentialCitationCount': 65,\n",
       "  'citationCount': 2201,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd0cf9923b9f9f58522b1407df60e6f96d4588f29': {'arxiv_id': None,\n",
       "  's2_paperId': 'd0cf9923b9f9f58522b1407df60e6f96d4588f29',\n",
       "  'title': 'NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 60,\n",
       "  'publicationType': 'Not available'},\n",
       " '562813793dd92cffbf17f4405c232aa198337069': {'arxiv_id': None,\n",
       "  's2_paperId': '562813793dd92cffbf17f4405c232aa198337069',\n",
       "  'title': 'Supplementary material for DEHB: Evolutionary Hyperband for Scalable, Robust and Efﬁcient Hyperparameter Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 47,\n",
       "  'publicationType': 'Not available'},\n",
       " '9819b600a828a57e1cde047bbe710d3446b30da5': {'arxiv_id': None,\n",
       "  's2_paperId': '9819b600a828a57e1cde047bbe710d3446b30da5',\n",
       "  'title': 'Recurrent neural network based language model',\n",
       "  'abstract': None,\n",
       "  'venue': 'Interspeech',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 497,\n",
       "  'citationCount': 5874,\n",
       "  'publicationType': 'Not available'},\n",
       " '5d90f06bb70a0a3dced62413346235c02b1aa086': {'arxiv_id': None,\n",
       "  's2_paperId': '5d90f06bb70a0a3dced62413346235c02b1aa086',\n",
       "  'title': 'Learning Multiple Layers of Features from Tiny Images',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 7996,\n",
       "  'citationCount': 34528,\n",
       "  'publicationType': 'Not available'},\n",
       " '9238de967f7bd30d5fb5b2a9450e42023add7910': {'arxiv_id': None,\n",
       "  's2_paperId': '9238de967f7bd30d5fb5b2a9450e42023add7910',\n",
       "  'title': 'Calculating the singular values and pseudo-inverse of a matrix',\n",
       "  'abstract': None,\n",
       "  'venue': 'Milestones in Matrix Computation',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 125,\n",
       "  'citationCount': 1458,\n",
       "  'publicationType': 'Not available'},\n",
       " '211855f1de279c452858177331860cbc326351ab': {'arxiv_id': None,\n",
       "  's2_paperId': '211855f1de279c452858177331860cbc326351ab',\n",
       "  'title': 'Designing Neural Networks Using Genetic Algorithms with Graph Generation System',\n",
       "  'abstract': None,\n",
       "  'venue': 'Complex Systems',\n",
       "  'year': 1990,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 44,\n",
       "  'citationCount': 831,\n",
       "  'publicationType': 'Not available'},\n",
       " '05d31db3f6d6265a30e82b9e89435cacc7618308': {'arxiv_id': None,\n",
       "  's2_paperId': '05d31db3f6d6265a30e82b9e89435cacc7618308',\n",
       "  'title': 'CALCULATING THE SINGULAR VALUES AND PSEUDOINVERSE OF A MATRIX',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': None,\n",
       "  'referenceCount': 1,\n",
       "  'influentialCitationCount': 43,\n",
       "  'citationCount': 440,\n",
       "  'publicationType': 'Not available'},\n",
       " '25cfb808b30e365b109e42d66cee7eb23393644a': {'arxiv_id': '2106.05967v3',\n",
       "  's2_paperId': '25cfb808b30e365b109e42d66cee7eb23393644a',\n",
       "  'title': 'Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations',\n",
       "  'abstract': 'Contrastive self-supervised learning has outperformed supervised pretraining on many downstream tasks like segmentation and object detection. However, current methods are still primarily applied to curated datasets like ImageNet. In this paper, we first study how biases in the dataset affect existing methods. Our results show that current contrastive approaches work surprisingly well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed and (iii) general versus domain-specific datasets. Second, given the generality of the approach, we try to realize further gains with minor modifications. We show that learning additional invariances -- through the use of multi-scale cropping, stronger augmentations and nearest neighbors -- improves the representations. Finally, we observe that MoCo learns spatially structured representations when trained with a multi-crop strategy. The representations can be used for semantic segment retrieval and video instance segmentation without finetuning. Moreover, the results are on par with specialized models. We hope this work will serve as a useful study for other researchers. The code and models are available at https://github.com/wvangansbeke/Revisiting-Contrastive-SSL.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 68,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 65,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '2d67415394aba2c834be722f7de02519842155d7': {'arxiv_id': None,\n",
       "  's2_paperId': '2d67415394aba2c834be722f7de02519842155d7',\n",
       "  'title': 'With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 77,\n",
       "  'influentialCitationCount': 68,\n",
       "  'citationCount': 441,\n",
       "  'publicationType': 'Not available'},\n",
       " '4f8ac76becef4536472b4b98e486c52bf19700d4': {'arxiv_id': None,\n",
       "  's2_paperId': '4f8ac76becef4536472b4b98e486c52bf19700d4',\n",
       "  'title': 'Region Similarity Representation Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 58,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 116,\n",
       "  'publicationType': 'Not available'},\n",
       " '95c8c13d21932811e285193ca03c8387e16874a7': {'arxiv_id': None,\n",
       "  's2_paperId': '95c8c13d21932811e285193ca03c8387e16874a7',\n",
       "  'title': 'Efficient Visual Pretraining with Contrastive Detection',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 70,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 160,\n",
       "  'publicationType': 'Not available'},\n",
       " '4f7a77d04e7e1cff1f6e306f082217be78203643': {'arxiv_id': None,\n",
       "  's2_paperId': '4f7a77d04e7e1cff1f6e306f082217be78203643',\n",
       "  'title': 'Instance Localization for Self-supervised Detection Pretraining',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 144,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a1caff14b4534fa44fc559a613fc9328ffd4822f': {'arxiv_id': None,\n",
       "  's2_paperId': 'a1caff14b4534fa44fc559a613fc9328ffd4822f',\n",
       "  'title': 'Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 102,\n",
       "  'influentialCitationCount': 31,\n",
       "  'citationCount': 247,\n",
       "  'publicationType': 'Not available'},\n",
       " '88ce9fb5fe020092bfc576f63ebb382aaf9a4905': {'arxiv_id': None,\n",
       "  's2_paperId': '88ce9fb5fe020092bfc576f63ebb382aaf9a4905',\n",
       "  'title': 'CASTing Your Model: Learning to Localize Improves Self-Supervised Representations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 79,\n",
       "  'publicationType': 'Not available'},\n",
       " '65f60b889b15531a01e59dce460caa1888adc9bb': {'arxiv_id': None,\n",
       "  's2_paperId': '65f60b889b15531a01e59dce460caa1888adc9bb',\n",
       "  'title': 'Self-Supervised Visual Representation Learning from Hierarchical Grouping',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 74,\n",
       "  'publicationType': 'Not available'},\n",
       " '00969b4dcf8f9b21895bd038a51a038018da84f0': {'arxiv_id': None,\n",
       "  's2_paperId': '00969b4dcf8f9b21895bd038a51a038018da84f0',\n",
       "  'title': 'How Well Do Self-Supervised Models Transfer?',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 69,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 269,\n",
       "  'publicationType': 'Not available'},\n",
       " '0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d': {'arxiv_id': None,\n",
       "  's2_paperId': '0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d',\n",
       "  'title': 'Exploring Simple Siamese Representation Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 689,\n",
       "  'citationCount': 3870,\n",
       "  'publicationType': 'Not available'},\n",
       " '6f92dcefc5f6b4346f619ae7546a8bd2d6decade': {'arxiv_id': None,\n",
       "  's2_paperId': '6f92dcefc5f6b4346f619ae7546a8bd2d6decade',\n",
       "  'title': 'Dense Contrastive Learning for Self-Supervised Visual Pre-Training',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 114,\n",
       "  'citationCount': 661,\n",
       "  'publicationType': 'Not available'},\n",
       " '79ef0efa7217bf85712c652b3b3640176b9e2feb': {'arxiv_id': '2011.05499v2',\n",
       "  's2_paperId': '79ef0efa7217bf85712c652b3b3640176b9e2feb',\n",
       "  'title': 'Unsupervised Learning of Dense Visual Representations',\n",
       "  'abstract': 'Contrastive self-supervised learning has emerged as a promising approach to unsupervised visual representation learning. In general, these methods learn global (image-level) representations that are invariant to different views (i.e., compositions of data augmentation) of the same image. However, many visual understanding tasks require dense (pixel-level) representations. In this paper, we propose View-Agnostic Dense Representation (VADeR) for unsupervised learning of dense representations. VADeR learns pixelwise representations by forcing local features to remain constant over different viewing conditions. Specifically, this is achieved through pixel-level contrastive learning: matching features (that is, features that describes the same location of the scene on different views) should be close in an embedding space, while non-matching features should be apart. VADeR provides a natural representation for dense prediction tasks and transfers well to downstream tasks. Our method outperforms ImageNet supervised pretraining (and strong unsupervised baselines) in multiple dense prediction tasks.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 79,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 187,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " 'bb00cd7a1238efc90ac06b7eeb9b481882b30f8e': {'arxiv_id': None,\n",
       "  's2_paperId': 'bb00cd7a1238efc90ac06b7eeb9b481882b30f8e',\n",
       "  'title': 'Unsupervised Feature Learning by Cross-Level Discrimination between Instances and Groups',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 17,\n",
       "  'publicationType': 'Not available'},\n",
       " '10569a326b860e87f6ebb7bf569af9ea59cc252a': {'arxiv_id': None,\n",
       "  's2_paperId': '10569a326b860e87f6ebb7bf569af9ea59cc252a',\n",
       "  'title': 'Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 209,\n",
       "  'publicationType': 'Not available'},\n",
       " '94a88260f72088fd0e0468ea6856164fdae65df9': {'arxiv_id': None,\n",
       "  's2_paperId': '94a88260f72088fd0e0468ea6856164fdae65df9',\n",
       "  'title': 'Space-Time Correspondence as a Contrastive Random Walk',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 124,\n",
       "  'influentialCitationCount': 50,\n",
       "  'citationCount': 287,\n",
       "  'publicationType': 'Not available'},\n",
       " '008ba3c574f8e5085dfb1eef0342e8b408565e45': {'arxiv_id': None,\n",
       "  's2_paperId': '008ba3c574f8e5085dfb1eef0342e8b408565e45',\n",
       "  'title': 'Labelling unlabelled videos from scratch with multi-modal self-supervision',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 88,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 150,\n",
       "  'publicationType': 'Not available'},\n",
       " '1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af': {'arxiv_id': None,\n",
       "  's2_paperId': '1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af',\n",
       "  'title': 'Unsupervised Learning of Visual Features by Contrasting Cluster Assignments',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 69,\n",
       "  'influentialCitationCount': 569,\n",
       "  'citationCount': 3921,\n",
       "  'publicationType': 'Not available'},\n",
       " '9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2': {'arxiv_id': '2006.10029v2',\n",
       "  's2_paperId': '9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2',\n",
       "  'title': 'Big Self-Supervised Models are Strong Semi-Supervised Learners',\n",
       "  'abstract': 'One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\\\\% ImageNet top-1 accuracy with just 1\\\\% of the labels ($\\\\le$13 labeled images per class) using ResNet-50, a $10\\\\times$ improvement in label efficiency over the previous state-of-the-art. With 10\\\\% of labels, ResNet-50 trained with our method achieves 77.5\\\\% top-1 accuracy, outperforming standard supervised training with all of the labels.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 75,\n",
       "  'influentialCitationCount': 337,\n",
       "  'citationCount': 2180,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '38f93092ece8eee9771e61c1edaf11b1293cae1b': {'arxiv_id': None,\n",
       "  's2_paperId': '38f93092ece8eee9771e61c1edaf11b1293cae1b',\n",
       "  'title': 'Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 103,\n",
       "  'influentialCitationCount': 1148,\n",
       "  'citationCount': 6531,\n",
       "  'publicationType': 'Not available'},\n",
       " '199d88fb9ec7430ca653f4c066b02aa7c3b4dd98': {'arxiv_id': None,\n",
       "  's2_paperId': '199d88fb9ec7430ca653f4c066b02aa7c3b4dd98',\n",
       "  'title': 'What makes instance discrimination good for transfer learning?',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 167,\n",
       "  'publicationType': 'Not available'},\n",
       " '3e86f5a0e2a97894de1cf1f1587799ac79bad0f2': {'arxiv_id': None,\n",
       "  's2_paperId': '3e86f5a0e2a97894de1cf1f1587799ac79bad0f2',\n",
       "  'title': 'VirTex: Learning Visual Representations from Textual Annotations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 120,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 425,\n",
       "  'publicationType': 'Not available'},\n",
       " '152880494ca264adffbab59fa6123c8cbcbc3f20': {'arxiv_id': None,\n",
       "  's2_paperId': '152880494ca264adffbab59fa6123c8cbcbc3f20',\n",
       "  'title': 'Learning To Classify Images Without Labels',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 63,\n",
       "  'influentialCitationCount': 120,\n",
       "  'citationCount': 516,\n",
       "  'publicationType': 'Not available'},\n",
       " '7f768fa192a76ab097ccfda0a68523bc36425423': {'arxiv_id': None,\n",
       "  's2_paperId': '7f768fa192a76ab097ccfda0a68523bc36425423',\n",
       "  'title': 'What makes for good views for contrastive learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 91,\n",
       "  'influentialCitationCount': 117,\n",
       "  'citationCount': 1279,\n",
       "  'publicationType': 'Not available'},\n",
       " '8bf6c69bae0956db13aa9129fedc69fdc1256dce': {'arxiv_id': None,\n",
       "  's2_paperId': '8bf6c69bae0956db13aa9129fedc69fdc1256dce',\n",
       "  'title': 'Prototypical Contrastive Learning of Unsupervised Representations',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 60,\n",
       "  'influentialCitationCount': 110,\n",
       "  'citationCount': 935,\n",
       "  'publicationType': 'Not available'},\n",
       " '00e6834700e9805cb1618433f05915c261a6ba08': {'arxiv_id': None,\n",
       "  's2_paperId': '00e6834700e9805cb1618433f05915c261a6ba08',\n",
       "  'title': 'Multi-Task Learning for Dense Prediction Tasks: A Survey',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 105,\n",
       "  'influentialCitationCount': 54,\n",
       "  'citationCount': 679,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a1b8a8df281bbaec148a897927a49ea47ea31515': {'arxiv_id': None,\n",
       "  's2_paperId': 'a1b8a8df281bbaec148a897927a49ea47ea31515',\n",
       "  'title': 'Improved Baselines with Momentum Contrastive Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 667,\n",
       "  'citationCount': 3318,\n",
       "  'publicationType': 'Not available'},\n",
       " '17d9185640e46a49b260a1e1e533c36a71bffdc8': {'arxiv_id': None,\n",
       "  's2_paperId': '17d9185640e46a49b260a1e1e533c36a71bffdc8',\n",
       "  'title': 'MAST: A Memory-Augmented Self-Supervised Tracker',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 75,\n",
       "  'influentialCitationCount': 26,\n",
       "  'citationCount': 182,\n",
       "  'publicationType': 'Not available'},\n",
       " '7af72a461ed7cda180e7eab878efd5f35d79bbf4': {'arxiv_id': None,\n",
       "  's2_paperId': '7af72a461ed7cda180e7eab878efd5f35d79bbf4',\n",
       "  'title': 'A Simple Framework for Contrastive Learning of Visual Representations',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 68,\n",
       "  'influentialCitationCount': 3455,\n",
       "  'citationCount': 17875,\n",
       "  'publicationType': 'Not available'},\n",
       " '0170bb0b524df2c81b5adc3062c6001a2eb34c96': {'arxiv_id': None,\n",
       "  's2_paperId': '0170bb0b524df2c81b5adc3062c6001a2eb34c96',\n",
       "  'title': 'Self-Supervised Learning of Pretext-Invariant Representations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 92,\n",
       "  'influentialCitationCount': 96,\n",
       "  'citationCount': 1428,\n",
       "  'publicationType': 'Not available'},\n",
       " 'add2f205338d70e10ce5e686df4a690e2851bdfc': {'arxiv_id': None,\n",
       "  's2_paperId': 'add2f205338d70e10ce5e686df4a690e2851bdfc',\n",
       "  'title': 'Momentum Contrast for Unsupervised Visual Representation Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 1898,\n",
       "  'citationCount': 11669,\n",
       "  'publicationType': 'Not available'},\n",
       " '87045bfc6f8036d032ab6ad1ebeb0377db05da9a': {'arxiv_id': None,\n",
       "  's2_paperId': '87045bfc6f8036d032ab6ad1ebeb0377db05da9a',\n",
       "  'title': 'Self-labelling via simultaneous clustering and representation learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 77,\n",
       "  'influentialCitationCount': 88,\n",
       "  'citationCount': 754,\n",
       "  'publicationType': 'Not available'},\n",
       " '1530deed1d3e8ed487fcfaee00becfbcf62ea36a': {'arxiv_id': None,\n",
       "  's2_paperId': '1530deed1d3e8ed487fcfaee00becfbcf62ea36a',\n",
       "  'title': 'SegSort: Segmentation by Discriminative Sorting of Segments',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 85,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 148,\n",
       "  'publicationType': 'Not available'},\n",
       " '97f4d09175705be4677d675fa27e55defac44800': {'arxiv_id': None,\n",
       "  's2_paperId': '97f4d09175705be4677d675fa27e55defac44800',\n",
       "  'title': 'Contrastive Multiview Coding',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 99,\n",
       "  'influentialCitationCount': 265,\n",
       "  'citationCount': 2325,\n",
       "  'publicationType': 'Not available'},\n",
       " '21de3a36cb51adc205fad8a1d3d69118891dc3dd': {'arxiv_id': None,\n",
       "  's2_paperId': '21de3a36cb51adc205fad8a1d3d69118891dc3dd',\n",
       "  'title': 'AutoAugment: Learning Augmentation Strategies From Data',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 74,\n",
       "  'influentialCitationCount': 260,\n",
       "  'citationCount': 2146,\n",
       "  'publicationType': 'Not available'},\n",
       " '0b313332dbfc4abfea6140d0be0b810808493cf5': {'arxiv_id': '1904.11567v3',\n",
       "  's2_paperId': '0b313332dbfc4abfea6140d0be0b810808493cf5',\n",
       "  'title': 'Unsupervised Deep Learning by Neighbourhood Discovery',\n",
       "  'abstract': 'Deep convolutional neural networks (CNNs) have demonstrated remarkable success in computer vision by supervisedly learning strong visual feature representations. However, training CNNs relies heavily on the availability of exhaustive training data annotations, limiting significantly their deployment and scalability in many application scenarios. In this work, we introduce a generic unsupervised deep learning approach to training deep models without the need for any manual label supervision. Specifically, we progressively discover sample anchored/centred neighbourhoods to reason and learn the underlying class decision boundaries iteratively and accumulatively. Every single neighbourhood is specially formulated so that all the member samples can share the same unseen class labels at high probability for facilitating the extraction of class discriminative feature representations during training. Experiments on image classification show the performance advantages of the proposed method over the state-of-the-art unsupervised learning models on six benchmarks including both coarse-grained and fine-grained object image categorisation.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 152,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '73c07e0a998576bb9d9409e5eed713788c0be037': {'arxiv_id': None,\n",
       "  's2_paperId': '73c07e0a998576bb9d9409e5eed713788c0be037',\n",
       "  'title': 'Large-Scale Long-Tailed Recognition in an Open World',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 68,\n",
       "  'influentialCitationCount': 244,\n",
       "  'citationCount': 1117,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b': {'arxiv_id': None,\n",
       "  's2_paperId': 'e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b',\n",
       "  'title': 'Unsupervised Embedding Learning via Invariant and Spreading Instance Feature',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 47,\n",
       "  'citationCount': 564,\n",
       "  'publicationType': 'Not available'},\n",
       " '4c94ee7df6bc2bfcac76703be4f059a79010f7e5': {'arxiv_id': None,\n",
       "  's2_paperId': '4c94ee7df6bc2bfcac76703be4f059a79010f7e5',\n",
       "  'title': 'Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 220,\n",
       "  'influentialCitationCount': 64,\n",
       "  'citationCount': 1660,\n",
       "  'publicationType': 'Not available'},\n",
       " '5ac18d505ed6d10e8692cbb7d33f6852e6782692': {'arxiv_id': None,\n",
       "  's2_paperId': '5ac18d505ed6d10e8692cbb7d33f6852e6782692',\n",
       "  'title': 'The Open Images Dataset V4',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Journal of Computer Vision',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 82,\n",
       "  'influentialCitationCount': 124,\n",
       "  'citationCount': 1324,\n",
       "  'publicationType': 'Not available'},\n",
       " '1d033b30f38642e4b6dd146bb8b464bfb58aad96': {'arxiv_id': None,\n",
       "  's2_paperId': '1d033b30f38642e4b6dd146bb8b464bfb58aad96',\n",
       "  'title': 'Deep Clustering for Unsupervised Learning of Visual Features',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 80,\n",
       "  'influentialCitationCount': 211,\n",
       "  'citationCount': 1823,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b227f3e4c0dc96e5ac5426b85485a70f2175a205': {'arxiv_id': None,\n",
       "  's2_paperId': 'b227f3e4c0dc96e5ac5426b85485a70f2175a205',\n",
       "  'title': 'Representation Learning with Contrastive Predictive Coding',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 1289,\n",
       "  'citationCount': 9761,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f986968735459e789890f24b6b277b0920a9725d': {'arxiv_id': None,\n",
       "  's2_paperId': 'f986968735459e789890f24b6b277b0920a9725d',\n",
       "  'title': 'Places: A 10 Million Image Database for Scene Recognition',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 614,\n",
       "  'citationCount': 3994,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e65c2b0feddfe4c89e9955ca9b5ece6ef416628f': {'arxiv_id': None,\n",
       "  's2_paperId': 'e65c2b0feddfe4c89e9955ca9b5ece6ef416628f',\n",
       "  'title': 'BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 261,\n",
       "  'citationCount': 2041,\n",
       "  'publicationType': 'Not available'},\n",
       " '155b7782dbd713982a4133df3aee7adfd0b6b304': {'arxiv_id': None,\n",
       "  's2_paperId': '155b7782dbd713982a4133df3aee7adfd0b6b304',\n",
       "  'title': 'Unsupervised Feature Learning via Non-parametric Instance Discrimination',\n",
       "  'abstract': None,\n",
       "  'venue': '2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 343,\n",
       "  'citationCount': 3354,\n",
       "  'publicationType': 'Not available'},\n",
       " '36ad8eaa6d01ceec2efda3a9563320822920088f': {'arxiv_id': None,\n",
       "  's2_paperId': '36ad8eaa6d01ceec2efda3a9563320822920088f',\n",
       "  'title': 'On Regularized Losses for Weakly-supervised CNN Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 52,\n",
       "  'citationCount': 292,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b36a5bb1707bb9c70025294b3a310138aae8327a': {'arxiv_id': None,\n",
       "  's2_paperId': 'b36a5bb1707bb9c70025294b3a310138aae8327a',\n",
       "  'title': 'Automatic differentiation in PyTorch',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 6,\n",
       "  'influentialCitationCount': 1575,\n",
       "  'citationCount': 14809,\n",
       "  'publicationType': 'Not available'},\n",
       " '204e3073870fae3d05bcbc2f6a8e263d9b72e776': {'arxiv_id': None,\n",
       "  's2_paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "  'title': 'Attention is All you Need',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 16613,\n",
       "  'citationCount': 124100,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c8c494ee5488fe20e0aa01bddf3fc4632086d654': {'arxiv_id': None,\n",
       "  's2_paperId': 'c8c494ee5488fe20e0aa01bddf3fc4632086d654',\n",
       "  'title': 'The Cityscapes Dataset for Semantic Urban Scene Understanding',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 86,\n",
       "  'influentialCitationCount': 2256,\n",
       "  'citationCount': 11334,\n",
       "  'publicationType': 'Not available'},\n",
       " '31f9eb39d840821979e5df9f34a6e92dd9c879f2': {'arxiv_id': None,\n",
       "  's2_paperId': '31f9eb39d840821979e5df9f34a6e92dd9c879f2',\n",
       "  'title': 'Learning Deep Features for Discriminative Localization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 1432,\n",
       "  'citationCount': 9098,\n",
       "  'publicationType': 'Not available'},\n",
       " '696ca58d93f6404fea0fc75c62d1d7b378f47628': {'arxiv_id': None,\n",
       "  's2_paperId': '696ca58d93f6404fea0fc75c62d1d7b378f47628',\n",
       "  'title': 'Microsoft COCO Captions: Data Collection and Evaluation Server',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 447,\n",
       "  'citationCount': 2399,\n",
       "  'publicationType': 'Not available'},\n",
       " 'da2f85e313160992b1a0e3db70eb02b58ec740c0': {'arxiv_id': None,\n",
       "  's2_paperId': 'da2f85e313160992b1a0e3db70eb02b58ec740c0',\n",
       "  'title': 'Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Computer Vision',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 72,\n",
       "  'citationCount': 906,\n",
       "  'publicationType': 'Not available'},\n",
       " '8e3f12804882b60ad5f59aad92755c5edb34860e': {'arxiv_id': None,\n",
       "  's2_paperId': '8e3f12804882b60ad5f59aad92755c5edb34860e',\n",
       "  'title': 'Food-101 - Mining Discriminative Components with Random Forests',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 285,\n",
       "  'citationCount': 2298,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c1994ba5946456fc70948c549daf62363f13fa2d': {'arxiv_id': None,\n",
       "  's2_paperId': 'c1994ba5946456fc70948c549daf62363f13fa2d',\n",
       "  'title': 'Indoor Segmentation and Support Inference from RGBD Images',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Computer Vision',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 961,\n",
       "  'citationCount': 5522,\n",
       "  'publicationType': 'Not available'},\n",
       " '84b50ebe85f7a1721800125e7882fce8c45b5c5a': {'arxiv_id': None,\n",
       "  's2_paperId': '84b50ebe85f7a1721800125e7882fce8c45b5c5a',\n",
       "  'title': 'Cats and dogs',\n",
       "  'abstract': None,\n",
       "  'venue': '2012 IEEE Conference on Computer Vision and Pattern Recognition',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 206,\n",
       "  'citationCount': 2009,\n",
       "  'publicationType': 'Not available'},\n",
       " '908091b4a8757c3b2f7d9cfa2c4f616ee12c5157': {'arxiv_id': None,\n",
       "  's2_paperId': '908091b4a8757c3b2f7d9cfa2c4f616ee12c5157',\n",
       "  'title': 'SUN database: Large-scale scene recognition from abbey to zoo',\n",
       "  'abstract': 'Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.',\n",
       "  'venue': '2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 419,\n",
       "  'citationCount': 3603,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e3ce36b9deb47aa6bb2aa19c4bfa71283b505025': {'arxiv_id': None,\n",
       "  's2_paperId': 'e3ce36b9deb47aa6bb2aa19c4bfa71283b505025',\n",
       "  'title': 'Noise-contrastive estimation: A new estimation principle for unnormalized statistical models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 250,\n",
       "  'citationCount': 2327,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd2c733e34d48784a37d717fe43d9e93277a8c53e': {'arxiv_id': None,\n",
       "  's2_paperId': 'd2c733e34d48784a37d717fe43d9e93277a8c53e',\n",
       "  'title': 'ImageNet: A large-scale hierarchical image database',\n",
       "  'abstract': None,\n",
       "  'venue': '2009 IEEE Conference on Computer Vision and Pattern Recognition',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 9419,\n",
       "  'citationCount': 61716,\n",
       "  'publicationType': 'Not available'},\n",
       " '0967f182cb3e69c6248abde1f084010fe664b10b': {'arxiv_id': None,\n",
       "  's2_paperId': '0967f182cb3e69c6248abde1f084010fe664b10b',\n",
       "  'title': '\\nAcknowledgment',\n",
       "  'abstract': None,\n",
       "  'venue': 'Plant Biotechnology Reports',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 120,\n",
       "  'publicationType': 'Not available'},\n",
       " 'eae500ce89f7cc5cd48a58c4ba7edb2f02826b85': {'arxiv_id': None,\n",
       "  's2_paperId': 'eae500ce89f7cc5cd48a58c4ba7edb2f02826b85',\n",
       "  'title': 'Collecting a Large-scale Dataset of Fine-grained Cars',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 13,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 200,\n",
       "  'publicationType': 'Not available'},\n",
       " '494753f9dce0a95c02d40142d86b2a668a2a0d2a': {'arxiv_id': '1608.02146v2',\n",
       "  's2_paperId': '494753f9dce0a95c02d40142d86b2a668a2a0d2a',\n",
       "  'title': 'Leveraging Union of Subspace Structure to Improve Constrained Clustering',\n",
       "  'abstract': 'Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present a pairwise-constrained clustering algorithm that actively selects queries based on the union-of-subspaces model. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. We prove that points lying near the intersection of subspaces are points with low margin. Our procedure can be used after any subspace clustering algorithm that outputs an affinity matrix. We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the state-of-the-art active query algorithms on datasets with subspace structure and is competitive on other datasets.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 17,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'db4e3943d20204e9e852c03ee5b65f2ecebd5f18': {'arxiv_id': None,\n",
       "  's2_paperId': 'db4e3943d20204e9e852c03ee5b65f2ecebd5f18',\n",
       "  'title': 'Necessary and sufficient conditions for sketched subspace clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'Allerton Conference on Communication, Control, and Computing',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 17,\n",
       "  'publicationType': 'Not available'},\n",
       " '5f6ebc79212c0289f24651bc9d76e774207e9501': {'arxiv_id': None,\n",
       "  's2_paperId': '5f6ebc79212c0289f24651bc9d76e774207e9501',\n",
       "  'title': 'RandNLA',\n",
       "  'abstract': None,\n",
       "  'venue': 'Communications of the ACM',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 158,\n",
       "  'publicationType': 'Not available'},\n",
       " '98faa760e1ca74154fadf0f7d1bbe54284a647c1': {'arxiv_id': None,\n",
       "  's2_paperId': '98faa760e1ca74154fadf0f7d1bbe54284a647c1',\n",
       "  'title': 'Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 41,\n",
       "  'citationCount': 237,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd56da12a4d9638ec46d7cc5f0e1616e08c0c5ef8': {'arxiv_id': None,\n",
       "  's2_paperId': 'd56da12a4d9638ec46d7cc5f0e1616e08c0c5ef8',\n",
       "  'title': 'A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 36,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e767a107df5d559152912b663b196b009d67f000': {'arxiv_id': None,\n",
       "  's2_paperId': 'e767a107df5d559152912b663b196b009d67f000',\n",
       "  'title': 'Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 60,\n",
       "  'citationCount': 341,\n",
       "  'publicationType': 'Not available'},\n",
       " '7d251dc4e27abd0d1657b3c4c6286af599eaf72f': {'arxiv_id': None,\n",
       "  's2_paperId': '7d251dc4e27abd0d1657b3c4c6286af599eaf72f',\n",
       "  'title': 'Graph Connectivity in Noisy Sparse Subspace Clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 33,\n",
       "  'publicationType': 'Not available'},\n",
       " '5bfd9add3a43c1a720e023be05204bbfe0ad9cf2': {'arxiv_id': None,\n",
       "  's2_paperId': '5bfd9add3a43c1a720e023be05204bbfe0ad9cf2',\n",
       "  'title': 'Online Low-Rank Subspace Clustering by Basis Dictionary Pursuit',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 57,\n",
       "  'publicationType': 'Not available'},\n",
       " '55ab85d44879a4500e9a3a8ac48d69fc0507b53c': {'arxiv_id': None,\n",
       "  's2_paperId': '55ab85d44879a4500e9a3a8ac48d69fc0507b53c',\n",
       "  'title': 'Greedy Subspace Clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 99,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e9b3b53b6758a4f5688311f04f9667694cf7be11': {'arxiv_id': None,\n",
       "  's2_paperId': 'e9b3b53b6758a4f5688311f04f9667694cf7be11',\n",
       "  'title': 'Noise-adaptive Margin-based Active Learning for Multi-dimensional Data',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 9,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd400e1daffc96267eca39de51661a628147614eb': {'arxiv_id': None,\n",
       "  's2_paperId': 'd400e1daffc96267eca39de51661a628147614eb',\n",
       "  'title': 'Active Image Clustering with Pairwise Constraints from Humans',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Journal of Computer Vision',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 83,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 39,\n",
       "  'publicationType': 'Not available'},\n",
       " '7af96885e32f4c0187f380a55996037da90c3020': {'arxiv_id': None,\n",
       "  's2_paperId': '7af96885e32f4c0187f380a55996037da90c3020',\n",
       "  'title': 'Active Clustering with Model-Based Uncertainty Reduction',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 54,\n",
       "  'publicationType': 'Not available'},\n",
       " '1d43ac039b8541fc5ae1b6790e7906f062835ec9': {'arxiv_id': None,\n",
       "  's2_paperId': '1d43ac039b8541fc5ae1b6790e7906f062835ec9',\n",
       "  'title': 'Robust Subspace Clustering via Thresholding',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 62,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 153,\n",
       "  'publicationType': 'Not available'},\n",
       " '132f7199145feab3ddd4bc9215110df7c97d6a94': {'arxiv_id': None,\n",
       "  's2_paperId': '132f7199145feab3ddd4bc9215110df7c97d6a94',\n",
       "  'title': 'On the Fundamental Limits of Recovering Tree Sparse Vectors From Noisy Linear Measurements',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 60,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 22,\n",
       "  'publicationType': 'Not available'},\n",
       " '91e5b3c7750f554bf1626c303beb9ce9f29a3483': {'arxiv_id': '1309.1233v2',\n",
       "  's2_paperId': '91e5b3c7750f554bf1626c303beb9ce9f29a3483',\n",
       "  'title': 'Noisy Sparse Subspace Clustering',\n",
       "  'abstract': 'This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabelled input data points, which are assumed to lie in a union of low-dimensional subspaces. We show that a modified version of SSC is provably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to the practical setting and provides justification to the success of SSC in a class of real applications.',\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 200,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " 'a6de47fa6b9e9f6165c3a2021f4a4e729904d804': {'arxiv_id': None,\n",
       "  's2_paperId': 'a6de47fa6b9e9f6165c3a2021f4a4e729904d804',\n",
       "  'title': 'Low-Rank Matrix and Tensor Completion via Adaptive Sampling',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 158,\n",
       "  'publicationType': 'Not available'},\n",
       " 'be921e6d9b3824c4bad331415445a8a8a6829d9c': {'arxiv_id': None,\n",
       "  's2_paperId': 'be921e6d9b3824c4bad331415445a8a8a6829d9c',\n",
       "  'title': 'Active learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'The Veterinary Record',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 75,\n",
       "  'influentialCitationCount': 168,\n",
       "  'citationCount': 1975,\n",
       "  'publicationType': 'Not available'},\n",
       " '6b2266474b2b5e9ec59f6395ed78035a7e5a3d4d': {'arxiv_id': None,\n",
       "  's2_paperId': '6b2266474b2b5e9ec59f6395ed78035a7e5a3d4d',\n",
       "  'title': 'Robust Subspace Clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 73,\n",
       "  'influentialCitationCount': 45,\n",
       "  'citationCount': 364,\n",
       "  'publicationType': 'Not available'},\n",
       " 'cb41b4b984b9f12a7fb8c397038016e01e07a8f8': {'arxiv_id': None,\n",
       "  's2_paperId': 'cb41b4b984b9f12a7fb8c397038016e01e07a8f8',\n",
       "  'title': 'Compressive Clustering of High-Dimensional Data',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning and Applications',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 10,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b23d572c7f34fdfa8ce6c548f6d8119d79d545d1': {'arxiv_id': None,\n",
       "  's2_paperId': 'b23d572c7f34fdfa8ce6c548f6d8119d79d545d1',\n",
       "  'title': 'Subspace clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'WIREs Data Mining Knowl. Discov.',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 113,\n",
       "  'influentialCitationCount': 62,\n",
       "  'citationCount': 839,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e47deec18f4d68b49d7c08128dd1188ecde44a46': {'arxiv_id': None,\n",
       "  's2_paperId': 'e47deec18f4d68b49d7c08128dd1188ecde44a46',\n",
       "  'title': 'Performance Limits of Compressive Sensing-Based Signal Classification',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Signal Processing',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 52,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b19d27e1bda4d039f9eaf2b2eb9d47f75abe9d75': {'arxiv_id': None,\n",
       "  's2_paperId': 'b19d27e1bda4d039f9eaf2b2eb9d47f75abe9d75',\n",
       "  'title': 'Sparse Subspace Clustering: Algorithm, Theory, and Applications',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 73,\n",
       "  'influentialCitationCount': 492,\n",
       "  'citationCount': 2316,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b42dda210eb740631deb281bca79574805cd16cb': {'arxiv_id': None,\n",
       "  's2_paperId': 'b42dda210eb740631deb281bca79574805cd16cb',\n",
       "  'title': 'A Geometric Analysis of Subspace Clustering with Outliers',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 65,\n",
       "  'influentialCitationCount': 72,\n",
       "  'citationCount': 413,\n",
       "  'publicationType': 'Not available'},\n",
       " 'de829c5f9acef9e9be39f2be55b315bd4516e716': {'arxiv_id': None,\n",
       "  's2_paperId': 'de829c5f9acef9e9be39f2be55b315bd4516e716',\n",
       "  'title': 'On the Power of Adaptivity in Sparse Recovery',\n",
       "  'abstract': \"The goal of (stable) sparse recovery is to recover a $k$-sparse approximation $x^*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is to recover $x^*$ such that$$\\\\norm{p}{x-x^*} \\\\le C \\\\min_{k\\\\text{-sparse } x'} \\\\norm{q}{x-x'}$$for some constant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or $p=q=2$, this task can be accomplished using $m=O(k \\\\log (n/k))$ {\\\\em non-adaptive}measurements~\\\\cite{CRT06:Stable-Signal} and that this bound is tight~\\\\cite{DIPW, FPRU, PW11}. In this paper we show that if one is allowed to perform measurements that are {\\\\em adaptive}, then the number of measurements can be considerably reduced. Specifically, for $C=1+\\\\epsilon$ and $p=q=2$ we show\\\\begin{itemize}\\\\item A scheme with $m=O(\\\\frac{1}{\\\\eps}k \\\\log \\\\log (n\\\\eps/k))$ measurements that uses $O(\\\\log^* k \\\\cdot \\\\log \\\\log (n\\\\eps/k))$ rounds. This is a significant improvement over the best possible non-adaptive bound. \\\\item A scheme with $m=O(\\\\frac{1}{\\\\eps}k \\\\log (k/\\\\eps) + k \\\\log (n/k))$ measurements that uses {\\\\em two} rounds. This improves over the best possible non-adaptive bound. \\\\end{itemize} To the best of our knowledge, these are the first results of this type.\",\n",
       "  'venue': 'IEEE Annual Symposium on Foundations of Computer Science',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 78,\n",
       "  'publicationType': 'Not available'},\n",
       " '53aadb104f418ac1fda96433b0947f1ebc9a3cfa': {'arxiv_id': None,\n",
       "  's2_paperId': '53aadb104f418ac1fda96433b0947f1ebc9a3cfa',\n",
       "  'title': 'Fast Approximate Text Document Clustering Using Compressive Sampling',\n",
       "  'abstract': None,\n",
       "  'venue': 'ECML/PKDD',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 10,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 7,\n",
       "  'publicationType': 'Not available'},\n",
       " '3d22552235f5c51d33b657fb816050820c6e055e': {'arxiv_id': '1109.0367v1',\n",
       "  's2_paperId': '3d22552235f5c51d33b657fb816050820c6e055e',\n",
       "  'title': 'Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation',\n",
       "  'abstract': 'Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the sub-problems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve low-rank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity O(n3) of the original ADM based method to O(rn2), where r and n are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 168,\n",
       "  'citationCount': 1201,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'a58fd0822e57490a9d9a80bef460ba7087a17579': {'arxiv_id': None,\n",
       "  's2_paperId': 'a58fd0822e57490a9d9a80bef460ba7087a17579',\n",
       "  'title': 'Graph Regularized Nonnegative Matrix Factorization for Data Representation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 232,\n",
       "  'citationCount': 1679,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f73f980b020a1dfdf4c017e67d269ea439c7411b': {'arxiv_id': None,\n",
       "  's2_paperId': 'f73f980b020a1dfdf4c017e67d269ea439c7411b',\n",
       "  'title': 'Active Spectral Clustering',\n",
       "  'abstract': None,\n",
       "  'venue': '2010 IEEE International Conference on Data Mining',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 92,\n",
       "  'publicationType': 'Not available'},\n",
       " '8ee2a855551bfb6803d688d7bd15ab265aa9c405': {'arxiv_id': None,\n",
       "  's2_paperId': '8ee2a855551bfb6803d688d7bd15ab265aa9c405',\n",
       "  'title': 'Hybrid Linear Modeling via Local Best-Fit Flats',\n",
       "  'abstract': 'We present a simple and fast geometric method for modeling data by a union of affine subspaces. The method begins by forming a collection of local best-fit affine subspaces, i.e., subspaces approximating the data in local neighborhoods. The correct sizes of the local neighborhoods are determined automatically by the Jones’ β2 numbers (we prove under certain geometric conditions that our method finds the optimal local neighborhoods). The collection of subspaces is further processed by a greedy selection procedure or a spectral method to generate the final model. We discuss applications to tracking-based motion segmentation and clustering of faces under different illuminating conditions. We give extensive experimental evidence demonstrating the state of the art accuracy and speed of the suggested algorithms on these problems and also on synthetic hybrid linear data as well as the MNIST handwritten digits data; and we demonstrate how to use our algorithms for fast determination of the number of affine subspaces.',\n",
       "  'venue': 'International Journal of Computer Vision',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 204,\n",
       "  'publicationType': 'Not available'},\n",
       " '047175fb23f6f152d86e81100ba7140dd2847636': {'arxiv_id': None,\n",
       "  's2_paperId': '047175fb23f6f152d86e81100ba7140dd2847636',\n",
       "  'title': 'Robust Subspace Segmentation by Low-Rank Representation',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 27,\n",
       "  'influentialCitationCount': 303,\n",
       "  'citationCount': 1644,\n",
       "  'publicationType': 'Not available'},\n",
       " '0400bec2d4ae5b265e235627c8a44b8787c7128f': {'arxiv_id': None,\n",
       "  's2_paperId': '0400bec2d4ae5b265e235627c8a44b8787c7128f',\n",
       "  'title': 'Distilled Sensing: Adaptive Sampling for Sparse Detection and Estimation',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 154,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c8831d7d318b8d59f9b958d250a58f253f08bd8a': {'arxiv_id': None,\n",
       "  's2_paperId': 'c8831d7d318b8d59f9b958d250a58f253f08bd8a',\n",
       "  'title': 'Robust principal component analysis?',\n",
       "  'abstract': None,\n",
       "  'venue': 'JACM',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 67,\n",
       "  'influentialCitationCount': 1232,\n",
       "  'citationCount': 6966,\n",
       "  'publicationType': 'Not available'},\n",
       " '8b77d4fc4472ded7218538dae84fc77c9859461e': {'arxiv_id': None,\n",
       "  's2_paperId': '8b77d4fc4472ded7218538dae84fc77c9859461e',\n",
       "  'title': 'Active query selection for semi-supervised clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Pattern Recognition',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 12,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 121,\n",
       "  'publicationType': 'Not available'},\n",
       " '22effe373836d4feae2ce3e8abec0058438bebb9': {'arxiv_id': None,\n",
       "  's2_paperId': '22effe373836d4feae2ce3e8abec0058438bebb9',\n",
       "  'title': 'A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': '2007 IEEE Conference on Computer Vision and Pattern Recognition',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 103,\n",
       "  'citationCount': 739,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fe0aa826fb1ac85d2170afda1f80c414e2c309f7': {'arxiv_id': None,\n",
       "  's2_paperId': 'fe0aa826fb1ac85d2170afda1f80c414e2c309f7',\n",
       "  'title': 'Measuring Constraint-Set Utility for Partitional Clustering Algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': 'European Conference on Principles of Data Mining and Knowledge Discovery',\n",
       "  'year': 2006,\n",
       "  'referenceCount': 11,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 217,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c05fa5806f6ceadfebe7fb7f477fdd72b3584e24': {'arxiv_id': None,\n",
       "  's2_paperId': 'c05fa5806f6ceadfebe7fb7f477fdd72b3584e24',\n",
       "  'title': 'Active Constrained Clustering by Examining Spectral Eigenvectors',\n",
       "  'abstract': None,\n",
       "  'venue': 'IFIP Working Conference on Database Semantics',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 86,\n",
       "  'publicationType': 'Not available'},\n",
       " '95a2ade2834ce6a85d0e8e6b82c81deb34cd115d': {'arxiv_id': None,\n",
       "  's2_paperId': '95a2ade2834ce6a85d0e8e6b82c81deb34cd115d',\n",
       "  'title': 'Active Semi-Supervision for Pairwise Constrained Clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'SDM',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 103,\n",
       "  'citationCount': 666,\n",
       "  'publicationType': 'Not available'},\n",
       " '66e6a411a7342203ebbc22fbe9a3740b744d7cbc': {'arxiv_id': None,\n",
       "  's2_paperId': '66e6a411a7342203ebbc22fbe9a3740b744d7cbc',\n",
       "  'title': 'Lambertian reflectance and linear subspaces',\n",
       "  'abstract': None,\n",
       "  'venue': 'Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001',\n",
       "  'year': 2001,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 168,\n",
       "  'citationCount': 1913,\n",
       "  'publicationType': 'Not available'},\n",
       " '0bacca0993a3f51649a6bb8dbb093fc8d8481ad4': {'arxiv_id': None,\n",
       "  's2_paperId': '0bacca0993a3f51649a6bb8dbb093fc8d8481ad4',\n",
       "  'title': 'Constrained K-means Clustering with Background Knowledge',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2001,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 255,\n",
       "  'citationCount': 2867,\n",
       "  'publicationType': 'Not available'},\n",
       " '18c72175ddbb7d5956d180b65a96005c100f6014': {'arxiv_id': None,\n",
       "  's2_paperId': '18c72175ddbb7d5956d180b65a96005c100f6014',\n",
       "  'title': 'From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 2001,\n",
       "  'referenceCount': 99,\n",
       "  'influentialCitationCount': 397,\n",
       "  'citationCount': 5094,\n",
       "  'publicationType': 'Not available'},\n",
       " '21cea8f56a0d067d640f923b2d69e18ed5542f6d': {'arxiv_id': None,\n",
       "  's2_paperId': '21cea8f56a0d067d640f923b2d69e18ed5542f6d',\n",
       "  'title': 'A Bayesian Computer Vision System for Modeling Human Interactions',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'year': 1999,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 105,\n",
       "  'citationCount': 1933,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ee07d502c11c8ef8a152b6feef22695249d0764a': {'arxiv_id': None,\n",
       "  's2_paperId': 'ee07d502c11c8ef8a152b6feef22695249d0764a',\n",
       "  'title': 'Shape and motion from image streams under orthography: a factorization method',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Journal of Computer Vision',\n",
       "  'year': 1992,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 263,\n",
       "  'citationCount': 2573,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fbb65d2c24b4e71ca1de8128af33622d61488609': {'arxiv_id': None,\n",
       "  's2_paperId': 'fbb65d2c24b4e71ca1de8128af33622d61488609',\n",
       "  'title': 'Generalized Principal Component Analysis',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 202,\n",
       "  'publicationType': 'Not available'},\n",
       " '6d73c9947d840cd84a8eee79c224add5fbbb929e': {'arxiv_id': None,\n",
       "  's2_paperId': '6d73c9947d840cd84a8eee79c224add5fbbb929e',\n",
       "  'title': 'FINDING STRUCTURE WITH RANDOMNESS : PROBABILISTIC ALGORITHMS FOR CONSTRUCTING',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 5,\n",
       "  'influentialCitationCount': 305,\n",
       "  'citationCount': 1743,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ccd77a082021fbe6535972a2eb87076237069061': {'arxiv_id': None,\n",
       "  's2_paperId': 'ccd77a082021fbe6535972a2eb87076237069061',\n",
       "  'title': 'Margin-based active subspace clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 6,\n",
       "  'publicationType': 'Not available'},\n",
       " '444d70e3331b5083b40ef32e49390ef683a65e67': {'arxiv_id': None,\n",
       "  's2_paperId': '444d70e3331b5083b40ef32e49390ef683a65e67',\n",
       "  'title': 'Matrix Computations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Encyclopedia of Parallel Computing',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 5,\n",
       "  'influentialCitationCount': 1306,\n",
       "  'citationCount': 12976,\n",
       "  'publicationType': 'Not available'},\n",
       " '3291d89de761058bdf52b7302909318e4c30ee8f': {'arxiv_id': None,\n",
       "  's2_paperId': '3291d89de761058bdf52b7302909318e4c30ee8f',\n",
       "  'title': 'Graph Regularized Non-negative Matrix Factorization for Data Representation ∗',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 114,\n",
       "  'publicationType': 'Not available'},\n",
       " 'dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2': {'arxiv_id': None,\n",
       "  's2_paperId': 'dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2',\n",
       "  'title': 'The mnist database of handwritten digits',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 1,\n",
       "  'influentialCitationCount': 1159,\n",
       "  'citationCount': 6664,\n",
       "  'publicationType': 'Not available'},\n",
       " '06eba1e64658c38eb9bea5e1188e0191267b0a71': {'arxiv_id': None,\n",
       "  's2_paperId': '06eba1e64658c38eb9bea5e1188e0191267b0a71',\n",
       "  'title': 'Personal correspondence',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 45,\n",
       "  'citationCount': 548,\n",
       "  'publicationType': 'Not available'},\n",
       " '7400912ca8c9ffbe2b9435ea9e4d97776b002228': {'arxiv_id': None,\n",
       "  's2_paperId': '7400912ca8c9ffbe2b9435ea9e4d97776b002228',\n",
       "  'title': 'k-Plane Clustering',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Global Optimization',\n",
       "  'year': 2000,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 40,\n",
       "  'citationCount': 405,\n",
       "  'publicationType': 'Not available'},\n",
       " '77afac8f4d7f47c8b34371d8f8355cefbea1d4f6': {'arxiv_id': None,\n",
       "  's2_paperId': '77afac8f4d7f47c8b34371d8f8355cefbea1d4f6',\n",
       "  'title': 'Columbia Object Image Library (COIL100)',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1996,\n",
       "  'referenceCount': 5,\n",
       "  'influentialCitationCount': 306,\n",
       "  'citationCount': 2364,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c29a5a6bd5dc47638d34584a5774d05076c0fd0f': {'arxiv_id': '2105.10373v1',\n",
       "  's2_paperId': 'c29a5a6bd5dc47638d34584a5774d05076c0fd0f',\n",
       "  'title': 'A Precise Performance Analysis of Support Vector Regression',\n",
       "  'abstract': 'In this paper, we study the hard and soft support vector regression techniques applied to a set of $n$ linear measurements of the form $y_i=\\\\boldsymbol{\\\\beta}_\\\\star^{T}{\\\\bf x}_i +n_i$ where $\\\\boldsymbol{\\\\beta}_\\\\star$ is an unknown vector, $\\\\left\\\\{{\\\\bf x}_i\\\\right\\\\}_{i=1}^n$ are the feature vectors and $\\\\left\\\\{{n}_i\\\\right\\\\}_{i=1}^n$ model the noise. Particularly, under some plausible assumptions on the statistical distribution of the data, we characterize the feasibility condition for the hard support vector regression in the regime of high dimensions and, when feasible, derive an asymptotic approximation for its risk. Similarly, we study the test risk for the soft support vector regression as a function of its parameters. Our results are then used to optimally tune the parameters intervening in the design of hard and soft support vector regression algorithms. Based on our analysis, we illustrate that adding more samples may be harmful to the test performance of support vector regression, while it is always beneficial when the parameters are optimally selected. Such a result reminds a similar phenomenon observed in modern learning architectures according to which optimally tuned architectures present a decreasing test performance curve with respect to the number of samples.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 3,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '6424671b027de8a686a56082c237c1ee24d14f64': {'arxiv_id': None,\n",
       "  's2_paperId': '6424671b027de8a686a56082c237c1ee24d14f64',\n",
       "  'title': 'Regularization in High-Dimensional Regression and Classification via Random Matrix Theory',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 13,\n",
       "  'publicationType': 'Not available'},\n",
       " '1ba1e560036362256fdff0180bd6837fac094ee5': {'arxiv_id': None,\n",
       "  's2_paperId': '1ba1e560036362256fdff0180bd6837fac094ee5',\n",
       "  'title': 'On the Precise Error Analysis of Support Vector Machines',\n",
       "  'abstract': 'This paper investigates the asymptotic behavior of the soft-margin and hard-margin support vector machine (SVM) classifiers for simultaneously high-dimensional and numerous data (large <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math></inline-formula> and large <inline-formula><tex-math notation=\"LaTeX\">$p$</tex-math></inline-formula> with <inline-formula><tex-math notation=\"LaTeX\">$n/p\\\\to \\\\delta$</tex-math></inline-formula>) drawn from a Gaussian mixture distribution. Sharp predictions of the classification error rate of the hard-margin and soft-margin SVM are provided, as well as asymptotic limits of as such important parameters as the margin and the bias. As a further outcome, the analysis allows for the identification of the maximum number of training samples that the hard-margin SVM is able to separate. The precise nature of our results allows for an accurate performance comparison of the hard-margin and soft-margin SVM as well as a better understanding of the involved parameters (such as the number of measurements and the margin parameter) on the classification performance. Our analysis, confirmed by a set of numerical experiments, builds upon the convex Gaussian min-max Theorem, and extends its scope to new problems never studied before by this framework.',\n",
       "  'venue': 'IEEE Open Journal of Signal Processing',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 39,\n",
       "  'publicationType': 'Not available'},\n",
       " '4df60eaad8933ae16eb8744fe2cc7229fbe4879a': {'arxiv_id': None,\n",
       "  's2_paperId': '4df60eaad8933ae16eb8744fe2cc7229fbe4879a',\n",
       "  'title': 'Optimal Regularization Can Mitigate Double Descent',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 22,\n",
       "  'citationCount': 127,\n",
       "  'publicationType': 'Not available'},\n",
       " '906649c59f1cddc8d668bb20eda1ef70c1eeb939': {'arxiv_id': None,\n",
       "  's2_paperId': '906649c59f1cddc8d668bb20eda1ef70c1eeb939',\n",
       "  'title': 'Analytic Study of Double Descent in Binary Classification: The Impact of Loss',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Symposium on Information Theory',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 51,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 51,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ea415809bf87ef4b99966c6c50de6cb996a02a97': {'arxiv_id': None,\n",
       "  's2_paperId': 'ea415809bf87ef4b99966c6c50de6cb996a02a97',\n",
       "  'title': 'Deep double descent: where bigger models and more data hurt',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 103,\n",
       "  'citationCount': 902,\n",
       "  'publicationType': 'Not available'},\n",
       " '6360c5d7c6cfc980e7d7625fc735b713229b8537': {'arxiv_id': None,\n",
       "  's2_paperId': '6360c5d7c6cfc980e7d7625fc735b713229b8537',\n",
       "  'title': 'A Model of Double Descent for High-dimensional Binary Linear Classification',\n",
       "  'abstract': None,\n",
       "  'venue': 'Information and Inference A Journal of the IMA',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 68,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 143,\n",
       "  'publicationType': 'Not available'},\n",
       " 'cd114ecad76bffdb805c7d8ff129cc638383005f': {'arxiv_id': None,\n",
       "  's2_paperId': 'cd114ecad76bffdb805c7d8ff129cc638383005f',\n",
       "  'title': 'The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 67,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 170,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b01415647c2d8eb82e16200ad724a73963d5882c': {'arxiv_id': None,\n",
       "  's2_paperId': 'b01415647c2d8eb82e16200ad724a73963d5882c',\n",
       "  'title': 'Understanding overfitting peaks in generalization error: Analytical risk curves for l2 and l1 penalized interpolation',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 50,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bd949f2a0c23742427d8886e43c1d8c98c1865ea': {'arxiv_id': None,\n",
       "  's2_paperId': 'bd949f2a0c23742427d8886e43c1d8c98c1865ea',\n",
       "  'title': 'Harmless interpolation of noisy data in regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Symposium on Information Theory',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 22,\n",
       "  'citationCount': 203,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd43983700c99dc272b42d0a993679fd429c106cf': {'arxiv_id': None,\n",
       "  's2_paperId': 'd43983700c99dc272b42d0a993679fd429c106cf',\n",
       "  'title': 'Surprises in High-Dimensional Ridgeless Least Squares Interpolation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annals of Statistics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 97,\n",
       "  'influentialCitationCount': 125,\n",
       "  'citationCount': 722,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f8a5278d4142215b33b516db5df1d9eb0d1d066e': {'arxiv_id': None,\n",
       "  's2_paperId': 'f8a5278d4142215b33b516db5df1d9eb0d1d066e',\n",
       "  'title': 'Two models of double descent for weak features',\n",
       "  'abstract': 'The \"double descent\" risk curve was recently proposed to qualitatively describe the out-of-sample prediction accuracy of variably-parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares/least norm predictor. Specifically, it is shown that the risk peaks when the number of features $p$ is close to the sample size $n$, but also that the risk decreases towards its minimum as $p$ increases beyond $n$. This behavior is contrasted with that of \"prescient\" models that select features in an a priori optimal order.',\n",
       "  'venue': 'SIAM Journal on Mathematics of Data Science',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 62,\n",
       "  'citationCount': 371,\n",
       "  'publicationType': 'Not available'},\n",
       " '126b5958cc19b7e40a1f0c07b0a261c5c5094225': {'arxiv_id': None,\n",
       "  's2_paperId': '126b5958cc19b7e40a1f0c07b0a261c5c5094225',\n",
       "  'title': 'Scaling description of generalization with number of parameters in deep learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Statistical Mechanics: Theory and Experiment',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 63,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 194,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f86f1748d1b6d22870f4347fd5d65314ba800583': {'arxiv_id': None,\n",
       "  's2_paperId': 'f86f1748d1b6d22870f4347fd5d65314ba800583',\n",
       "  'title': 'Reconciling modern machine-learning practice and the classical bias–variance trade-off',\n",
       "  'abstract': None,\n",
       "  'venue': 'Proceedings of the National Academy of Sciences of the United States of America',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 133,\n",
       "  'citationCount': 1595,\n",
       "  'publicationType': 'Not available'},\n",
       " '2d636429422559e9cac2c1db33574057cc11b9b4': {'arxiv_id': None,\n",
       "  's2_paperId': '2d636429422559e9cac2c1db33574057cc11b9b4',\n",
       "  'title': 'A jamming transition from under- to over-parametrization affects generalization in deep learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Physics A: Mathematical and Theoretical',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 151,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f6fcdc4e0bb806f4b5c225de53ac7a38f08d3561': {'arxiv_id': None,\n",
       "  's2_paperId': 'f6fcdc4e0bb806f4b5c225de53ac7a38f08d3561',\n",
       "  'title': 'Does data interpolation contradict statistical optimality?',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 23,\n",
       "  'citationCount': 216,\n",
       "  'publicationType': 'Not available'},\n",
       " '2e70943d1bca4b53445c7e1146413bd711000320': {'arxiv_id': None,\n",
       "  's2_paperId': '2e70943d1bca4b53445c7e1146413bd711000320',\n",
       "  'title': 'Optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 89,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a8a55766b61e2a122c0153cf1fc3a0cdbacbc2bd': {'arxiv_id': None,\n",
       "  's2_paperId': 'a8a55766b61e2a122c0153cf1fc3a0cdbacbc2bd',\n",
       "  'title': 'The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annals of Statistics',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 137,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bb2204f2015a7d771b785ea2c52974f63b60aaac': {'arxiv_id': None,\n",
       "  's2_paperId': 'bb2204f2015a7d771b785ea2c52974f63b60aaac',\n",
       "  'title': 'A New Kernel of Support Vector Regression for Forecasting High-Frequency Stock Returns',\n",
       "  'abstract': 'This paper investigates the value of designing a new kernel of support vector regression for the application of forecasting high-frequency stock returns. Under the assumption that each return is an event that triggers momentum and reversal periodically, we decompose each future return into a collection of decaying cosine waves that are functions of past returns. Under realistic assumptions, we reach an analytical expression of the nonlinear relationship between past and future returns and introduce a new kernel for forecasting future returns accordingly. Using high-frequency prices of Chinese CSI 300 index from January 4, 2010, to March 3, 2014, as empirical data, we have the following observations: (1) the new kernel significantly beats the radial basis function kernel and the sigmoid function kernel out-of-sample in both the prediction mean square error and the directional forecast accuracy rate. (2) Besides, the capital gain of a simple trading strategy based on the out-of-sample predictions with the new kernel is also significantly higher. Therefore, we conclude that it is statistically and economically valuable to design a new kernel of support vector regression for forecasting high-frequency stock returns.',\n",
       "  'venue': '',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 25,\n",
       "  'publicationType': 'Not available'},\n",
       " '9835e61b572198069282f2158e6e4d28343d20c6': {'arxiv_id': None,\n",
       "  's2_paperId': '9835e61b572198069282f2158e6e4d28343d20c6',\n",
       "  'title': 'Precise Error Analysis of Regularized  $M$ -Estimators in High Dimensions',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 92,\n",
       "  'influentialCitationCount': 46,\n",
       "  'citationCount': 216,\n",
       "  'publicationType': 'Not available'},\n",
       " '44a3055fffb43b550c5bc405bdf88919fbac0446': {'arxiv_id': None,\n",
       "  's2_paperId': '44a3055fffb43b550c5bc405bdf88919fbac0446',\n",
       "  'title': 'Spectral Analysis of Large Dimensional Random Matrices',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 286,\n",
       "  'citationCount': 1795,\n",
       "  'publicationType': 'Not available'},\n",
       " '7681f59fc2c3b81ed0ec885689d65468f1be1d8b': {'arxiv_id': None,\n",
       "  's2_paperId': '7681f59fc2c3b81ed0ec885689d65468f1be1d8b',\n",
       "  'title': 'On qualitative robustness of support vector machines',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Multivariate Analysis',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 55,\n",
       "  'publicationType': 'Not available'},\n",
       " '634dbb559213ab8c26582851b6d89e364ca34149': {'arxiv_id': None,\n",
       "  's2_paperId': '634dbb559213ab8c26582851b6d89e364ca34149',\n",
       "  'title': 'Localized support vector regression for time series prediction',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neurocomputing',\n",
       "  'year': 2009,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 166,\n",
       "  'publicationType': 'Not available'},\n",
       " '2312e7dc16c186a386746d2d8d5999544947922f': {'arxiv_id': None,\n",
       "  's2_paperId': '2312e7dc16c186a386746d2d8d5999544947922f',\n",
       "  'title': 'Robustness and Regularization of Support Vector Machines',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 55,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 465,\n",
       "  'publicationType': 'Not available'},\n",
       " '77f5130d34d40f372fe30e48b5d37186bb003971': {'arxiv_id': None,\n",
       "  's2_paperId': '77f5130d34d40f372fe30e48b5d37186bb003971',\n",
       "  'title': 'A Forecasting Methodology Using Support Vector Regression and Dynamic Feature Selection',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Information & Knowledge Management',\n",
       "  'year': 2006,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 23,\n",
       "  'publicationType': 'Not available'},\n",
       " '84d58b0385515ea36e879662e4ebb69aaa6c9927': {'arxiv_id': None,\n",
       "  's2_paperId': '84d58b0385515ea36e879662e4ebb69aaa6c9927',\n",
       "  'title': 'The differogram: Non-parametric noise variance estimation and its use for model selection',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neurocomputing',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 20,\n",
       "  'publicationType': 'Not available'},\n",
       " '71c7291ba5fd3d17937f1f81dcde36c883867ba1': {'arxiv_id': None,\n",
       "  's2_paperId': '71c7291ba5fd3d17937f1f81dcde36c883867ba1',\n",
       "  'title': 'Selection of Meta-parameters for Support Vector Regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Neural Networks',\n",
       "  'year': 2002,\n",
       "  'referenceCount': 12,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 103,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e58c0b517abdc0229973ecbeaa3ea014d211e184': {'arxiv_id': None,\n",
       "  's2_paperId': 'e58c0b517abdc0229973ecbeaa3ea014d211e184',\n",
       "  'title': 'Dynamics of Training',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 1996,\n",
       "  'referenceCount': 10,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 13,\n",
       "  'publicationType': 'Not available'},\n",
       " '7ddc47281860f278ce816d90a04f72cfe651abce': {'arxiv_id': None,\n",
       "  's2_paperId': '7ddc47281860f278ce816d90a04f72cfe651abce',\n",
       "  'title': 'Some inequalities for Gaussian processes and applications',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1985,\n",
       "  'referenceCount': 5,\n",
       "  'influentialCitationCount': 63,\n",
       "  'citationCount': 359,\n",
       "  'publicationType': 'Not available'},\n",
       " '522da3897fedc49e64a31ded9a20c7b740c1f619': {'arxiv_id': None,\n",
       "  's2_paperId': '522da3897fedc49e64a31ded9a20c7b740c1f619',\n",
       "  'title': 'On general minimax theorems',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1958,\n",
       "  'referenceCount': 7,\n",
       "  'influentialCitationCount': 225,\n",
       "  'citationCount': 2052,\n",
       "  'publicationType': 'Not available'},\n",
       " '951841a5603a5fb525d37926d365f572790c6bb3': {'arxiv_id': None,\n",
       "  's2_paperId': '951841a5603a5fb525d37926d365f572790c6bb3',\n",
       "  'title': 'Accurate prediction of continuous blood glucose based on support vector regression and differential evolution algorithm',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 98,\n",
       "  'publicationType': 'Not available'},\n",
       " '7d176acb084c29d3dd7ab2b27cf04d471c096e3f': {'arxiv_id': None,\n",
       "  's2_paperId': '7d176acb084c29d3dd7ab2b27cf04d471c096e3f',\n",
       "  'title': \"Support Vector Regression and Time Series Analysis for the Forecasting of Bayannur's Total Water Requirement\",\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Information Technology and Quantitative Management',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 9,\n",
       "  'publicationType': 'Not available'},\n",
       " 'eaedbadc173e6dda73a0523d763288e27cac56fe': {'arxiv_id': None,\n",
       "  's2_paperId': 'eaedbadc173e6dda73a0523d763288e27cac56fe',\n",
       "  'title': 'Practical selection of SVM parameters and noise estimation for SVM regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Networks',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 125,\n",
       "  'citationCount': 1895,\n",
       "  'publicationType': 'Not available'},\n",
       " '8f9d1456c05bb92cc2e79bbd0675255344ccbdf4': {'arxiv_id': None,\n",
       "  's2_paperId': '8f9d1456c05bb92cc2e79bbd0675255344ccbdf4',\n",
       "  'title': \"On Milman's inequality and random subspaces which escape through a mesh in ℝ n\",\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1988,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 53,\n",
       "  'citationCount': 369,\n",
       "  'publicationType': 'Not available'},\n",
       " 'eccf9e5cdda2012172958f5c49204f11933fb250': {'arxiv_id': '2301.12950v2',\n",
       "  's2_paperId': 'eccf9e5cdda2012172958f5c49204f11933fb250',\n",
       "  'title': 'Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs',\n",
       "  'abstract': 'Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pre-generated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite the encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program policies that describe out-of-distributionally complex behaviors and directly assign credits to programs that induce desired behaviors. The experimental results in the Karel domain show that our proposed framework outperforms baselines. The ablation studies confirm the limitations of LEAPS and justify our design choices.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 70,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 16,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '9ada8fa11b1cdece31f253acae50b62df8d5f823': {'arxiv_id': None,\n",
       "  's2_paperId': '9ada8fa11b1cdece31f253acae50b62df8d5f823',\n",
       "  'title': 'CodeT5+: Open Code Large Language Models for Code Understanding and Generation',\n",
       "  'abstract': None,\n",
       "  'venue': 'Conference on Empirical Methods in Natural Language Processing',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 71,\n",
       "  'influentialCitationCount': 55,\n",
       "  'citationCount': 436,\n",
       "  'publicationType': 'Not available'},\n",
       " '9e3c493fb09dcd61bb05e8c5659f23327b7b6340': {'arxiv_id': None,\n",
       "  's2_paperId': '9e3c493fb09dcd61bb05e8c5659f23327b7b6340',\n",
       "  'title': 'Teaching Large Language Models to Self-Debug',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 76,\n",
       "  'influentialCitationCount': 52,\n",
       "  'citationCount': 613,\n",
       "  'publicationType': 'Not available'},\n",
       " 'dc7d3adcfd3ea596128ec2e5c4f19b3adcdd4e7c': {'arxiv_id': None,\n",
       "  's2_paperId': 'dc7d3adcfd3ea596128ec2e5c4f19b3adcdd4e7c',\n",
       "  'title': 'Hierarchical Neural Program Synthesis',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 53,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 9,\n",
       "  'publicationType': 'Not available'},\n",
       " '91deaf9d324c8feafc189da0da03e60a60287bca': {'arxiv_id': None,\n",
       "  's2_paperId': '91deaf9d324c8feafc189da0da03e60a60287bca',\n",
       "  'title': 'Code as Policies: Language Model Programs for Embodied Control',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Robotics and Automation',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 62,\n",
       "  'influentialCitationCount': 69,\n",
       "  'citationCount': 848,\n",
       "  'publicationType': 'Not available'},\n",
       " '5cbe278b65a81602a864184bbca37de91448a5f5': {'arxiv_id': None,\n",
       "  's2_paperId': '5cbe278b65a81602a864184bbca37de91448a5f5',\n",
       "  'title': 'Competition-level code generation with AlphaCode',\n",
       "  'abstract': None,\n",
       "  'venue': 'Science',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 83,\n",
       "  'influentialCitationCount': 128,\n",
       "  'citationCount': 1254,\n",
       "  'publicationType': 'Not available'},\n",
       " '58adc31df514e1f5816e961856cf1a7fb4795971': {'arxiv_id': None,\n",
       "  's2_paperId': '58adc31df514e1f5816e961856cf1a7fb4795971',\n",
       "  'title': 'Outracing champion Gran Turismo drivers with deep reinforcement learning',\n",
       "  'abstract': 'Many potential applications of artificial intelligence involve making real-time decisions in physical systems while interacting with humans. Automobile racing represents an extreme example of these conditions; drivers must execute complex tactical manoeuvres to pass or block opponents while operating their vehicles at their traction limits1. Racing simulations, such as the PlayStation game Gran Turismo, faithfully reproduce the non-linear control challenges of real race cars while also encapsulating the complex multi-agent interactions. Here we describe how we trained agents for Gran Turismo that can compete with the world’s best e-sports drivers. We combine state-of-the-art, model-free, deep reinforcement learning algorithms with mixed-scenario training to learn an integrated control policy that combines exceptional speed with impressive tactics. In addition, we construct a reward function that enables the agent to be competitive while adhering to racing’s important, but under-specified, sportsmanship rules. We demonstrate the capabilities of our agent, Gran Turismo Sophy, by winning a head-to-head competition against four of the world’s best Gran Turismo drivers. By describing how we trained championship-level racers, we demonstrate the possibilities and challenges of using these techniques to control complex dynamical systems in domains where agents must respect imprecisely defined human norms. Using the game Gran Turismo, an agent was trained with a combination of deep reinforcement learning algorithms and specialized training scenarios, demonstrating success against championship-level human racers.',\n",
       "  'venue': 'Nature',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 31,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 346,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b62d63580b81a2cbb20c3c1593dd62d118e4cb07': {'arxiv_id': None,\n",
       "  's2_paperId': 'b62d63580b81a2cbb20c3c1593dd62d118e4cb07',\n",
       "  'title': 'Synchromesh: Reliable code generation from pre-trained language models',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 152,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd095f9ffcb5905bf0858ad1769d3d90e2e8737e2': {'arxiv_id': None,\n",
       "  's2_paperId': 'd095f9ffcb5905bf0858ad1769d3d90e2e8737e2',\n",
       "  'title': 'Jigsaw: Large Language Models meet Program Synthesis',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Software Engineering',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 45,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 189,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a30f912f8c5e2a2bfb06351d4578e1ba3fa37896': {'arxiv_id': None,\n",
       "  's2_paperId': 'a30f912f8c5e2a2bfb06351d4578e1ba3fa37896',\n",
       "  'title': 'CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation',\n",
       "  'abstract': 'Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.',\n",
       "  'venue': 'Conference on Empirical Methods in Natural Language Processing',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 281,\n",
       "  'citationCount': 1434,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a176b0de62840f7118006277d94bbc1547162a4d': {'arxiv_id': None,\n",
       "  's2_paperId': 'a176b0de62840f7118006277d94bbc1547162a4d',\n",
       "  'title': 'Learning to Synthesize Programs as Interpretable and Generalizable Policies',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 126,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 68,\n",
       "  'publicationType': 'Not available'},\n",
       " 'acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269': {'arxiv_id': None,\n",
       "  's2_paperId': 'acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269',\n",
       "  'title': 'Evaluating Large Language Models Trained on Code',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 127,\n",
       "  'influentialCitationCount': 917,\n",
       "  'citationCount': 4835,\n",
       "  'publicationType': 'Not available'},\n",
       " 'fd963ea665e7a6e49753652dc7efe5affba5feb0': {'arxiv_id': None,\n",
       "  's2_paperId': 'fd963ea665e7a6e49753652dc7efe5affba5feb0',\n",
       "  'title': 'DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM-SIGPLAN Symposium on Programming Language Design and Implementation',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 58,\n",
       "  'influentialCitationCount': 33,\n",
       "  'citationCount': 178,\n",
       "  'publicationType': 'Not available'},\n",
       " '400811ee31020a3f002551476dac25973e13035e': {'arxiv_id': None,\n",
       "  's2_paperId': '400811ee31020a3f002551476dac25973e13035e',\n",
       "  'title': 'How to train your robot with deep reinforcement learning: lessons we have learned',\n",
       "  'abstract': 'Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low-level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time, real-world robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn: as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world.',\n",
       "  'venue': 'Int. J. Robotics Res.',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 171,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 510,\n",
       "  'publicationType': 'Not available'},\n",
       " '905c9e24416b4a2654711837f376384fa0480802': {'arxiv_id': '2012.00377v2',\n",
       "  's2_paperId': '905c9e24416b4a2654711837f376384fa0480802',\n",
       "  'title': 'Latent Programmer: Discrete Latent Codes for Program Synthesis',\n",
       "  'abstract': 'In many sequence learning tasks, such as program synthesis and document summarization, a key problem is searching over a large space of possible output sequences. We propose to learn representations of the outputs that are specifically meant for search: rich enough to specify the desired output but compact enough to make search more efficient. Discrete latent codes are appealing for this purpose, as they naturally allow sophisticated combinatorial search strategies. The latent codes are learned using a self-supervised learning principle, in which first a discrete autoencoder is trained on the output sequences, and then the resulting latent codes are used as intermediate targets for the end-to-end sequence prediction task. Based on these insights, we introduce the \\\\emph{Latent Programmer}, a program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language. We evaluate the Latent Programmer on two domains: synthesis of string transformation programs, and generation of programs from natural language descriptions. We demonstrate that the discrete latent representation significantly improves synthesis accuracy.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 21,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'ef2bbcd928749978b4395460a96c9869833c9c89': {'arxiv_id': None,\n",
       "  's2_paperId': 'ef2bbcd928749978b4395460a96c9869833c9c89',\n",
       "  'title': 'DreamCoder: growing generalizable, interpretable knowledge with wake–sleep Bayesian program learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Philosophical Transactions of the Royal Society A',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 83,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 185,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e1b77eb03d753da13d1c6ce7cb45dd98e64c7fb6': {'arxiv_id': None,\n",
       "  's2_paperId': 'e1b77eb03d753da13d1c6ce7cb45dd98e64c7fb6',\n",
       "  'title': 'Explainable Reinforcement Learning: A Survey',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Cross-Domain Conference on Machine Learning and Knowledge Extraction',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 75,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 233,\n",
       "  'publicationType': 'Not available'},\n",
       " '745aa968dd81c3639b8d765ea63855cf0741ad92': {'arxiv_id': None,\n",
       "  's2_paperId': '745aa968dd81c3639b8d765ea63855cf0741ad92',\n",
       "  'title': 'Synthesizing Programmatic Policies that Inductively Generalize',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 46,\n",
       "  'publicationType': 'Not available'},\n",
       " '6f6d6da7b4c6219c55d0da09fd2b1f9809535d6d': {'arxiv_id': None,\n",
       "  's2_paperId': '6f6d6da7b4c6219c55d0da09fd2b1f9809535d6d',\n",
       "  'title': 'Program Guided Agent',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 94,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 57,\n",
       "  'publicationType': 'Not available'},\n",
       " '51a920c3d201eec57bcc2e97e0268304f53b5161': {'arxiv_id': None,\n",
       "  's2_paperId': '51a920c3d201eec57bcc2e97e0268304f53b5161',\n",
       "  'title': 'TreeGen: A Tree-Based Transformer Architecture for Code Generation',\n",
       "  'abstract': 'A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, TreeGen, for code generation. TreeGen uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel AST reader (encoder) to incorporate grammar rules and AST structures into the network. We evaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing benchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art approach by 4.5 percentage points on HearthStone, and achieved the best accuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%). We also conducted an ablation test to better understand each component of our model.',\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 172,\n",
       "  'publicationType': 'Not available'},\n",
       " '361c00b22e29d0816ca896513d2c165e26399821': {'arxiv_id': None,\n",
       "  's2_paperId': '361c00b22e29d0816ca896513d2c165e26399821',\n",
       "  'title': 'Grandmaster level in StarCraft II using multi-agent reinforcement learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Nature',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 159,\n",
       "  'citationCount': 3604,\n",
       "  'publicationType': 'Not available'},\n",
       " '27ffe05b33993618a42d7a6cfbb8dd1d14db8e99': {'arxiv_id': None,\n",
       "  's2_paperId': '27ffe05b33993618a42d7a6cfbb8dd1d14db8e99',\n",
       "  'title': 'Imitation-Projected Programmatic Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 60,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 82,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f68e35cdd571c3cf66f071384725135191ebac4d': {'arxiv_id': None,\n",
       "  's2_paperId': 'f68e35cdd571c3cf66f071384725135191ebac4d',\n",
       "  'title': 'Synthesizing Environment-Aware Activities via Activity Sketches',\n",
       "  'abstract': None,\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 38,\n",
       "  'publicationType': 'Not available'},\n",
       " '3a6447361b20c249f5306ae17dee43f645430e31': {'arxiv_id': None,\n",
       "  's2_paperId': '3a6447361b20c249f5306ae17dee43f645430e31',\n",
       "  'title': 'Neural Logic Machines',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 83,\n",
       "  'influentialCitationCount': 25,\n",
       "  'citationCount': 244,\n",
       "  'publicationType': 'Not available'},\n",
       " '374adc06806697f5fc6cede97fa4dfdbc5494a0d': {'arxiv_id': None,\n",
       "  's2_paperId': '374adc06806697f5fc6cede97fa4dfdbc5494a0d',\n",
       "  'title': 'Few-Shot Bayesian Imitation Learning with Logical Program Policies',\n",
       "  'abstract': 'Humans can learn many novel tasks from a very small number (1–5) of demonstrations, in stark contrast to the data requirements of nearly tabula rasa deep learning methods. We propose an expressive class of policies, a strong but general prior, and a learning algorithm that, together, can learn interesting policies from very few examples. We represent policies as logical combinations of programs drawn from a domain-specific language (DSL), define a prior over policies with a probabilistic grammar, and derive an approximate Bayesian inference algorithm to learn policies from demonstrations. In experiments, we study six strategy games played on a 2D grid with one shared DSL. After a few demonstrations of each game, the inferred policies generalize to new game instances that differ substantially from the demonstrations. Our policy learning is 20–1,000x more data efficient than convolutional and fully convolutional policy learning and many orders of magnitude more computationally efficient than vanilla program induction. We argue that the proposed method is an apt choice for tasks that have scarce training data and feature significant, structured variation between task instances.',\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 50,\n",
       "  'publicationType': 'Not available'},\n",
       " '7e2f5eca9465cf114043ed6c95ea59d9dbea45a1': {'arxiv_id': None,\n",
       "  's2_paperId': '7e2f5eca9465cf114043ed6c95ea59d9dbea45a1',\n",
       "  'title': 'Learning to Infer and Execute 3D Shape Programs',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 144,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ef2bc452812d6005ab0a66af6c3f97b6b0ba837e': {'arxiv_id': None,\n",
       "  's2_paperId': 'ef2bc452812d6005ab0a66af6c3f97b6b0ba837e',\n",
       "  'title': 'Quantifying Generalization in Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 71,\n",
       "  'citationCount': 648,\n",
       "  'publicationType': 'Not available'},\n",
       " '6c41bedc4637f3fd504c68baa3b3d8881e056ac1': {'arxiv_id': None,\n",
       "  's2_paperId': '6c41bedc4637f3fd504c68baa3b3d8881e056ac1',\n",
       "  'title': 'Execution-Guided Neural Program Synthesis',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 155,\n",
       "  'publicationType': 'Not available'},\n",
       " '58dcc24c63ed179d9a9b458d5f1284a7a297c5d6': {'arxiv_id': None,\n",
       "  's2_paperId': '58dcc24c63ed179d9a9b458d5f1284a7a297c5d6',\n",
       "  'title': 'Learning to Describe Scenes with Programs',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 61,\n",
       "  'publicationType': 'Not available'},\n",
       " '74e12851de2d542aa2aef7b8a39ef021a5802689': {'arxiv_id': None,\n",
       "  's2_paperId': '74e12851de2d542aa2aef7b8a39ef021a5802689',\n",
       "  'title': 'Composing Complex Skills by Learning Transition Policies',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 90,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c35f8f48f748c040c33bbe1f21c794e209341987': {'arxiv_id': None,\n",
       "  's2_paperId': 'c35f8f48f748c040c33bbe1f21c794e209341987',\n",
       "  'title': 'Neural Program Synthesis from Diverse Demonstration Videos',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 72,\n",
       "  'publicationType': 'Not available'},\n",
       " '9a8e6feb271bf1cce8b1393cf41e70692a7f6625': {'arxiv_id': '1805.08328v2',\n",
       "  's2_paperId': '9a8e6feb271bf1cce8b1393cf41e70692a7f6625',\n",
       "  'title': 'Verifiable Reinforcement Learning via Policy Extraction',\n",
       "  'abstract': 'While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 56,\n",
       "  'citationCount': 317,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '041d99f442dc22cf51118dc992095be9aa0972e0': {'arxiv_id': None,\n",
       "  's2_paperId': '041d99f442dc22cf51118dc992095be9aa0972e0',\n",
       "  'title': 'A Study on Overfitting in Deep Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 21,\n",
       "  'citationCount': 380,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c43bba87b4237a93d96b2a3e91da25d91fd0bb91': {'arxiv_id': None,\n",
       "  's2_paperId': 'c43bba87b4237a93d96b2a3e91da25d91fd0bb91',\n",
       "  'title': 'Programmatically Interpretable Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 29,\n",
       "  'citationCount': 340,\n",
       "  'publicationType': 'Not available'},\n",
       " 'dea6aeb514b1969ab879c793d46a0d2eceaa2cbf': {'arxiv_id': None,\n",
       "  's2_paperId': 'dea6aeb514b1969ab879c793d46a0d2eceaa2cbf',\n",
       "  'title': 'Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 40,\n",
       "  'citationCount': 214,\n",
       "  'publicationType': 'Not available'},\n",
       " '429b65937d4922578a81e1f0ef5aeab7361ae36b': {'arxiv_id': None,\n",
       "  's2_paperId': '429b65937d4922578a81e1f0ef5aeab7361ae36b',\n",
       "  'title': 'NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Language Resources and Evaluation',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 49,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 163,\n",
       "  'publicationType': 'Not available'},\n",
       " '811df72e210e20de99719539505da54762a11c6d': {'arxiv_id': '1801.01290v2',\n",
       "  's2_paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "  'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor',\n",
       "  'abstract': 'Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 1651,\n",
       "  'citationCount': 7948,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'c27db32efa8137cbf654902f8f728f338e55cd1c': {'arxiv_id': None,\n",
       "  's2_paperId': 'c27db32efa8137cbf654902f8f728f338e55cd1c',\n",
       "  'title': 'Mastering the game of Go without human knowledge',\n",
       "  'abstract': None,\n",
       "  'venue': 'Nature',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 68,\n",
       "  'influentialCitationCount': 384,\n",
       "  'citationCount': 9290,\n",
       "  'publicationType': 'Not available'},\n",
       " 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b': {'arxiv_id': None,\n",
       "  's2_paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "  'title': 'Proximal Policy Optimization Algorithms',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 3510,\n",
       "  'citationCount': 17706,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e60f6be86bc88fc2309b17d3df2885886448d16e': {'arxiv_id': None,\n",
       "  's2_paperId': 'e60f6be86bc88fc2309b17d3df2885886448d16e',\n",
       "  'title': 'Neural Scene De-rendering',\n",
       "  'abstract': 'We study the problem of holistic scene understanding. We would like to obtain a compact, expressive, and interpretable representation of scenes that encodes information such as the number of objects and their categories, poses, positions, etc. Such a representation would allow us to reason about and even reconstruct or manipulate elements of the scene. Previous works have used encoder-decoder based neural architectures to learn image representations, however, representations obtained in this way are typically uninterpretable, or only explain a single object in the scene. In this work, we propose a new approach to learn an interpretable distributed representation of scenes. Our approach employs a deterministic rendering function as the decoder, mapping a naturally structured and disentangled scene description, which we named scene XML, to an image. By doing so, the encoder is forced to perform the inverse of the rendering operation (a.k.a. de-rendering) to transform an input image to the structured scene XML that the decoder used to produce the image. We use a object proposal based encoder that is trained by minimizing both the supervised prediction and the unsupervised reconstruction errors. Experiments demonstrate that our approach works well on scene de-rendering with two different graphics engines, and our learned representation can be easily adapted for a wide range of applications like image editing, inpainting, visual analogy-making, and image captioning.',\n",
       "  'venue': 'Computer Vision and Pattern Recognition',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 148,\n",
       "  'publicationType': 'Not available'},\n",
       " '3ff0af64279929a952ee340e645256b7e0580f65': {'arxiv_id': None,\n",
       "  's2_paperId': '3ff0af64279929a952ee340e645256b7e0580f65',\n",
       "  'title': 'RobustFill: Neural Program Learning under Noisy I/O',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 41,\n",
       "  'citationCount': 382,\n",
       "  'publicationType': 'Not available'},\n",
       " '049c6e5736313374c6e594c34b9be89a3a09dced': {'arxiv_id': None,\n",
       "  's2_paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "  'title': 'FeUdal Networks for Hierarchical Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 65,\n",
       "  'citationCount': 883,\n",
       "  'publicationType': 'Not available'},\n",
       " '8a25c9403d8a0e2fb8ca362a1b26262afd57417f': {'arxiv_id': None,\n",
       "  's2_paperId': '8a25c9403d8a0e2fb8ca362a1b26262afd57417f',\n",
       "  'title': 'DeepCoder: Learning to Write Programs',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 76,\n",
       "  'citationCount': 560,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a90226c41b79f8b06007609f39f82757073641e2': {'arxiv_id': None,\n",
       "  's2_paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "  'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 582,\n",
       "  'citationCount': 4853,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e37b999f0c96d7136db07b0185b837d5decd599a': {'arxiv_id': None,\n",
       "  's2_paperId': 'e37b999f0c96d7136db07b0185b837d5decd599a',\n",
       "  'title': 'Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Robotics and Automation',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 30,\n",
       "  'citationCount': 1461,\n",
       "  'publicationType': 'Not available'},\n",
       " '15b26d8cb35d7e795c8832fe08794224ee1e9f84': {'arxiv_id': None,\n",
       "  's2_paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "  'title': 'The Option-Critic Architecture',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 46,\n",
       "  'influentialCitationCount': 105,\n",
       "  'citationCount': 1057,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd516daff247f7157fccde6649ace91d969cd1973': {'arxiv_id': None,\n",
       "  's2_paperId': 'd516daff247f7157fccde6649ace91d969cd1973',\n",
       "  'title': 'The mythos of model interpretability',\n",
       "  'abstract': None,\n",
       "  'venue': 'Queue',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 39,\n",
       "  'influentialCitationCount': 239,\n",
       "  'citationCount': 3601,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b59d91e0699d4e1896a15bae13fd180bdaf77ea5': {'arxiv_id': None,\n",
       "  's2_paperId': 'b59d91e0699d4e1896a15bae13fd180bdaf77ea5',\n",
       "  'title': 'Neural Programmer-Interpreters',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 37,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 405,\n",
       "  'publicationType': 'Not available'},\n",
       " '5259755f9c100e220ffaa7e08439c5d34be7757a': {'arxiv_id': None,\n",
       "  's2_paperId': '5259755f9c100e220ffaa7e08439c5d34be7757a',\n",
       "  'title': 'Reinforcement Learning Neural Turing Machines - Revised',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 158,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c1126fbffd6b8547a44c58b192b36b08b18299de': {'arxiv_id': None,\n",
       "  's2_paperId': 'c1126fbffd6b8547a44c58b192b36b08b18299de',\n",
       "  'title': 'Neural Turing Machines',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 229,\n",
       "  'citationCount': 2303,\n",
       "  'publicationType': 'Not available'},\n",
       " '1eb09fecd75eb27825dce4f964b97f4f5cc399d7': {'arxiv_id': None,\n",
       "  's2_paperId': '1eb09fecd75eb27825dce4f964b97f4f5cc399d7',\n",
       "  'title': 'On the Properties of Neural Machine Translation: Encoder–Decoder Approaches',\n",
       "  'abstract': 'Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.',\n",
       "  'venue': 'SSST@EMNLP',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 15,\n",
       "  'influentialCitationCount': 866,\n",
       "  'citationCount': 6659,\n",
       "  'publicationType': 'Not available'},\n",
       " 'abd1c342495432171beb7ca8fd9551ef13cbd0ff': {'arxiv_id': None,\n",
       "  's2_paperId': 'abd1c342495432171beb7ca8fd9551ef13cbd0ff',\n",
       "  'title': 'ImageNet classification with deep convolutional neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Communications of the ACM',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 71,\n",
       "  'influentialCitationCount': 12903,\n",
       "  'citationCount': 118038,\n",
       "  'publicationType': 'Not available'},\n",
       " '0514499e57c9612ccd6f4fbbcd06f7681c426a60': {'arxiv_id': None,\n",
       "  's2_paperId': '0514499e57c9612ccd6f4fbbcd06f7681c426a60',\n",
       "  'title': 'Learning Teleoreactive Logic Programs from Problem Solving',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Inductive Logic Programming',\n",
       "  'year': 2005,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 43,\n",
       "  'publicationType': 'Not available'},\n",
       " '13a8845ecf610f228e1424c3c2f639f79e027446': {'arxiv_id': None,\n",
       "  's2_paperId': '13a8845ecf610f228e1424c3c2f639f79e027446',\n",
       "  'title': 'DISTILL: Learning Domain-Specific Planners by Example',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 63,\n",
       "  'publicationType': 'Not available'},\n",
       " '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d': {'arxiv_id': None,\n",
       "  's2_paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "  'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Artificial Intelligence',\n",
       "  'year': 1999,\n",
       "  'referenceCount': 80,\n",
       "  'influentialCitationCount': 436,\n",
       "  'citationCount': 3729,\n",
       "  'publicationType': 'Not available'},\n",
       " '7d119debf7b5485a05a69296c984d79e444dc581': {'arxiv_id': None,\n",
       "  's2_paperId': '7d119debf7b5485a05a69296c984d79e444dc581',\n",
       "  'title': 'Optimization of computer simulation models with rare events',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1997,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 47,\n",
       "  'citationCount': 744,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e1409e30a1e3e3945c1f6798a181f9681e0fb03e': {'arxiv_id': None,\n",
       "  's2_paperId': 'e1409e30a1e3e3945c1f6798a181f9681e0fb03e',\n",
       "  'title': 'Karel the Robot: A Gentle Introduction to the Art of Programming',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1994,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 42,\n",
       "  'citationCount': 411,\n",
       "  'publicationType': 'Not available'},\n",
       " '8207fadf3f782151bce470141b59b30efa10a246': {'arxiv_id': None,\n",
       "  's2_paperId': '8207fadf3f782151bce470141b59b30efa10a246',\n",
       "  'title': 'Improving Generalization with Cross-State Behavior Matching in Deep Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Adaptive Agents and Multi-Agent Systems',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 1,\n",
       "  'publicationType': 'Not available'},\n",
       " '89ccced177a13aaf8b7172739e1bfaeada88bc1c': {'arxiv_id': None,\n",
       "  's2_paperId': '89ccced177a13aaf8b7172739e1bfaeada88bc1c',\n",
       "  'title': 'Generalizable Imitation Learning from Observation via Inferring Goal Proximity',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 44,\n",
       "  'publicationType': 'Not available'},\n",
       " '9ec1b4287654784652c36dbfbef9cc2a59561733': {'arxiv_id': None,\n",
       "  's2_paperId': '9ec1b4287654784652c36dbfbef9cc2a59561733',\n",
       "  'title': 'Discovering symbolic policies with deep reinforcement learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2021,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 86,\n",
       "  'publicationType': 'Not available'},\n",
       " 'bec0b38f0083bd351d25daf8b31e6e049c376f94': {'arxiv_id': None,\n",
       "  's2_paperId': 'bec0b38f0083bd351d25daf8b31e6e049c376f94',\n",
       "  'title': 'Improving Neural Program Synthesis with Inferred Execution Traces',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 54,\n",
       "  'publicationType': 'Not available'},\n",
       " '0a8149fb5aa8a5684e7d530c264451a5cb9250f5': {'arxiv_id': None,\n",
       "  's2_paperId': '0a8149fb5aa8a5684e7d530c264451a5cb9250f5',\n",
       "  'title': 'Recent Advances in Hierarchical Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Discrete event dynamic systems',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 96,\n",
       "  'influentialCitationCount': 96,\n",
       "  'citationCount': 1305,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8': {'arxiv_id': None,\n",
       "  's2_paperId': 'a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8',\n",
       "  'title': 'Recent Advances in Hierarchical Reinforcement Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Discrete event dynamic systems',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 24,\n",
       "  'citationCount': 564,\n",
       "  'publicationType': 'Not available'},\n",
       " '9b2541b8d8ca872149b4dabd2ccdc0cacc46ebf5': {'arxiv_id': None,\n",
       "  's2_paperId': '9b2541b8d8ca872149b4dabd2ccdc0cacc46ebf5',\n",
       "  'title': 'Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1982,\n",
       "  'referenceCount': 7,\n",
       "  'influentialCitationCount': 36,\n",
       "  'citationCount': 1004,\n",
       "  'publicationType': 'Not available'},\n",
       " '864ccab6d1ade1c110c7fd35cbc0879ceff4f916': {'arxiv_id': '2002.08404v2',\n",
       "  's2_paperId': '864ccab6d1ade1c110c7fd35cbc0879ceff4f916',\n",
       "  'title': 'Implicit Regularization of Random Feature Models',\n",
       "  'abstract': 'Random Feature (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with $P$ features, $N$ data points, and a ridge $\\\\lambda$, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an effective ridge $\\\\tilde{\\\\lambda}$. We show that $\\\\tilde{\\\\lambda} > \\\\lambda$ and $\\\\tilde{\\\\lambda} \\\\searrow \\\\lambda$ monotonically as $P$ grows, thus revealing the implicit regularization effect of finite RF sampling. We then compare the risk (i.e. test error) of the $\\\\tilde{\\\\lambda}$-KRR predictor with the average risk of the $\\\\lambda$-RF predictor and obtain a precise and explicit bound on their difference. Finally, we empirically find an extremely good agreement between the test errors of the average $\\\\lambda$-RF predictor and $\\\\tilde{\\\\lambda}$-KRR predictor.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 81,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '014e8de014d1a4aaeeb1b1ba8cdfeb04b5220fb2': {'arxiv_id': '2003.01054v2',\n",
       "  's2_paperId': '014e8de014d1a4aaeeb1b1ba8cdfeb04b5220fb2',\n",
       "  'title': 'Double Trouble in Double Descent : Bias and Variance(s) in the Lazy Regime',\n",
       "  'abstract': 'Deep neural networks can achieve remarkable generalization performances while interpolating the training data perfectly. Rather than the U-curve emblematic of the bias-variance trade-off, their test error often follows a \"double descent\" - a mark of the beneficial role of overparametrization. In this work, we develop a quantitative theory for this phenomenon in the so-called lazy learning regime of neural networks, by considering the problem of learning a high-dimensional function with random features regression. We obtain a precise asymptotic expression for the bias-variance decomposition of the test error, and show that the bias displays a phase transition at the interpolation threshold, beyond which it remains constant. We disentangle the variances stemming from the sampling of the dataset, from the additive noise corrupting the labels, and from the initialization of the weights. Following up on Geiger et al. 2019, we first show that the latter two contributions are the crux of the double descent: they lead to the overfitting peak at the interpolation threshold and to the decay of the test error upon overparametrization. We then quantify how they are suppressed by ensemble averaging the outputs of K independently initialized estimators. When K is sent to infinity, the test error remains constant beyond the interpolation threshold. We further compare the effects of overparametrizing, ensembling and regularizing. Finally, we present numerical experiments on classic deep learning setups to show that our results hold qualitatively in realistic lazy learning scenarios.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 60,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 150,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '1cb99a1c7e7d44ec0efbb1953e0622fb59a1a085': {'arxiv_id': None,\n",
       "  's2_paperId': '1cb99a1c7e7d44ec0efbb1953e0622fb59a1a085',\n",
       "  'title': 'Ridge Regression: Structure, Cross-Validation, and Sketching',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 48,\n",
       "  'publicationType': 'Not available'},\n",
       " '41c0be3adfd33cb6d0cb24c6fb1de109929276ca': {'arxiv_id': None,\n",
       "  's2_paperId': '41c0be3adfd33cb6d0cb24c6fb1de109929276ca',\n",
       "  'title': 'The Generalization Error of Random Features Regression: Precise Asymptotics and the Double Descent Curve',\n",
       "  'abstract': None,\n",
       "  'venue': 'Communications on Pure and Applied Mathematics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 72,\n",
       "  'influentialCitationCount': 68,\n",
       "  'citationCount': 619,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c563c0e06684f42bc5d76dfc7304581c11312393': {'arxiv_id': None,\n",
       "  's2_paperId': 'c563c0e06684f42bc5d76dfc7304581c11312393',\n",
       "  'title': 'Benign overfitting in linear regression',\n",
       "  'abstract': None,\n",
       "  'venue': 'Proceedings of the National Academy of Sciences of the United States of America',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 120,\n",
       "  'citationCount': 753,\n",
       "  'publicationType': 'Not available'},\n",
       " '96d745656fcaacc00db9abf9c35b341b27d40f51': {'arxiv_id': None,\n",
       "  's2_paperId': '96d745656fcaacc00db9abf9c35b341b27d40f51',\n",
       "  'title': 'Eigenvalue distribution of nonlinear models of random matrices',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 36,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 31,\n",
       "  'publicationType': 'Not available'},\n",
       " '844c256aa9344880da187e7a9438782a07df5776': {'arxiv_id': None,\n",
       "  's2_paperId': '844c256aa9344880da187e7a9438782a07df5776',\n",
       "  'title': 'Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization through Self-Concordance',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 51,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c1ba2fd31016c3c52f28890bfa2b14567c6fa2e9': {'arxiv_id': None,\n",
       "  's2_paperId': 'c1ba2fd31016c3c52f28890bfa2b14567c6fa2e9',\n",
       "  'title': 'Reconciling modern machine learning and the bias-variance trade-off',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 130,\n",
       "  'publicationType': 'Not available'},\n",
       " '463a1779585aab66a6ef3fdc1b19e8c5d34d8070': {'arxiv_id': None,\n",
       "  's2_paperId': '463a1779585aab66a6ef3fdc1b19e8c5d34d8070',\n",
       "  'title': 'A Modern Take on the Bias-Variance Tradeoff in Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 74,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 166,\n",
       "  'publicationType': 'Not available'},\n",
       " '7a84a692327534fd227fa1e07fcb3816b633c591': {'arxiv_id': None,\n",
       "  's2_paperId': '7a84a692327534fd227fa1e07fcb3816b633c591',\n",
       "  'title': 'Neural Tangent Kernel: Convergence and Generalization in Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 603,\n",
       "  'citationCount': 3048,\n",
       "  'publicationType': 'Not available'},\n",
       " '6104568e318f140d7adcf646412f182906db69b1': {'arxiv_id': None,\n",
       "  's2_paperId': '6104568e318f140d7adcf646412f182906db69b1',\n",
       "  'title': 'High-dimensional dynamics of generalization error in neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Networks',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 73,\n",
       "  'influentialCitationCount': 32,\n",
       "  'citationCount': 463,\n",
       "  'publicationType': 'Not available'},\n",
       " '5f7308ea1735cdeadc84c960a5769abb49792609': {'arxiv_id': None,\n",
       "  's2_paperId': '5f7308ea1735cdeadc84c960a5769abb49792609',\n",
       "  'title': 'A Random Matrix Approach to Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 43,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 160,\n",
       "  'publicationType': 'Not available'},\n",
       " '54ddb00fa691728944fd8becea90a373d21597cf': {'arxiv_id': None,\n",
       "  's2_paperId': '54ddb00fa691728944fd8becea90a373d21597cf',\n",
       "  'title': 'Understanding deep learning requires rethinking generalization',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 379,\n",
       "  'citationCount': 4558,\n",
       "  'publicationType': 'Not available'},\n",
       " '32e934094c4d17fe4d734b2e169ba5e3cd0ee05e': {'arxiv_id': None,\n",
       "  's2_paperId': '32e934094c4d17fe4d734b2e169ba5e3cd0ee05e',\n",
       "  'title': 'Orthogonal Random Features',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 37,\n",
       "  'citationCount': 215,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e92cb981bd2f38e5129d38cee0b24315551ab0f5': {'arxiv_id': None,\n",
       "  's2_paperId': 'e92cb981bd2f38e5129d38cee0b24315551ab0f5',\n",
       "  'title': 'Generalization Properties of Learning with Random Features',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 64,\n",
       "  'influentialCitationCount': 52,\n",
       "  'citationCount': 321,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e26aa4b56e43a6d1360413f205aa79aa2386fda3': {'arxiv_id': None,\n",
       "  's2_paperId': 'e26aa4b56e43a6d1360413f205aa79aa2386fda3',\n",
       "  'title': 'High-Dimensional Asymptotics of Prediction: Ridge Regression and Classification',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 70,\n",
       "  'influentialCitationCount': 54,\n",
       "  'citationCount': 284,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd1c1107c75603353edb9484b83f35bd01713cfb1': {'arxiv_id': None,\n",
       "  's2_paperId': 'd1c1107c75603353edb9484b83f35bd01713cfb1',\n",
       "  'title': 'Combinatorial theory of permutation-invariant random matrices II: cumulants, freeness and Levy processes',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 38,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 16,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a986ae204a6979abe1d175b5e188d3af51cf57c3': {'arxiv_id': None,\n",
       "  's2_paperId': 'a986ae204a6979abe1d175b5e188d3af51cf57c3',\n",
       "  'title': 'Optimal Rates for Random Fourier Features',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 45,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 127,\n",
       "  'publicationType': 'Not available'},\n",
       " '4b675d8f63888d7d6d7d77a0834efa5eaded64c5': {'arxiv_id': None,\n",
       "  's2_paperId': '4b675d8f63888d7d6d7d77a0834efa5eaded64c5',\n",
       "  'title': 'In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 43,\n",
       "  'citationCount': 648,\n",
       "  'publicationType': 'Not available'},\n",
       " '5e6554d3e1cced2b1e4c89e260af5cdcda77843d': {'arxiv_id': None,\n",
       "  's2_paperId': '5e6554d3e1cced2b1e4c89e260af5cdcda77843d',\n",
       "  'title': 'Free Probability and Random Matrices',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 219,\n",
       "  'influentialCitationCount': 17,\n",
       "  'citationCount': 180,\n",
       "  'publicationType': 'Not available'},\n",
       " '8f77e16f2834eb23307b0436f5422a9752339f0a': {'arxiv_id': None,\n",
       "  's2_paperId': '8f77e16f2834eb23307b0436f5422a9752339f0a',\n",
       "  'title': 'Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 22,\n",
       "  'influentialCitationCount': 27,\n",
       "  'citationCount': 345,\n",
       "  'publicationType': 'Not available'},\n",
       " '47aa6d7381cc9993da60e4547b01f415a04f3cf2': {'arxiv_id': None,\n",
       "  's2_paperId': '47aa6d7381cc9993da60e4547b01f415a04f3cf2',\n",
       "  'title': 'Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 96,\n",
       "  'citationCount': 785,\n",
       "  'publicationType': 'Not available'},\n",
       " '7a59fde27461a3ef4a21a249cc403d0d96e4a0d7': {'arxiv_id': None,\n",
       "  's2_paperId': '7a59fde27461a3ef4a21a249cc403d0d96e4a0d7',\n",
       "  'title': 'Random Features for Large-Scale Kernel Machines',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 685,\n",
       "  'citationCount': 4161,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a62167109fd2ab63e25b4ce2375131a331bea3b0': {'arxiv_id': None,\n",
       "  's2_paperId': 'a62167109fd2ab63e25b4ce2375131a331bea3b0',\n",
       "  'title': 'Optimal Rates for the Regularized Least-Squares Algorithm',\n",
       "  'abstract': None,\n",
       "  'venue': 'Foundations of Computational Mathematics',\n",
       "  'year': 2007,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 158,\n",
       "  'citationCount': 826,\n",
       "  'publicationType': 'Not available'},\n",
       " '668b1277fbece28c4841eeab1c97e4ebd0079700': {'arxiv_id': None,\n",
       "  's2_paperId': '668b1277fbece28c4841eeab1c97e4ebd0079700',\n",
       "  'title': 'Pattern Recognition and Machine Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'J. Electronic Imaging',\n",
       "  'year': 2006,\n",
       "  'referenceCount': 357,\n",
       "  'influentialCitationCount': 4311,\n",
       "  'citationCount': 37029,\n",
       "  'publicationType': 'Not available'},\n",
       " '3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9': {'arxiv_id': None,\n",
       "  's2_paperId': '3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9',\n",
       "  'title': 'Nonlinear Component Analysis as a Kernel Eigenvalue Problem',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Computation',\n",
       "  'year': 1998,\n",
       "  'referenceCount': 25,\n",
       "  'influentialCitationCount': 1080,\n",
       "  'citationCount': 8543,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ee54b5db5504ebf9a5b6e6ca75b718baa34f0186': {'arxiv_id': None,\n",
       "  's2_paperId': 'ee54b5db5504ebf9a5b6e6ca75b718baa34f0186',\n",
       "  'title': 'Strong convergence of the empirical distribution of eigenvalues of large dimensional random matrices',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1995,\n",
       "  'referenceCount': 4,\n",
       "  'influentialCitationCount': 84,\n",
       "  'citationCount': 546,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a34e35dbbc6911fa7b94894dffdc0076a261b6f0': {'arxiv_id': None,\n",
       "  's2_paperId': 'a34e35dbbc6911fa7b94894dffdc0076a261b6f0',\n",
       "  'title': 'Neural Networks and the Bias/Variance Dilemma',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Computation',\n",
       "  'year': 1992,\n",
       "  'referenceCount': 110,\n",
       "  'influentialCitationCount': 148,\n",
       "  'citationCount': 3863,\n",
       "  'publicationType': 'Not available'},\n",
       " '1fac1eac46e8a9d71d6fd616048d3b80a3879087': {'arxiv_id': None,\n",
       "  's2_paperId': '1fac1eac46e8a9d71d6fd616048d3b80a3879087',\n",
       "  'title': 'Multivariate statistics : a vector space approach',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1985,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 78,\n",
       "  'citationCount': 646,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a3547616a1ebebe7578ed9057e4fcb1c99289ac6': {'arxiv_id': None,\n",
       "  's2_paperId': 'a3547616a1ebebe7578ed9057e4fcb1c99289ac6',\n",
       "  'title': 'A jamming transition from under- to over-parametrization affects loss landscape and generalization',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 47,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 55,\n",
       "  'publicationType': 'Not available'},\n",
       " '93bf87b631103deef7aa9448402281abc96d6b12': {'arxiv_id': None,\n",
       "  's2_paperId': '93bf87b631103deef7aa9448402281abc96d6b12',\n",
       "  'title': 'Free probability and random matrices',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 11,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 267,\n",
       "  'publicationType': 'Not available'},\n",
       " '2cbb74b146b9263d8ca05952087ba3d3d85581a8': {'arxiv_id': None,\n",
       "  's2_paperId': '2cbb74b146b9263d8ca05952087ba3d3d85581a8',\n",
       "  'title': 'LARGE SAMPLE COVARIANCE MATRICES WITHOUT INDEPENDENCE STRUCTURES IN COLUMNS',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 18,\n",
       "  'influentialCitationCount': 38,\n",
       "  'citationCount': 168,\n",
       "  'publicationType': 'Not available'},\n",
       " '72b3434f2139cd050141540764a562a143026ef4': {'arxiv_id': None,\n",
       "  's2_paperId': '72b3434f2139cd050141540764a562a143026ef4',\n",
       "  'title': 'Effective Dimension and Generalization of Kernel Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2002,\n",
       "  'referenceCount': 5,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 48,\n",
       "  'publicationType': 'Not available'},\n",
       " '515a0b56689e2366310520684ea552f07bb1d5d4': {'arxiv_id': '2006.09668v2',\n",
       "  's2_paperId': '515a0b56689e2366310520684ea552f07bb1d5d4',\n",
       "  'title': 'Analysis and Design of Thompson Sampling for Stochastic Partial Monitoring',\n",
       "  'abstract': 'We investigate finite stochastic partial monitoring, which is a general model for sequential learning with limited feedback. While Thompson sampling is one of the most promising algorithms on a variety of online decision-making problems, its properties for stochastic partial monitoring have not been theoretically investigated, and the existing algorithm relies on a heuristic approximation of the posterior distribution. To mitigate these problems, we present a novel Thompson-sampling-based algorithm, which enables us to exactly sample the target parameter from the posterior distribution. Besides, we prove that the new algorithm achieves the logarithmic problem-dependent expected pseudo-regret $\\\\mathrm{O}(\\\\log T)$ for a linearized variant of the problem with local observability. This result is the first regret bound of Thompson sampling for partial monitoring, which also becomes the first logarithmic regret bound of Thompson sampling for linear bandits.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 8,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '9cdc3e333e583df3c6059ba41efcffa7fcf2a28e': {'arxiv_id': None,\n",
       "  's2_paperId': '9cdc3e333e583df3c6059ba41efcffa7fcf2a28e',\n",
       "  'title': 'Information Directed Sampling for Linear Partial Monitoring',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2020,\n",
       "  'referenceCount': 52,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 46,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd262dd0a45ccea00059e0120341f0503e42fe89a': {'arxiv_id': None,\n",
       "  's2_paperId': 'd262dd0a45ccea00059e0120341f0503e42fe89a',\n",
       "  'title': 'Connections Between Mirror Descent, Thompson Sampling and the Information Ratio',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 34,\n",
       "  'publicationType': 'Not available'},\n",
       " '129762bfc35c85362fc65dad10e2e3c6dc30deba': {'arxiv_id': None,\n",
       "  's2_paperId': '129762bfc35c85362fc65dad10e2e3c6dc30deba',\n",
       "  'title': 'An Information-Theoretic Approach to Minimax Regret in Partial Monitoring',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 40,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 70,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c0cba3fbc696102ba38c9d754a1f22d9e9eb6366': {'arxiv_id': None,\n",
       "  's2_paperId': 'c0cba3fbc696102ba38c9d754a1f22d9e9eb6366',\n",
       "  'title': 'A Tutorial on Thompson Sampling',\n",
       "  'abstract': None,\n",
       "  'venue': 'Found. Trends Mach. Learn.',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 89,\n",
       "  'influentialCitationCount': 50,\n",
       "  'citationCount': 938,\n",
       "  'publicationType': 'Not available'},\n",
       " '6d18c5fbd2eb1808b006f5d3433a7735e88f49d2': {'arxiv_id': None,\n",
       "  's2_paperId': '6d18c5fbd2eb1808b006f5d3433a7735e88f49d2',\n",
       "  'title': 'Ensemble Sampling',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 9,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 117,\n",
       "  'publicationType': 'Not available'},\n",
       " '98e514e676d6e251b74127e5bfe1075d4f7b0d4b': {'arxiv_id': '1509.09011v1',\n",
       "  's2_paperId': '98e514e676d6e251b74127e5bfe1075d4f7b0d4b',\n",
       "  'title': 'Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring',\n",
       "  'abstract': 'Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 22,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '848ac82224988c824e4d212a994ebd58acf46de7': {'arxiv_id': None,\n",
       "  's2_paperId': '848ac82224988c824e4d212a994ebd58acf46de7',\n",
       "  'title': 'Efficient Partial Monitoring with Prior Information',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 16,\n",
       "  'publicationType': 'Not available'},\n",
       " '8fb0aa9a04806dfe89ea1b1db5410b2fcff0c1d7': {'arxiv_id': None,\n",
       "  's2_paperId': '8fb0aa9a04806dfe89ea1b1db5410b2fcff0c1d7',\n",
       "  'title': 'Sampling from a multivariate Gaussian distribution truncated on a simplex: A review',\n",
       "  'abstract': None,\n",
       "  'venue': 'Symposium on Software Performance',\n",
       "  'year': 2014,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 27,\n",
       "  'publicationType': 'Not available'},\n",
       " '53b967ac34b5a422f81fb6947da0ba148ba90221': {'arxiv_id': None,\n",
       "  's2_paperId': '53b967ac34b5a422f81fb6947da0ba148ba90221',\n",
       "  'title': \"lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits\",\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 61,\n",
       "  'citationCount': 405,\n",
       "  'publicationType': 'Not available'},\n",
       " '18e1b0bad513365d85baf1502471efd956c50368': {'arxiv_id': None,\n",
       "  's2_paperId': '18e1b0bad513365d85baf1502471efd956c50368',\n",
       "  'title': 'Optimality of Thompson Sampling for Gaussian Bandits Depends on Priors',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 63,\n",
       "  'publicationType': 'Not available'},\n",
       " '47f77ba1b0bd629de3b8c6c1b7c95849228afd63': {'arxiv_id': '1307.3400v1',\n",
       "  's2_paperId': '47f77ba1b0bd629de3b8c6c1b7c95849228afd63',\n",
       "  'title': 'Thompson Sampling for 1-Dimensional Exponential Family Bandits',\n",
       "  'abstract': 'Thompson Sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for 1-dimensional exponential family bandits. Our proof builds on previous work, but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (through the Jeffreys prior) available in an exponential family. This allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2013,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 14,\n",
       "  'citationCount': 151,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'a81b723308fbfd494ca68a73a89a8b10cb3d1f53': {'arxiv_id': None,\n",
       "  's2_paperId': 'a81b723308fbfd494ca68a73a89a8b10cb3d1f53',\n",
       "  'title': 'Further Optimal Regret Bounds for Thompson Sampling',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 29,\n",
       "  'influentialCitationCount': 89,\n",
       "  'citationCount': 440,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f26f1a3c034b96514fc092dee99acacedd9c380b': {'arxiv_id': '1209.3352v4',\n",
       "  's2_paperId': 'f26f1a3c034b96514fc092dee99acacedd9c380b',\n",
       "  'title': 'Thompson Sampling for Contextual Bandits with Linear Payoffs',\n",
       "  'abstract': 'Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of O(d2/e√T1+e) in time T for any 0 < e < 1, where d is the dimension of each context vector and e is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of Ω(d√T) for this problem. This essentially solves a COLT open problem of Chapelle and Li [COLT 2012].',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 164,\n",
       "  'citationCount': 978,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'c6c0529cd038e33a34ef09ce22db3ba2074119c3': {'arxiv_id': None,\n",
       "  's2_paperId': 'c6c0529cd038e33a34ef09ce22db3ba2074119c3',\n",
       "  'title': 'An adaptive algorithm for finite stochastic partial monitoring',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 10,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 41,\n",
       "  'publicationType': 'Not available'},\n",
       " '2e96fc720aabb0039f1e9819f4b78259924965ae': {'arxiv_id': None,\n",
       "  's2_paperId': '2e96fc720aabb0039f1e9819f4b78259924965ae',\n",
       "  'title': 'Thompson Sampling: An Asymptotically Optimal Finite-Time Analysis',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Algorithmic Learning Theory',\n",
       "  'year': 2012,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 56,\n",
       "  'citationCount': 582,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd726386f37b1f16dbd7c56ab2353f832a4e678ec': {'arxiv_id': None,\n",
       "  's2_paperId': 'd726386f37b1f16dbd7c56ab2353f832a4e678ec',\n",
       "  'title': 'Minimax Regret of Finite Partial-Monitoring Games in Stochastic Environments',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 19,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 49,\n",
       "  'publicationType': 'Not available'},\n",
       " '6bea71fa6deb19c67e9586428f8f240e789fb3df': {'arxiv_id': None,\n",
       "  's2_paperId': '6bea71fa6deb19c67e9586428f8f240e789fb3df',\n",
       "  'title': 'Improved Algorithms for Linear Stochastic Bandits',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 582,\n",
       "  'citationCount': 1732,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ab867c140d2947511979c87e7ae580d9d3f0aeab': {'arxiv_id': None,\n",
       "  's2_paperId': 'ab867c140d2947511979c87e7ae580d9d3f0aeab',\n",
       "  'title': 'An Empirical Evaluation of Thompson Sampling',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 23,\n",
       "  'influentialCitationCount': 144,\n",
       "  'citationCount': 1456,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd52e789bbb4563cdb45b93bb37d64443605d5ae8': {'arxiv_id': None,\n",
       "  's2_paperId': 'd52e789bbb4563cdb45b93bb37d64443605d5ae8',\n",
       "  'title': 'Toward a classification of finite partial-monitoring games',\n",
       "  'abstract': None,\n",
       "  'venue': 'Theoretical Computer Science',\n",
       "  'year': 2010,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 97,\n",
       "  'publicationType': 'Not available'},\n",
       " '2f2dec50ec7fca07eb2872affe0eb970f5ed39ed': {'arxiv_id': None,\n",
       "  's2_paperId': '2f2dec50ec7fca07eb2872affe0eb970f5ed39ed',\n",
       "  'title': 'Regret Minimization Under Partial Monitoring',\n",
       "  'abstract': None,\n",
       "  'venue': \"2006 IEEE Information Theory Workshop - ITW '06 Punta del Este\",\n",
       "  'year': 2006,\n",
       "  'referenceCount': 57,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 171,\n",
       "  'publicationType': 'Not available'},\n",
       " '617b9447652595dfafba8b1698ff9e184ff6bccb': {'arxiv_id': None,\n",
       "  's2_paperId': '617b9447652595dfafba8b1698ff9e184ff6bccb',\n",
       "  'title': 'Minimizing regret with label efficient prediction',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on Information Theory',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 121,\n",
       "  'publicationType': 'Not available'},\n",
       " 'a35100e8954261d4802ef70b690ff7724653806f': {'arxiv_id': None,\n",
       "  's2_paperId': 'a35100e8954261d4802ef70b690ff7724653806f',\n",
       "  'title': 'The value of knowing a demand curve: bounds on regret for online posted-price auctions',\n",
       "  'abstract': None,\n",
       "  'venue': '44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 61,\n",
       "  'citationCount': 401,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e1f153c6df86d1ca8ecb9561daddfe7a54f901e7': {'arxiv_id': None,\n",
       "  's2_paperId': 'e1f153c6df86d1ca8ecb9561daddfe7a54f901e7',\n",
       "  'title': 'Online Convex Programming and Generalized Infinitesimal Gradient Ascent',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 446,\n",
       "  'citationCount': 2517,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b149faaa2bdac4bc9ec931fbb315cf4be409bb0c': {'arxiv_id': None,\n",
       "  's2_paperId': 'b149faaa2bdac4bc9ec931fbb315cf4be409bb0c',\n",
       "  'title': 'Online learning in online auctions',\n",
       "  'abstract': None,\n",
       "  'venue': 'ACM-SIAM Symposium on Discrete Algorithms',\n",
       "  'year': 2003,\n",
       "  'referenceCount': 13,\n",
       "  'influentialCitationCount': 15,\n",
       "  'citationCount': 203,\n",
       "  'publicationType': 'Not available'},\n",
       " 'eb3289c8315d3aa232587455107d9dea828cd1be': {'arxiv_id': None,\n",
       "  's2_paperId': 'eb3289c8315d3aa232587455107d9dea828cd1be',\n",
       "  'title': 'Discrete Prediction Games with Arbitrary Feedback and Loss',\n",
       "  'abstract': None,\n",
       "  'venue': 'COLT/EuroCOLT',\n",
       "  'year': 2001,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 13,\n",
       "  'citationCount': 77,\n",
       "  'publicationType': 'Not available'},\n",
       " '65ab2fd2014460cae72da80ea9dee2028c712f32': {'arxiv_id': None,\n",
       "  's2_paperId': '65ab2fd2014460cae72da80ea9dee2028c712f32',\n",
       "  'title': 'Minimizing Regret : The General Case',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1999,\n",
       "  'referenceCount': 17,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 127,\n",
       "  'publicationType': 'Not available'},\n",
       " '4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa': {'arxiv_id': None,\n",
       "  's2_paperId': '4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa',\n",
       "  'title': 'Asymptotically efficient adaptive allocation rules',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1985,\n",
       "  'referenceCount': 3,\n",
       "  'influentialCitationCount': 181,\n",
       "  'citationCount': 1759,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ee2cd1d17f833d3c157a1016a778c7c22af555a2': {'arxiv_id': None,\n",
       "  's2_paperId': 'ee2cd1d17f833d3c157a1016a778c7c22af555a2',\n",
       "  'title': 'ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 1933,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 272,\n",
       "  'citationCount': 3249,\n",
       "  'publicationType': 'Not available'},\n",
       " '551e19e5113cdff60a3c545d684fc4b9eb9a7306': {'arxiv_id': None,\n",
       "  's2_paperId': '551e19e5113cdff60a3c545d684fc4b9eb9a7306',\n",
       "  'title': 'Stochastic Linear Optimization under Bandit Feedback',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annual Conference Computational Learning Theory',\n",
       "  'year': 2008,\n",
       "  'referenceCount': 16,\n",
       "  'influentialCitationCount': 127,\n",
       "  'citationCount': 877,\n",
       "  'publicationType': 'Not available'},\n",
       " '997f48403260d37af002180f252be0e64b0ea4e2': {'arxiv_id': None,\n",
       "  's2_paperId': '997f48403260d37af002180f252be0e64b0ea4e2',\n",
       "  'title': 'Generalized Accept-Reject sampling schemes',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2004,\n",
       "  'referenceCount': 0,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 144,\n",
       "  'publicationType': 'Not available'},\n",
       " '69238d0a3761e2765e0cb9ed17f1144efbcde14a': {'arxiv_id': None,\n",
       "  's2_paperId': '69238d0a3761e2765e0cb9ed17f1144efbcde14a',\n",
       "  'title': 'Minimizing regret with label-eﬀicient prediction',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': None,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 8,\n",
       "  'publicationType': 'Not available'},\n",
       " '41c3bf144244859a489206ef0b1cac4ad278be48': {'arxiv_id': '1912.12970v5',\n",
       "  's2_paperId': '41c3bf144244859a489206ef0b1cac4ad278be48',\n",
       "  'title': 'Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework',\n",
       "  'abstract': \"This paper develops a Pontryagin differentiable programming (PDP) methodology, which establishes a unified framework to solve a broad class of learning and control tasks. The PDP methodology distinguishes from existing methods by two novel techniques: first, we differentiate the Pontryagin's Maximum Principle, and this allows us to obtain analytical gradient of a trajectory with respect to a tunable parameter of a system, thus enabling end-to-end learning of system dynamics, policy, or/and control objective function; and second, we propose an auxiliary control system in backward pass of the PDP framework, and show that the output of the auxiliary control system is exactly the gradient of the system trajectory with respect to the parameter, which can be iteratively obtained using control tools. We investigate three learning modes of the PDP: inverse reinforcement learning, system identification, and control/planning, respectively. We demonstrate the capability of the PDP in each learning mode using various high-dimensional systems, including multilink robot arm, 6-DoF maneuvering UAV, and 6-DoF rocket powered landing.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 105,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 77,\n",
       "  'publicationType': 'JournalArticle'},\n",
       " '2195f61f4c40d724cfeb47498bbb26f176c87390': {'arxiv_id': None,\n",
       "  's2_paperId': '2195f61f4c40d724cfeb47498bbb26f176c87390',\n",
       "  'title': 'Variational Integrator Networks for Physically Structured Embeddings',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 49,\n",
       "  'publicationType': 'Not available'},\n",
       " '329fb2f19fd1cecbb0a4ff317a4dbd833e43f903': {'arxiv_id': None,\n",
       "  's2_paperId': '329fb2f19fd1cecbb0a4ff317a4dbd833e43f903',\n",
       "  'title': 'Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 30,\n",
       "  'influentialCitationCount': 21,\n",
       "  'citationCount': 265,\n",
       "  'publicationType': 'Not available'},\n",
       " '4fc08dca4cfbff4200b3330f09fb9e385b791ca2': {'arxiv_id': None,\n",
       "  's2_paperId': '4fc08dca4cfbff4200b3330f09fb9e385b791ca2',\n",
       "  'title': 'Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 114,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 71,\n",
       "  'publicationType': 'Not available'},\n",
       " '8a21e9fc33ec4147118a97a4c0ec238c51b3cbd8': {'arxiv_id': None,\n",
       "  's2_paperId': '8a21e9fc33ec4147118a97a4c0ec238c51b3cbd8',\n",
       "  'title': 'Inverse Optimal Control for Multiphase Cost Functions',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on robotics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 3,\n",
       "  'citationCount': 46,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e129e344083b307e005c5342ba49524d9981a420': {'arxiv_id': None,\n",
       "  's2_paperId': 'e129e344083b307e005c5342ba49524d9981a420',\n",
       "  'title': 'Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 35,\n",
       "  'citationCount': 367,\n",
       "  'publicationType': 'Not available'},\n",
       " '9001698e033524864d4d45f051a5ba362d4afd9e': {'arxiv_id': None,\n",
       "  's2_paperId': '9001698e033524864d4d45f051a5ba362d4afd9e',\n",
       "  'title': 'When to Trust Your Model: Model-Based Policy Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 45,\n",
       "  'influentialCitationCount': 190,\n",
       "  'citationCount': 913,\n",
       "  'publicationType': 'Not available'},\n",
       " '3f5ffeb75522d3aedb0d7dcd274c9fb7e980a2a6': {'arxiv_id': None,\n",
       "  's2_paperId': '3f5ffeb75522d3aedb0d7dcd274c9fb7e980a2a6',\n",
       "  'title': 'Active Learning of Dynamics for Data-Driven Control Using Koopman Operators',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE Transactions on robotics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 66,\n",
       "  'influentialCitationCount': 4,\n",
       "  'citationCount': 160,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd3850595d3ae7c73e9488054c9b437f75511b569': {'arxiv_id': '1905.12149v1',\n",
       "  's2_paperId': 'd3850595d3ae7c73e9488054c9b437f75511b569',\n",
       "  'title': 'SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver',\n",
       "  'abstract': 'Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a \"visual Sudok\" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 35,\n",
       "  'citationCount': 256,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '0eeec891873959294ae69f79e39d51b83e289ac6': {'arxiv_id': None,\n",
       "  's2_paperId': '0eeec891873959294ae69f79e39d51b83e289ac6',\n",
       "  'title': 'Selection dynamics for deep neural networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Differential Equations',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 65,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 23,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f8f82100ae73bcc0433bb724405993cd11da57fc': {'arxiv_id': None,\n",
       "  's2_paperId': 'f8f82100ae73bcc0433bb724405993cd11da57fc',\n",
       "  'title': 'You Only Propagate Once: Painless Adversarial Training Using Maximal Principle',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 12,\n",
       "  'citationCount': 65,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e074b55372d3a30ee0995f3b478e99649e839319': {'arxiv_id': None,\n",
       "  's2_paperId': 'e074b55372d3a30ee0995f3b478e99649e839319',\n",
       "  'title': 'Deep learning as optimal control problems: models and numerical methods',\n",
       "  'abstract': 'We consider recent work of Haber and Ruthotto 2017 and Chang et al. 2018, where deep learning neural networks have been interpreted as discretisations of an optimal control problem subject to an ordinary differential equation constraint. We review the first order conditions for optimality, and the conditions ensuring optimality after discretisation. This leads to a class of algorithms for solving the discrete optimal control problem which guarantee that the corresponding discrete necessary conditions for optimality are fulfilled. The differential equation setting lends itself to learning additional parameters such as the time discretisation. We explore this extension alongside natural constraints (e.g. time steps lie in a simplex). We compare these deep learning algorithms numerically in terms of induced flow and generalisation ability.',\n",
       "  'venue': 'Journal of Computational Dynamics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 50,\n",
       "  'influentialCitationCount': 9,\n",
       "  'citationCount': 80,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd86084808994ac54ef4840ae65295f3c0ec4decd': {'arxiv_id': None,\n",
       "  's2_paperId': 'd86084808994ac54ef4840ae65295f3c0ec4decd',\n",
       "  'title': 'Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of Computational Physics',\n",
       "  'year': 2019,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 736,\n",
       "  'citationCount': 10120,\n",
       "  'publicationType': 'Not available'},\n",
       " '68df76350dffba2e5f5f965df57c3747c66bb4d0': {'arxiv_id': None,\n",
       "  's2_paperId': '68df76350dffba2e5f5f965df57c3747c66bb4d0',\n",
       "  'title': 'Differentiable MPC for End-to-end Planning and Control',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 58,\n",
       "  'influentialCitationCount': 26,\n",
       "  'citationCount': 361,\n",
       "  'publicationType': 'Not available'},\n",
       " '6a9013a8cdd84e423223f76a903028011c84c4ab': {'arxiv_id': None,\n",
       "  's2_paperId': '6a9013a8cdd84e423223f76a903028011c84c4ab',\n",
       "  'title': 'Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 72,\n",
       "  'influentialCitationCount': 18,\n",
       "  'citationCount': 219,\n",
       "  'publicationType': 'Not available'},\n",
       " '015a9bec58a53f64c19a59fedea51c68d6a741f1': {'arxiv_id': None,\n",
       "  's2_paperId': '015a9bec58a53f64c19a59fedea51c68d6a741f1',\n",
       "  'title': 'Melding the Data-Decisions Pipeline: Decision-Focused Learning for Combinatorial Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'AAAI Conference on Artificial Intelligence',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 49,\n",
       "  'citationCount': 287,\n",
       "  'publicationType': 'Not available'},\n",
       " '71b03c8885b425c02e0b722c1133fcccb70ba413': {'arxiv_id': None,\n",
       "  's2_paperId': '71b03c8885b425c02e0b722c1133fcccb70ba413',\n",
       "  'title': 'CasADi: a software framework for nonlinear optimization and optimal control',\n",
       "  'abstract': None,\n",
       "  'venue': 'Mathematical Programming Computation',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 131,\n",
       "  'influentialCitationCount': 189,\n",
       "  'citationCount': 2934,\n",
       "  'publicationType': 'Not available'},\n",
       " '84e1dd0722c752321a4c4a7778246ec285404614': {'arxiv_id': None,\n",
       "  's2_paperId': '84e1dd0722c752321a4c4a7778246ec285404614',\n",
       "  'title': 'A mean-field optimal control formulation of deep learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Research in the Mathematical Sciences',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 71,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 179,\n",
       "  'publicationType': 'Not available'},\n",
       " '449310e3538b08b43227d660227dfd2875c3c3c1': {'arxiv_id': None,\n",
       "  's2_paperId': '449310e3538b08b43227d660227dfd2875c3c3c1',\n",
       "  'title': 'Neural Ordinary Differential Equations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 64,\n",
       "  'influentialCitationCount': 1015,\n",
       "  'citationCount': 4836,\n",
       "  'publicationType': 'Not available'},\n",
       " 'b53a4e3bcc7bca42009f1752437267976c968bae': {'arxiv_id': None,\n",
       "  's2_paperId': 'b53a4e3bcc7bca42009f1752437267976c968bae',\n",
       "  'title': 'Universal Planning Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 69,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 145,\n",
       "  'publicationType': 'Not available'},\n",
       " 'c841da0614ee52a7f7e13d187fac86c64a623525': {'arxiv_id': None,\n",
       "  's2_paperId': 'c841da0614ee52a7f7e13d187fac86c64a623525',\n",
       "  'title': 'Inverse optimal control from incomplete trajectory observations',\n",
       "  'abstract': None,\n",
       "  'venue': 'Int. J. Robotics Res.',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 35,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 48,\n",
       "  'publicationType': 'Not available'},\n",
       " '1aae9d5816e9954b5ec372c8e9916c51ed64f4f7': {'arxiv_id': None,\n",
       "  's2_paperId': '1aae9d5816e9954b5ec372c8e9916c51ed64f4f7',\n",
       "  'title': 'Inverse Optimal Control with Incomplete Observations',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 28,\n",
       "  'influentialCitationCount': 2,\n",
       "  'citationCount': 17,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e9d852ba2853a3f8b4928b322d32708c32822e06': {'arxiv_id': None,\n",
       "  's2_paperId': 'e9d852ba2853a3f8b4928b322d32708c32822e06',\n",
       "  'title': 'An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 59,\n",
       "  'influentialCitationCount': 11,\n",
       "  'citationCount': 74,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e0155830d8982da4631cb71546fca782b2e00c20': {'arxiv_id': None,\n",
       "  's2_paperId': 'e0155830d8982da4631cb71546fca782b2e00c20',\n",
       "  'title': 'Composable Planning with Attributes',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 5,\n",
       "  'citationCount': 63,\n",
       "  'publicationType': 'Not available'},\n",
       " '56f1ffacf2ceb93f160c7edaf2a322f69322dab6': {'arxiv_id': None,\n",
       "  's2_paperId': '56f1ffacf2ceb93f160c7edaf2a322f69322dab6',\n",
       "  'title': 'MPC-Inspired Neural Network Policies for Sequential Decision Making',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 26,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 38,\n",
       "  'publicationType': 'Not available'},\n",
       " '142a8610d6c1428784a7d0901ce3fce6b896c5c5': {'arxiv_id': None,\n",
       "  's2_paperId': '142a8610d6c1428784a7d0901ce3fce6b896c5c5',\n",
       "  'title': 'Approximation Methods for Bilevel Programming',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 20,\n",
       "  'influentialCitationCount': 129,\n",
       "  'citationCount': 276,\n",
       "  'publicationType': 'Not available'},\n",
       " 'f14513ee0eb48defda66bd6b2e162319c53157b3': {'arxiv_id': None,\n",
       "  's2_paperId': 'f14513ee0eb48defda66bd6b2e162319c53157b3',\n",
       "  'title': 'Successive Convexification for 6-DoF Mars Rocket Powered Landing with Free-Final-Time',\n",
       "  'abstract': None,\n",
       "  'venue': '',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 7,\n",
       "  'citationCount': 116,\n",
       "  'publicationType': 'Not available'},\n",
       " '6adeda1af8abc6bc3c17c0b39f635a845476cd9f': {'arxiv_id': None,\n",
       "  's2_paperId': '6adeda1af8abc6bc3c17c0b39f635a845476cd9f',\n",
       "  'title': 'Deep learning for universal linear embeddings of nonlinear dynamics',\n",
       "  'abstract': 'Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear has the potential to enable nonlinear prediction, estimation, and control using linear theory. The Koopman operator is a leading data-driven embedding, and its eigenfunctions provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven challenging. This work leverages deep learning to discover representations of Koopman eigenfunctions from data. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold. We identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems with continuous spectra. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding, while connecting our models to decades of asymptotics. Thus, we benefit from the power of deep learning, while retaining the physical interpretability of Koopman embeddings. It is often advantageous to transform a strongly nonlinear system into a linear one in order to simplify its analysis for prediction and control. Here the authors combine dynamical systems with deep learning to identify these hard-to-find transformations.',\n",
       "  'venue': 'Nature Communications',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 67,\n",
       "  'influentialCitationCount': 61,\n",
       "  'citationCount': 1187,\n",
       "  'publicationType': 'Not available'},\n",
       " '5791b6ae21fe554efd48b57224fb8fb7220320d0': {'arxiv_id': None,\n",
       "  's2_paperId': '5791b6ae21fe554efd48b57224fb8fb7220320d0',\n",
       "  'title': 'Inverse KKT - Learning Cost Functions of Manipulation Tasks from Demonstrations',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Symposium of Robotics Research',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 44,\n",
       "  'influentialCitationCount': 19,\n",
       "  'citationCount': 120,\n",
       "  'publicationType': 'Not available'},\n",
       " '5e3fd9e6e7bcfc37fa751385ea3c8c7c7ac80c43': {'arxiv_id': None,\n",
       "  's2_paperId': '5e3fd9e6e7bcfc37fa751385ea3c8c7c7ac80c43',\n",
       "  'title': 'Maximum Principle Based Algorithms for Deep Learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Journal of machine learning research',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 62,\n",
       "  'influentialCitationCount': 30,\n",
       "  'citationCount': 221,\n",
       "  'publicationType': 'Not available'},\n",
       " '422ebe63f56be0bcc160a39ad3be6165c8d37b8e': {'arxiv_id': None,\n",
       "  's2_paperId': '422ebe63f56be0bcc160a39ad3be6165c8d37b8e',\n",
       "  'title': 'Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study',\n",
       "  'abstract': None,\n",
       "  'venue': 'SDM',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 72,\n",
       "  'influentialCitationCount': 10,\n",
       "  'citationCount': 143,\n",
       "  'publicationType': 'Not available'},\n",
       " '696ec77d7bcf5b71039877e07dbbdb341d04b4e3': {'arxiv_id': None,\n",
       "  's2_paperId': '696ec77d7bcf5b71039877e07dbbdb341d04b4e3',\n",
       "  'title': 'Path Integral Networks: End-to-End Differentiable Optimal Control',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 31,\n",
       "  'influentialCitationCount': 1,\n",
       "  'citationCount': 56,\n",
       "  'publicationType': 'Not available'},\n",
       " '33051bfb4bbf12431b8ab6bc8e33396d09a5448c': {'arxiv_id': None,\n",
       "  's2_paperId': '33051bfb4bbf12431b8ab6bc8e33396d09a5448c',\n",
       "  'title': 'A new concept using LSTM Neural Networks for dynamic system identification',\n",
       "  'abstract': None,\n",
       "  'venue': 'American Control Conference',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 24,\n",
       "  'influentialCitationCount': 8,\n",
       "  'citationCount': 194,\n",
       "  'publicationType': 'Not available'},\n",
       " 'e3f8c8253767b6fe0891025cdf772cd286337921': {'arxiv_id': None,\n",
       "  's2_paperId': 'e3f8c8253767b6fe0891025cdf772cd286337921',\n",
       "  'title': 'A Proposal on Machine Learning via Dynamical Systems',\n",
       "  'abstract': None,\n",
       "  'venue': 'Communications in Mathematics and Statistics',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 14,\n",
       "  'influentialCitationCount': 35,\n",
       "  'citationCount': 745,\n",
       "  'publicationType': 'Not available'},\n",
       " '7b581c9ce200b031451f592478c7c34b5fc47898': {'arxiv_id': None,\n",
       "  's2_paperId': '7b581c9ce200b031451f592478c7c34b5fc47898',\n",
       "  'title': 'Task-based End-to-end Model Learning in Stochastic Optimization',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 32,\n",
       "  'influentialCitationCount': 40,\n",
       "  'citationCount': 321,\n",
       "  'publicationType': 'Not available'},\n",
       " '0076b232181e4e5be58dce8354a813ad2bbf663a': {'arxiv_id': '1703.00443v5',\n",
       "  's2_paperId': '0076b232181e4e5be58dce8354a813ad2bbf663a',\n",
       "  'title': 'OptNet: Differentiable Optimization as a Layer in Neural Networks',\n",
       "  'abstract': 'This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2017,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 134,\n",
       "  'citationCount': 935,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '5f184c2b2c45444e9b0352f9bf61bc20eecd0535': {'arxiv_id': None,\n",
       "  's2_paperId': '5f184c2b2c45444e9b0352f9bf61bc20eecd0535',\n",
       "  'title': 'Deep Learning Approximation for Stochastic Control Problems',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 21,\n",
       "  'influentialCitationCount': 26,\n",
       "  'citationCount': 190,\n",
       "  'publicationType': 'Not available'},\n",
       " 'ecad86edc9ab2e21597d99d5cb3dd64b289b110d': {'arxiv_id': None,\n",
       "  's2_paperId': 'ecad86edc9ab2e21597d99d5cb3dd64b289b110d',\n",
       "  'title': 'Learning from the hindsight plan — Episodic MPC improvement',\n",
       "  'abstract': None,\n",
       "  'venue': 'IEEE International Conference on Robotics and Automation',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 0,\n",
       "  'citationCount': 64,\n",
       "  'publicationType': 'Not available'},\n",
       " '5129a9cbb6de3c6579f6a7d974394d392ac29829': {'arxiv_id': '1605.09128v1',\n",
       "  's2_paperId': '5129a9cbb6de3c6579f6a7d974394d392ac29829',\n",
       "  'title': 'Control of Memory, Active Perception, and Action in Minecraft',\n",
       "  'abstract': 'In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 42,\n",
       "  'influentialCitationCount': 20,\n",
       "  'citationCount': 300,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " 'f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb': {'arxiv_id': '1605.07157v4',\n",
       "  's2_paperId': 'f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb',\n",
       "  'title': 'Unsupervised Learning for Physical Interaction through Video Prediction',\n",
       "  'abstract': 'A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot\\'s future actions amounts to learning a \"visual imagination\" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 34,\n",
       "  'influentialCitationCount': 90,\n",
       "  'citationCount': 1036,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '0e3cc46583217ec81e87045a4f9ae3478a008227': {'arxiv_id': None,\n",
       "  's2_paperId': '0e3cc46583217ec81e87045a4f9ae3478a008227',\n",
       "  'title': 'End to End Learning for Self-Driving Cars',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 10,\n",
       "  'influentialCitationCount': 346,\n",
       "  'citationCount': 4103,\n",
       "  'publicationType': 'Not available'},\n",
       " '9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d': {'arxiv_id': None,\n",
       "  's2_paperId': '9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d',\n",
       "  'title': 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems',\n",
       "  'abstract': None,\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 54,\n",
       "  'influentialCitationCount': 1095,\n",
       "  'citationCount': 11050,\n",
       "  'publicationType': 'Not available'},\n",
       " 'd358d41c69450b171327ebd99462b6afef687269': {'arxiv_id': None,\n",
       "  's2_paperId': 'd358d41c69450b171327ebd99462b6afef687269',\n",
       "  'title': 'Continuous Deep Q-Learning with Model-based Acceleration',\n",
       "  'abstract': None,\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 41,\n",
       "  'influentialCitationCount': 80,\n",
       "  'citationCount': 1003,\n",
       "  'publicationType': 'Not available'},\n",
       " '9d6fe89150401d69a90ae10dc9c4135ab406903c': {'arxiv_id': None,\n",
       "  's2_paperId': '9d6fe89150401d69a90ae10dc9c4135ab406903c',\n",
       "  'title': 'Generalizing Koopman Theory to Allow for Inputs and Control',\n",
       "  'abstract': 'We develop a new generalization of Koopman operator theory that incorporates the e ects of inputs and control. Koopman spectral analysis is a theoretical tool for the analysis of nonlinear dynamical systems. Moreover, Koopman is intimately connected to dynamic mode decomposition (DMD), a method that discovers coherent, spatio-temporal modes from data, connects local-linear analysis to nonlinear operator theory, and importantly creates an equation-free architecture for the study of complex systems. For actuated systems, standard Koopman analysis and DMD are incapable of producing input-output models; moreover, the dynamics and the modes will be corrupted by external forcing. Our new theoretical developments extend Koopman operator theory to allow for systems with nonlinear input-output characteristics. We show how this generalization is rigorously connected to a recent development called dynamic mode decomposition with control. We demonstrate this new theory on nonlinear dynamical systems, including a standard susceptible-infectious-recovered model with relevance to the analysis of infectious disease data with mass vaccination (actuation).',\n",
       "  'venue': 'SIAM Journal on Applied Dynamical Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 56,\n",
       "  'influentialCitationCount': 16,\n",
       "  'citationCount': 314,\n",
       "  'publicationType': 'Not available'},\n",
       " '84680b30a20775e5d319419a7f3f2a93e57c2a61': {'arxiv_id': '1602.02867v4',\n",
       "  's2_paperId': '84680b30a20775e5d319419a7f3f2a93e57c2a61',\n",
       "  'title': 'Value Iteration Networks',\n",
       "  'abstract': \"We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 64,\n",
       "  'influentialCitationCount': 89,\n",
       "  'citationCount': 641,\n",
       "  'publicationType': 'JournalArticle | Conference'},\n",
       " '54913e375f1d3fa6b018e6897957943dbef462f0': {'arxiv_id': None,\n",
       "  's2_paperId': '54913e375f1d3fa6b018e6897957943dbef462f0',\n",
       "  'title': 'Adapting Deep Visuomotor Representations with Weak Pairwise Constraints',\n",
       "  'abstract': None,\n",
       "  'venue': 'Workshop on the Algorithmic Foundations of Robotics',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 48,\n",
       "  'influentialCitationCount': 6,\n",
       "  'citationCount': 138,\n",
       "  'publicationType': 'Not available'},\n",
       " '6640f4e4beae786f301928d82a9f8eb037aa6935': {'arxiv_id': None,\n",
       "  's2_paperId': '6640f4e4beae786f301928d82a9f8eb037aa6935',\n",
       "  'title': 'Learning Continuous Control Policies by Stochastic Value Gradients',\n",
       "  'abstract': None,\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2015,\n",
       "  'referenceCount': 33,\n",
       "  'influentialCitationCount': 54,\n",
       "  'citationCount': 557,\n",
       "  'publicationType': 'Not available'},\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d111bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ids_file='data/teacher_graph/processed_ids.pickle'\n",
    "papers_data_file='data/teacher_graph/papers_data.pickle'\n",
    "s2_map_file='data/teacher_graph/s2_map.pickle'\n",
    "paper_index = pickle.load(open(papers_data_file, \"rb\")) if os.path.exists(papers_data_file) else {}\n",
    "processed = pickle.load(open(processed_ids_file, \"rb\")) if os.path.exists(processed_ids_file) else []\n",
    "s2_map = pickle.load(open(s2_map_file, \"rb\")) if os.path.exists(s2_map_file) else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6b692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1764/1764 [00:00<00:00, 10447.15it/s]\n"
     ]
    }
   ],
   "source": [
    "student_set_ids = set()\n",
    "teacher_set_ids = set()\n",
    "venues = set()\n",
    "special_ids = set()\n",
    "for arxiv_id, paper in tqdm(paper_index.items()):\n",
    "    if 's2_paperId' not in paper.keys():\n",
    "        continue\n",
    "    student_set_ids.add(paper['s2_paperId'])\n",
    "    for j, citation in enumerate(paper['citations']):\n",
    "        teacher_set_ids.add(citation['paperId'])\n",
    "        if citation['venue'] in ['Neural Information Processing Systems', 'International Conference on Learning Representations', 'International Conference on Machine Learning', 'Conference on Computer Vision and Pattern Recognition', 'European Conference on Computer Vision', 'International Conference on Computer Vision', 'Association for the Advancement of Artificial Intelligence', 'International Joint Conference on Artificial Intelligence', 'International Conference on Data Mining', 'International Conference on Knowledge Discovery and Data Mining', 'International Conference on Web Search and Data Mining', 'International Conference on Machine Learning and Applications', 'International Conference on Pattern Recognition']:\n",
    "            special_ids.add(citation['paperId'])\n",
    "        venues.add(citation['venue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac39f715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1760, 126527, 6722)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(student_set_ids), len(teacher_set_ids), len(venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1feac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_paper_ids = pickle.load(open('data/teacher_graph/paper_ids.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c7f2aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14451"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(special_ids.difference(student_paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7522f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 5000 ids from the teacher set\n",
    "import random\n",
    "new_ids = special_ids.difference(student_paper_ids)\n",
    "random_ids = random.sample(list(new_ids), 5000)\n",
    "\n",
    "pickle.dump(random_ids, open('data/teacher_graph/remaining_ids.pickle', \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7521ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15984\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "remaining_ids = pickle.load(open('data/teacher_graph/remaining_ids.pickle', \"rb\"))\n",
    "ref_ids = ref_ids.difference(remaining_ids)\n",
    "ref_ids = ref_ids.difference(student_paper_ids)\n",
    "print(len(ref_ids))\n",
    "\n",
    "# randomly select 3000 ids from the ref_ids\n",
    "random_ids = random.sample(list(ref_ids), 3000)\n",
    "# split the random_ids into 2 lists\n",
    "eval_ids = random_ids[:1000]\n",
    "test_ids = random_ids[1000:]\n",
    "\n",
    "pickle.dump(eval_ids, open('data/teacher_graph/eval_ids.pickle', \"wb\"))\n",
    "pickle.dump(test_ids, open('data/teacher_graph/test_ids.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b6b1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(teacher_set_ids.difference(student_set_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0639ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ids = pickle.load(open('data/teacher_graph/possible_new_data.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "063405b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_s2_ids = []\n",
    "for i in new_ids:\n",
    "    for j in new_ids[i]['citations']:\n",
    "        if j['venue'] in ['ICML', 'NeurIPS', 'ICLR']:\n",
    "            new_s2_ids.append(j['paperId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "774aa1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20574"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(new_s2_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24420717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16941"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(new_s2_ids).difference(student_paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c65f3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_ids = set(new_s2_ids).difference(student_paper_ids)\n",
    "\n",
    "import random\n",
    "# randomly select 8000 ids from the extra_ids\n",
    "random_ids = random.sample(list(extra_ids), 8000)\n",
    "# split the random_ids into 3 lists of 5000, 2,000 and 1,000\n",
    "eval_ids = random_ids[:1500]\n",
    "test_ids = random_ids[1500:3000]\n",
    "extra_train_ids = random_ids[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b9ca58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_ids, open('data/teacher_graph/eval_ids.pickle', \"wb\"))\n",
    "pickle.dump(test_ids, open('data/teacher_graph/test_ids.pickle', \"wb\"))\n",
    "pickle.dump(extra_train_ids, open('data/teacher_graph/extra_train_ids.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7009d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_records = pickle.load(open('data/teacher_graph/records/paper_records.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffe1aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_author_ids = set()\n",
    "for paper in paper_records.values():\n",
    "    all_author_ids.update(paper['author_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e173df8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35983"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab50f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7207"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_records = pickle.load(open('data/teacher_graph/records/author_records.pkl', \"rb\"))\n",
    "author_ids = author_records.keys()\n",
    "len(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9beaa207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35980"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_records = pickle.load(open('data/teacher_graph/records/author_records.pkl', \"rb\"))\n",
    "author_ids = author_records.keys()\n",
    "len(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f5c227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_author_ids = all_author_ids.difference(author_ids)\n",
    "\n",
    "# save the remaining_author_ids to a file\n",
    "pickle.dump(remaining_author_ids, open('data/teacher_graph/remaining_author_ids.pickle', \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
